{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mC_7G7cE20lS",
        "outputId": "6e6dcb20-4eb5-454c-ac51-f9f1057d8c7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA2899Z7lFIV"
      },
      "source": [
        "# Just some testing\n",
        "\n",
        "Will attempt to load and run a test function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APMUT3JhlBfa"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import os\n",
        "import sys\n",
        "sys.path.append(os.getcwd())\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from pathlib import Path\n",
        "\n",
        "# # print(\"Downloading embeddings_sm.py\")\n",
        "# request = requests.get(\"https://raw.githubusercontent.com/shanaam/nlp_project_oviraptor_twist_g2/sm_working_branch/embeddings/embeddings_sm.py\")\n",
        "# with open(\"embeddings_sm.py\", \"wb\") as f:\n",
        "#     f.write(request.content)\n",
        "\n",
        "# request = requests.get(\"https://raw.githubusercontent.com/shanaam/nlp_project_oviraptor_twist_g2/sm_working_branch/model/model_sm.py\")\n",
        "# with open(\"model_sm.py\", \"wb\") as f:\n",
        "#     f.write(request.content)\n",
        "\n",
        "# files = [f for f in os.listdir('.') if os.path.isfile(f)]\n",
        "# files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# os.chdir('/content')\n",
        "# from embeddings_sm import TokenizedC4Dataset # NOTE: if embeddings_sm cannot be resolved, most likely need to cd into the correct directory\n",
        "# from model_sm import MLP"
      ],
      "metadata": {
        "id": "9yStdIfKS3_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fstvn6CrxqV"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8zSJQIOrwMo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0d43661-983b-4cd4-97b3-2fedb7ae7be3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 254106\n",
              "    })\n",
              "    balanced: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 40581\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "from datasets import load_dataset # hugging face datasets\n",
        "\n",
        "dataset = load_dataset(\"Numind/C4_sentiment-analysis\") # Note: this is already a dataset\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1198e3a-a1cf-4074-8018-dcfd0e9520da",
        "id": "BbdmYWMZRlQT"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading embeddings_sm.py\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['state_dict.pt', 'embeddings_sm.py']"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "import requests\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# Download helper functions from Learn PyTorch repo (if not already downloaded)\n",
        "\n",
        "print(\"Downloading embeddings_sm.py\")\n",
        "request = requests.get(\"https://raw.githubusercontent.com/shanaam/nlp_project_oviraptor_twist_g2/sm_working_branch/embeddings/embeddings_sm.py\")\n",
        "with open(\"embeddings_sm.py\", \"wb\") as f:\n",
        "    f.write(request.content)\n",
        "\n",
        "files = [f for f in os.listdir('.') if os.path.isfile(f)]\n",
        "files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46KwUWf529nh",
        "outputId": "353fc7da-b4a6-4ae0-fa93-f8c13f114b51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_dataset has 40581 rows\n",
            "\n",
            "Example:\n",
            "{'text': 'The market awoke from its slumber to a classic dump fest  The price fell over $10 from $255 to a low of $243 on Bitfinex It looks as if the rout will continue over night as the market pauses before resuming the downward spiral', 'label': 1}\n",
            "\n",
            "\n",
            "We would need to do our own train/test split. If we hold out 20% of data, we have 32464 rows to train on.\n"
          ]
        }
      ],
      "source": [
        "train_data = dataset['balanced']\n",
        "\n",
        "print(f\"Train_dataset has {len(train_data)} rows\")\n",
        "print(\"\\nExample:\")\n",
        "print(train_data[5])\n",
        "\n",
        "print(f\"\\n\\nWe would need to do our own train/test split. If we hold out 20% of data, we have {int(len(train_data) * 0.8)} rows to train on.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of unique labels: {len(set(train_data['label']))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpSwal2jc_TT",
        "outputId": "122093ba-0bc0-42d5-93d9-e39c3597d65f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique labels: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_df = pd.DataFrame(train_data)"
      ],
      "metadata": {
        "id": "W4Ua3NGac46g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_df.head(25)\n",
        "df = train_data_df[:40000]\n",
        "df.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "4B7VDPW2d4Z6",
        "outputId": "5becbcf7-6ef2-4502-9302-1a223a09a051"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  label\n",
              "0  The door of Peter Shire’s first ceramics studi...      1\n",
              "1  Another 400 charities are in danger of losing ...      1\n",
              "2  Action learning teambuilding programs for orga...      2\n",
              "3  From your physical assets down to business dat...      2\n",
              "4  PHILADELPHIA (CNN) -- Democrats kick off their...      1\n",
              "5  The market awoke from its slumber to a classic...      1\n",
              "6  Long Tall Deb and Colin John continue their ge...      0\n",
              "7  For months  many market watchers have describe...      2\n",
              "8  It seems that the database on this ancient unm...      1\n",
              "9  Funding banking helps to spice up the economy ...      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cc8feef7-7071-4a23-b3bb-711512dc7272\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The door of Peter Shire’s first ceramics studi...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Another 400 charities are in danger of losing ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Action learning teambuilding programs for orga...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>From your physical assets down to business dat...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PHILADELPHIA (CNN) -- Democrats kick off their...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>The market awoke from its slumber to a classic...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Long Tall Deb and Colin John continue their ge...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>For months  many market watchers have describe...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>It seems that the database on this ancient unm...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Funding banking helps to spice up the economy ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cc8feef7-7071-4a23-b3bb-711512dc7272')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cc8feef7-7071-4a23-b3bb-711512dc7272 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cc8feef7-7071-4a23-b3bb-711512dc7272');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1f249739-4a5c-4d49-927a-9545a66d24d4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1f249739-4a5c-4d49-927a-9545a66d24d4')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1f249739-4a5c-4d49-927a-9545a66d24d4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 40000,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 39982,\n        \"samples\": [\n          \"People need what you have to give Your light can shine through your art and the purpose you feel burning in your soul is real! You got this! One foot in front of the other! Don't miss the last video on finding your \\\"ikigai\\\" (purpose for life)\",\n          \"Always exuberant  always results-driven and always actionable  Nichola\\u2019s consulting  speaking and training services provide an infectious way to learn SEO.\",\n          \"Pardon me but you completely spelled One of the players name wrong in your article by the way his name is Jordaine Dawkins and he\\u2019s class 2021 You put \\u201cJordy Dawkins  Jordy Hawkins 2020\\u201d\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          2,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import string\n",
        "import re\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "4PC1RBH6KLiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNo0Tu4gLPqr",
        "outputId": "0bac0e3d-032c-44f0-9604-9ac41abe0f1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X,y = df['text'].values,df['label'].values\n",
        "x_train,x_test,y_train,y_test = train_test_split(X,y,stratify=y)\n",
        "print(f'shape of train data is {x_train.shape}')\n",
        "print(f'shape of test data is {x_test.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRHcNR0JKcdH",
        "outputId": "3dd3e867-089e-44fa-ff1d-84c1d6dd148d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (30000,)\n",
            "shape of test data is (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dd = pd.Series(y_train).value_counts()\n",
        "sns.barplot(x=np.array(['0','1', '2']),y=dd.values)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "gBDKGin9KouL",
        "outputId": "58ebe666-314d-44b1-f20f-69b9634e8ac7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_string(s):\n",
        "    # Remove all non-word characters (everything except numbers and letters)\n",
        "    s = re.sub(r\"[^\\w\\s]\", '', s)\n",
        "    # Replace all runs of whitespaces with no space\n",
        "    s = re.sub(r\"\\s+\", '', s)\n",
        "    # replace digits with no space\n",
        "    s = re.sub(r\"\\d\", '', s)\n",
        "\n",
        "    return s\n",
        "\n",
        "def tockenize(x_train,y_train,x_val,y_val):\n",
        "    word_list = []\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    for sent in x_train:\n",
        "        for word in sent.lower().split():\n",
        "            word = preprocess_string(word)\n",
        "            if word not in stop_words and word != '':\n",
        "                word_list.append(word)\n",
        "\n",
        "    corpus = Counter(word_list)\n",
        "    # sorting on the basis of most common words\n",
        "    corpus_ = sorted(corpus,key=corpus.get,reverse=True)[:1000]\n",
        "    # creating a dict\n",
        "    onehot_dict = {w:i+1 for i,w in enumerate(corpus_)}\n",
        "\n",
        "    # tockenize\n",
        "    final_list_train,final_list_test = [],[]\n",
        "    for sent in x_train:\n",
        "            final_list_train.append([onehot_dict[preprocess_string(word)] for word in sent.lower().split()\n",
        "                                     if preprocess_string(word) in onehot_dict.keys()])\n",
        "    for sent in x_val:\n",
        "            final_list_test.append([onehot_dict[preprocess_string(word)] for word in sent.lower().split()\n",
        "                                    if preprocess_string(word) in onehot_dict.keys()])\n",
        "\n",
        "    # encoded_train = [1 if label =='positive' else 0 for label in y_train]\n",
        "    # encoded_test = [1 if label =='positive' else 0 for label in y_val]\n",
        "    return final_list_train, y_train, final_list_test, y_val,onehot_dict\n"
      ],
      "metadata": {
        "id": "deYEMp1IK6ID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train,y_train,x_test,y_test ,vocab = tockenize(x_train,y_train,x_test,y_test)"
      ],
      "metadata": {
        "id": "azID_rMALAMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"length of first element of x_train: {len(x_train[0])}\")\n",
        "print(f\"\\nlength of second element of x_train: {len(x_train[1])}\")\n",
        "# note: vocab is a one-hot dictionary\n",
        "print(f'\\nLength of vocabulary is {len(vocab)}')"
      ],
      "metadata": {
        "id": "JcLM45tYLYdA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f88be29-b57f-4cb4-d0aa-aff3e1aca2b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of first element of x_train: 16\n",
            "\n",
            "length of second element of x_train: 4\n",
            "\n",
            "Length of vocabulary is 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "{k: vocab[k] for k in list(vocab)[:5]} # 5 values from vocab dict."
      ],
      "metadata": {
        "id": "RSS7batOLYW1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0e32420-344b-4331-9b30-5356b3595762"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'one': 1, 'new': 2, 'time': 3, 'like': 4, 'people': 5}"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rev_len = [len(i) for i in x_train]\n",
        "pd.Series(rev_len).hist()\n",
        "plt.show()\n",
        "pd.Series(rev_len).describe()"
      ],
      "metadata": {
        "id": "bI5hAEqeLleX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "outputId": "98d89f18-08b2-4bf6-f7c6-c905363e5e74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    30000.000000\n",
              "mean         7.776267\n",
              "std          4.721005\n",
              "min          0.000000\n",
              "25%          4.000000\n",
              "50%          7.000000\n",
              "75%         11.000000\n",
              "max         87.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def padding_(sentences, seq_len):\n",
        "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
        "    for ii, review in enumerate(sentences):\n",
        "        if len(review) != 0:\n",
        "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
        "    return features\n",
        "\n",
        "#we have very less number of reviews with length > 50.\n",
        "#So we will consideronly those below it.\n",
        "x_train_pad = padding_(x_train, 50)\n",
        "x_test_pad = padding_(x_test, 50)\n",
        "\n",
        "print(f\"length of first element of x_train: {len(x_train_pad[0])}\")\n",
        "print(f\"\\nlength of second element of x_train: {len(x_train_pad[1])}\")\n",
        "print(x_train_pad[0])\n",
        "print(y_train)"
      ],
      "metadata": {
        "id": "mmod_7S7LrMY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25f19be5-ca70-41f6-b9d5-067641f654e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of first element of x_train: 50\n",
            "\n",
            "length of second element of x_train: 50\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 180 828\n",
            " 918 828 417 727  96 195 203 127 860  28 861 542 836 860]\n",
            "[1 1 0 ... 2 2 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create Tensor datasets\n",
        "train_data = TensorDataset(torch.from_numpy(x_train_pad), torch.from_numpy(y_train))\n",
        "valid_data = TensorDataset(torch.from_numpy(x_test_pad), torch.from_numpy(y_test))\n",
        "\n",
        "# dataloaders\n",
        "batch_size = 100\n",
        "\n",
        "# make sure to SHUFFLE your data\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "DO5nyzIoL4lU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader"
      ],
      "metadata": {
        "id": "YssaqzbhMIT7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edf80edb-f6a7-4376-9ae0-06e009014f17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7b8d36e467a0>"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# obtain one batch of training data\n",
        "# dataiter = iter(train_loader)\n",
        "# sample_x, sample_y = dataiter.next()\n",
        "\n",
        "# print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
        "# print('Sample input: \\n', sample_x)\n",
        "# print('Sample input: \\n', sample_y)"
      ],
      "metadata": {
        "id": "XJjqO-CrL6Uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentRNN(nn.Module):\n",
        "    def __init__(self,n_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5):\n",
        "        super(SentimentRNN,self).__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.n_layers = n_layers\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # embedding and LSTM layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        #lstm\n",
        "        self.lstm = nn.LSTM(input_size = embedding_dim,\n",
        "                            hidden_size = self.hidden_dim,\n",
        "                            num_layers = n_layers,\n",
        "                            batch_first = True\n",
        "                            )\n",
        "\n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "        # linear and sigmoid layer\n",
        "        self.fc = nn.Linear(self.hidden_dim, 100)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(100, 3)\n",
        "\n",
        "    def forward(self,x,hidden):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        print(f\"batch size: {batch_size}\")\n",
        "        # embeddings and lstm_out\n",
        "        embeds = self.embedding(x)  # shape: B x S x Feature   since batch = True\n",
        "        print(f\"embed shape: {embeds.shape}\")  #[100, 50, 64]\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "\n",
        "        # lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim) # was used for 1d output\n",
        "\n",
        "        print(f\"\\nlstm out shape: {lstm_out.shape}\")\n",
        "        print(f\"lstm out: {lstm_out}\")\n",
        "\n",
        "        # dropout and fully connected layer\n",
        "        out = self.dropout(lstm_out)\n",
        "        # out = self.fc(out)\n",
        "        out = self.relu(self.fc(out))\n",
        "\n",
        "        # sigmoid function\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        # print(f\"\\nfc2 out shape: {out.shape}\")\n",
        "        # print(f\"fc2 out: {out}\")\n",
        "\n",
        "        out = F.log_softmax(out, dim=1)\n",
        "\n",
        "        # print(f\"\\nsoftmax out shape: {out.shape}\")\n",
        "        # print(f\"softmax out: {out}\")\n",
        "\n",
        "        # reshape to be batch_size first\n",
        "        # out = out.view(batch_size, -1)\n",
        "        # out = out[:, -1] # get last batch of labels\n",
        "\n",
        "        out = out[:, -1, :]\n",
        "\n",
        "        # print(f\"\\nOUT shape: {out.shape}\")\n",
        "        # print(f\"OUT: {out}\")\n",
        "\n",
        "        # return last sigmoid output and hidden state\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        h0 = torch.zeros((self.n_layers, batch_size, self.hidden_dim)).to(device)\n",
        "        c0 = torch.zeros((self.n_layers, batch_size, self.hidden_dim)).to(device)\n",
        "        hidden = (h0,c0)\n",
        "        return hidden\n"
      ],
      "metadata": {
        "id": "0y4mihniMSAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, CPU used\")"
      ],
      "metadata": {
        "id": "73_8wJgBMX_4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c67f0a5a-d17b-4350-f46c-747fda2ca23c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_layers = 2\n",
        "vocab_size = len(vocab) + 1 #extra 1 for padding\n",
        "embedding_dim = 64\n",
        "output_dim = 3\n",
        "hidden_dim = 300\n",
        "\n",
        "\n",
        "model = SentimentRNN(n_layers, vocab_size, hidden_dim, embedding_dim, drop_prob=0.5)\n",
        "\n",
        "#moving to gpu\n",
        "model.to(device)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "i0jmfQc7MUP2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9fef587-938f-45fc-d1fa-363e95eac29a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SentimentRNN(\n",
            "  (embedding): Embedding(1001, 64)\n",
            "  (lstm): LSTM(64, 300, num_layers=2, batch_first=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=300, out_features=100, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (fc2): Linear(in_features=100, out_features=3, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loss and optimization functions\n",
        "lr=0.001\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# function to predict accuracy\n",
        "# FIX pred squeeze and mean might not work for non binary case\n",
        "def acc(pred,label):\n",
        "    pred = torch.argmax(pred, dim = 1)\n",
        "    return torch.sum(pred == label)"
      ],
      "metadata": {
        "id": "n_148W8jMrEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clip = 5\n",
        "epochs = 5\n",
        "valid_loss_min = np.Inf\n",
        "# train for some number of epochs\n",
        "epoch_tr_loss,epoch_vl_loss = [],[]\n",
        "epoch_tr_acc,epoch_vl_acc = [],[]\n",
        "all_train_losses = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_losses = []\n",
        "    train_acc = 0.0\n",
        "    model.train()\n",
        "\n",
        "    # initialize hidden state\n",
        "    h = model.init_hidden(batch_size)\n",
        "    for inputs, labels in train_loader:\n",
        "\n",
        "        # print(f\"inputs: {inputs}\")\n",
        "        # print(f\"\\nlabels: {labels}\")\n",
        "\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        # Creating new variables for the hidden state, otherwise\n",
        "        # we'd backprop through the entire training history\n",
        "        h = tuple([each.data for each in h])\n",
        "\n",
        "        # print(f\"h shape: {h[0].shape}\")\n",
        "\n",
        "        model.zero_grad()\n",
        "        output,h = model(inputs,h)\n",
        "\n",
        "        # calculate the loss and perform backprop\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        train_losses.append(loss.item())\n",
        "        all_train_losses.append(loss.item())\n",
        "\n",
        "        # calculating accuracy\n",
        "        accuracy = acc(output, labels)\n",
        "        train_acc += accuracy\n",
        "        #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "    val_h = model.init_hidden(batch_size)\n",
        "    val_losses = []\n",
        "    val_acc = 0.0\n",
        "    model.eval()\n",
        "    for inputs, labels in valid_loader:\n",
        "            val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            output, val_h = model(inputs, val_h)\n",
        "            val_loss = criterion(output, labels)\n",
        "\n",
        "            val_losses.append(val_loss.item())\n",
        "\n",
        "            accuracy = acc(output, labels)\n",
        "            val_acc += accuracy\n",
        "\n",
        "    epoch_train_loss = np.mean(train_losses)\n",
        "    epoch_val_loss = np.mean(val_losses)\n",
        "    epoch_train_acc = train_acc/len(train_loader.dataset)\n",
        "    epoch_val_acc = val_acc/len(valid_loader.dataset)\n",
        "    epoch_tr_loss.append(epoch_train_loss)\n",
        "    epoch_vl_loss.append(epoch_val_loss)\n",
        "    epoch_tr_acc.append(epoch_train_acc)\n",
        "    epoch_vl_acc.append(epoch_val_acc)\n",
        "    print(f'Epoch {epoch+1}')\n",
        "    print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}')\n",
        "    print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')\n",
        "    if epoch_val_loss <= valid_loss_min:\n",
        "        torch.save(model.state_dict(), 'state_dict.pt')\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,epoch_val_loss))\n",
        "        valid_loss_min = epoch_val_loss\n",
        "    print(25*'==')"
      ],
      "metadata": {
        "id": "RkQXr9f5MhcW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74f2b360-b2e7-4b09-9d3d-32cd9ef8dcda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "         [ 0.0630, -0.2531, -0.0664,  ..., -0.2313,  0.0166, -0.0505],\n",
            "         [ 0.1007, -0.2343, -0.0372,  ..., -0.2572, -0.0846, -0.0743],\n",
            "         [ 0.0882, -0.1029, -0.0154,  ..., -0.2979, -0.1427, -0.1164]],\n",
            "\n",
            "        [[ 0.0142,  0.0141, -0.0257,  ..., -0.0367, -0.2712, -0.0298],\n",
            "         [ 0.0238,  0.0290, -0.0282,  ..., -0.0956, -0.2324, -0.0312],\n",
            "         [ 0.0240,  0.0540, -0.0320,  ..., -0.1120, -0.1717, -0.0291],\n",
            "         ...,\n",
            "         [ 0.0349,  0.1339, -0.0934,  ...,  0.0537, -0.0931, -0.0567],\n",
            "         [ 0.0039,  0.1312, -0.0282,  ...,  0.1221, -0.2003, -0.1232],\n",
            "         [ 0.0147,  0.0675, -0.0071,  ...,  0.1702, -0.3701, -0.1345]],\n",
            "\n",
            "        [[ 0.0080,  0.1142, -0.0686,  ..., -0.0200, -0.0689, -0.0721],\n",
            "         [ 0.0339,  0.1143, -0.0721,  ..., -0.0451, -0.0730, -0.0690],\n",
            "         [ 0.0281,  0.1269, -0.0784,  ..., -0.0583, -0.0633, -0.0700],\n",
            "         ...,\n",
            "         [ 0.1320, -0.2949,  0.0700,  ..., -0.1700,  0.0554,  0.0304],\n",
            "         [ 0.1172, -0.4497,  0.0734,  ..., -0.1875, -0.0015,  0.1200],\n",
            "         [-0.0565, -0.1514,  0.0894,  ..., -0.0882, -0.0411, -0.0279]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.1499, -0.2065,  0.1856,  ..., -0.4724,  0.0087,  0.5506],\n",
            "         [ 0.0372, -0.0792,  0.1777,  ..., -0.3992, -0.0145,  0.4479],\n",
            "         [-0.0165, -0.0072,  0.1185,  ..., -0.2901, -0.0358,  0.3885],\n",
            "         ...,\n",
            "         [-0.0457,  0.1882, -0.0216,  ...,  0.1399, -0.1819, -0.2718],\n",
            "         [-0.1336,  0.1295,  0.0111,  ...,  0.1580, -0.2386, -0.2638],\n",
            "         [-0.0354,  0.0419,  0.0030,  ...,  0.0299, -0.3142, -0.1589]],\n",
            "\n",
            "        [[-0.0290,  0.0351,  0.0090,  ...,  0.0273, -0.2794, -0.1314],\n",
            "         [-0.0036,  0.0296, -0.0149,  ..., -0.0829, -0.2313, -0.0763],\n",
            "         [ 0.0096,  0.0476, -0.0216,  ..., -0.1136, -0.1771, -0.0513],\n",
            "         ...,\n",
            "         [ 0.2590, -0.3217,  0.1153,  ..., -0.3626, -0.2107,  0.5561],\n",
            "         [ 0.2868, -0.2465,  0.1504,  ..., -0.5760, -0.2527,  0.5164],\n",
            "         [ 0.2045, -0.1688,  0.1724,  ..., -0.4569, -0.1556,  0.4946]],\n",
            "\n",
            "        [[ 0.0206,  0.1032, -0.0945,  ...,  0.0259, -0.1052, -0.0668],\n",
            "         [ 0.0437,  0.1059, -0.0801,  ..., -0.0228, -0.1167, -0.0548],\n",
            "         [ 0.0406,  0.1136, -0.0744,  ..., -0.0494, -0.1060, -0.0459],\n",
            "         ...,\n",
            "         [ 0.0895,  0.0886, -0.0379,  ..., -0.1854,  0.0330,  0.0127],\n",
            "         [ 0.1145, -0.1431, -0.0063,  ..., -0.3570, -0.0158,  0.1440],\n",
            "         [ 0.1039, -0.1533,  0.0299,  ..., -0.4228, -0.0137,  0.2479]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 0.0466,  0.0264, -0.0242,  ..., -0.1620, -0.1077, -0.1017],\n",
            "         [ 0.0285,  0.0846, -0.0458,  ..., -0.1158, -0.0832, -0.0957],\n",
            "         [ 0.0204,  0.1141, -0.0625,  ..., -0.0860, -0.0756, -0.0863],\n",
            "         ...,\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [-0.0487,  0.1131, -0.1180,  ..., -0.0814,  0.0063, -0.1215],\n",
            "         [-0.0410,  0.1314, -0.1452,  ..., -0.0864,  0.0769, -0.1616]],\n",
            "\n",
            "        [[ 0.0451,  0.0333, -0.0419,  ..., -0.0088, -0.3021, -0.0735],\n",
            "         [ 0.0348,  0.0546, -0.0438,  ..., -0.0802, -0.2214, -0.0723],\n",
            "         [ 0.0299,  0.0753, -0.0442,  ..., -0.0984, -0.1592, -0.0569],\n",
            "         ...,\n",
            "         [-0.0171,  0.0797, -0.0016,  ..., -0.1018, -0.0672,  0.0088],\n",
            "         [-0.0459, -0.0144, -0.0339,  ..., -0.1325,  0.0997, -0.0506],\n",
            "         [-0.0525,  0.1077,  0.0233,  ..., -0.0456,  0.0682, -0.1023]],\n",
            "\n",
            "        [[-0.0359, -0.0412,  0.0422,  ..., -0.0504, -0.0664, -0.0572],\n",
            "         [-0.0023,  0.0196, -0.0090,  ..., -0.0583, -0.0759, -0.0654],\n",
            "         [ 0.0140,  0.0616, -0.0422,  ..., -0.0680, -0.0820, -0.0620],\n",
            "         ...,\n",
            "         [-0.0970,  0.0696, -0.0242,  ...,  0.0690, -0.0837, -0.1284],\n",
            "         [-0.1514,  0.0264, -0.0320,  ...,  0.0640, -0.0851, -0.0767],\n",
            "         [ 0.0365, -0.0091, -0.0638,  ..., -0.0203, -0.1068, -0.0285]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0206,  0.0279, -0.0008,  ..., -0.1034, -0.2817, -0.0660],\n",
            "         [ 0.0259,  0.0452, -0.0133,  ..., -0.1282, -0.2025, -0.0559],\n",
            "         [ 0.0277,  0.0653, -0.0242,  ..., -0.1253, -0.1436, -0.0462],\n",
            "         ...,\n",
            "         [ 0.0396,  0.0366, -0.1815,  ..., -0.0555, -0.0916,  0.0012],\n",
            "         [ 0.0266,  0.0087, -0.1799,  ..., -0.0878, -0.0252,  0.0139],\n",
            "         [ 0.0668,  0.0524, -0.1930,  ..., -0.1169,  0.0526,  0.0013]],\n",
            "\n",
            "        [[ 0.1552, -0.0691,  0.1282,  ..., -0.3882, -0.0892,  0.3970],\n",
            "         [ 0.0806, -0.0230,  0.0711,  ..., -0.3024, -0.0678,  0.3348],\n",
            "         [ 0.0421,  0.0154,  0.0273,  ..., -0.2194, -0.0718,  0.2057],\n",
            "         ...,\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [-0.1769,  0.1432, -0.1047,  ..., -0.0050, -0.0461, -0.0729],\n",
            "         [ 0.0219,  0.0707, -0.0509,  ..., -0.0818, -0.0313,  0.0428]],\n",
            "\n",
            "        [[ 0.0309, -0.0395,  0.0753,  ..., -0.3547, -0.0171,  0.2371],\n",
            "         [-0.0107,  0.0277,  0.0688,  ..., -0.2770, -0.0218,  0.1914],\n",
            "         [-0.0202,  0.0662,  0.0336,  ..., -0.1868, -0.0391,  0.1143],\n",
            "         ...,\n",
            "         [-0.0031,  0.0702, -0.0906,  ...,  0.0620, -0.1632, -0.0467],\n",
            "         [-0.0045,  0.0659, -0.0677,  ..., -0.0307, -0.1592, -0.0452],\n",
            "         [-0.0687,  0.0888,  0.0067,  ..., -0.0175, -0.1665, -0.0698]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 2.1589e-02,  1.8286e-01, -9.8974e-02,  ..., -4.4267e-02,\n",
            "          -4.8027e-02, -7.9747e-02],\n",
            "         [ 1.0979e-02,  1.6050e-01, -1.1617e-01,  ..., -5.3625e-02,\n",
            "          -5.3864e-02, -7.0921e-02],\n",
            "         [ 3.4383e-03,  1.5486e-01, -1.1330e-01,  ..., -5.3102e-02,\n",
            "          -5.4762e-02, -6.5992e-02],\n",
            "         ...,\n",
            "         [ 5.6183e-03,  1.3523e-01, -9.6390e-02,  ..., -6.1194e-02,\n",
            "          -6.2959e-02, -5.8787e-02],\n",
            "         [-1.2439e-01,  1.2804e-01, -1.8817e-01,  ...,  1.4490e-02,\n",
            "           3.6508e-02, -1.3201e-01],\n",
            "         [-1.2094e-01,  8.8999e-02, -1.6587e-01,  ..., -1.4090e-02,\n",
            "           1.8292e-02, -1.2121e-01]],\n",
            "\n",
            "        [[ 3.3559e-03,  1.2099e-01,  1.2397e-03,  ..., -4.6964e-02,\n",
            "          -4.2799e-02, -7.3355e-02],\n",
            "         [ 1.3226e-02,  1.2397e-01, -3.8987e-02,  ..., -6.3298e-02,\n",
            "          -5.3564e-02, -7.3398e-02],\n",
            "         [ 1.0818e-02,  1.3171e-01, -6.1647e-02,  ..., -6.4811e-02,\n",
            "          -5.8031e-02, -7.1060e-02],\n",
            "         ...,\n",
            "         [-3.0590e-02,  9.2707e-02, -2.2080e-01,  ..., -1.4552e-02,\n",
            "           7.1728e-02, -8.8008e-02],\n",
            "         [ 2.9355e-02,  1.2381e-01, -1.8794e-01,  ..., -2.6539e-02,\n",
            "          -2.1506e-02, -5.3381e-02],\n",
            "         [-2.9026e-02,  1.3372e-01, -2.0929e-01,  ..., -3.8980e-02,\n",
            "           3.5429e-02, -7.4369e-02]],\n",
            "\n",
            "        [[ 2.3867e-02,  4.9007e-02, -4.3200e-02,  ..., -6.8785e-02,\n",
            "          -6.8075e-02, -6.1163e-02],\n",
            "         [ 1.8163e-02,  8.2488e-02, -5.3098e-02,  ..., -7.6584e-02,\n",
            "          -6.5917e-02, -6.1923e-02],\n",
            "         [ 1.4427e-02,  1.0484e-01, -6.3190e-02,  ..., -7.3480e-02,\n",
            "          -6.7992e-02, -5.9627e-02],\n",
            "         ...,\n",
            "         [ 9.7001e-02, -9.0236e-01,  1.7262e-03,  ..., -3.5405e-01,\n",
            "           1.8934e-01,  3.3272e-01],\n",
            "         [ 1.3826e-01, -9.1341e-01,  1.6385e-02,  ..., -3.6720e-01,\n",
            "           7.7008e-02,  3.0970e-01],\n",
            "         [ 1.8788e-01, -9.2438e-01,  5.8483e-03,  ..., -5.6539e-01,\n",
            "           1.0837e-01,  3.3138e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 2.7910e-02,  1.3528e-01, -1.2143e-01,  ..., -7.6379e-02,\n",
            "          -4.9151e-02, -2.5530e-02],\n",
            "         [ 1.8611e-02,  1.5955e-01, -1.0177e-01,  ..., -5.9684e-02,\n",
            "          -7.0332e-02, -5.1067e-02],\n",
            "         [ 1.3205e-02,  1.6125e-01, -9.5388e-02,  ..., -5.1925e-02,\n",
            "          -7.0273e-02, -6.6681e-02],\n",
            "         ...,\n",
            "         [-4.5786e-02, -4.7032e-01, -7.4238e-02,  ..., -1.0505e-01,\n",
            "           1.5795e-01,  2.6475e-02],\n",
            "         [ 2.1944e-02, -6.3152e-01, -6.4657e-02,  ..., -1.2709e-01,\n",
            "           1.8267e-01, -2.6022e-02],\n",
            "         [ 7.8941e-02, -6.3387e-01, -5.4436e-02,  ..., -1.5826e-01,\n",
            "           1.2764e-01, -1.0682e-02]],\n",
            "\n",
            "        [[ 9.0713e-03,  1.5423e-01, -1.0965e-02,  ..., -7.5199e-02,\n",
            "          -5.0542e-02,  6.5764e-02],\n",
            "         [ 4.6588e-04,  1.2790e-01, -3.5985e-02,  ..., -7.8613e-02,\n",
            "          -3.9103e-02,  1.4749e-02],\n",
            "         [-1.9670e-03,  1.3280e-01, -5.6501e-02,  ..., -6.5159e-02,\n",
            "          -4.3766e-02, -2.3284e-02],\n",
            "         ...,\n",
            "         [-8.7255e-02,  1.4924e-01, -1.5047e-02,  ..., -4.8881e-02,\n",
            "           2.9098e-02, -4.4823e-02],\n",
            "         [ 1.0316e-01,  7.8187e-03, -5.4590e-02,  ..., -2.1100e-01,\n",
            "          -8.8353e-02,  3.1929e-02],\n",
            "         [ 1.2497e-01, -7.9555e-02, -4.7398e-02,  ..., -3.4886e-01,\n",
            "          -4.7757e-02,  1.1591e-01]],\n",
            "\n",
            "        [[-8.2810e-03,  7.6170e-02,  1.1896e-02,  ..., -4.4882e-02,\n",
            "          -1.4237e-01, -4.5900e-02],\n",
            "         [ 8.7350e-03,  7.4649e-02, -9.5428e-03,  ..., -7.6657e-02,\n",
            "          -9.6367e-02, -4.8689e-02],\n",
            "         [ 1.2822e-02,  8.8421e-02, -2.7334e-02,  ..., -8.3900e-02,\n",
            "          -7.8625e-02, -4.8627e-02],\n",
            "         ...,\n",
            "         [-1.0501e-02, -2.5999e-01,  4.2334e-02,  ..., -3.0577e-01,\n",
            "           1.2192e-01,  7.2814e-02],\n",
            "         [ 1.0039e-01, -3.0678e-01,  3.8318e-03,  ..., -3.6915e-01,\n",
            "           9.9968e-04,  1.4983e-01],\n",
            "         [ 1.0301e-01, -4.3770e-01, -3.0099e-02,  ..., -3.9112e-01,\n",
            "           8.1565e-02,  2.1687e-01]]], device='cuda:0',\n",
            "       grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 1.0488e-02,  1.2317e-01, -9.2039e-02,  ..., -3.9948e-02,\n",
            "          -9.8501e-02, -8.8137e-02],\n",
            "         [ 1.1505e-02,  1.3422e-01, -9.4500e-02,  ..., -6.4867e-02,\n",
            "          -8.3790e-02, -8.5160e-02],\n",
            "         [ 8.2223e-03,  1.3734e-01, -9.2811e-02,  ..., -6.7586e-02,\n",
            "          -7.1391e-02, -7.3648e-02],\n",
            "         ...,\n",
            "         [ 1.1515e-01, -1.4476e-01,  7.5827e-02,  ..., -2.9422e-01,\n",
            "           3.4999e-02,  1.2927e-01],\n",
            "         [ 5.6250e-02, -2.1963e-01,  6.6153e-02,  ..., -2.2462e-01,\n",
            "           7.7155e-02,  9.1301e-02],\n",
            "         [ 1.8167e-02, -6.6360e-02,  7.2182e-02,  ..., -2.0473e-01,\n",
            "          -7.9390e-02,  8.8174e-04]],\n",
            "\n",
            "        [[ 4.2072e-02,  1.6820e-01, -1.2311e-01,  ..., -2.5165e-02,\n",
            "          -7.7534e-02, -5.8808e-02],\n",
            "         [ 3.8295e-02,  1.6987e-01, -1.1081e-01,  ..., -3.8048e-02,\n",
            "          -8.1566e-02, -6.6676e-02],\n",
            "         [ 2.5919e-02,  1.6297e-01, -1.0302e-01,  ..., -4.5100e-02,\n",
            "          -7.4169e-02, -7.0755e-02],\n",
            "         ...,\n",
            "         [ 6.1808e-02,  1.1972e-01, -9.7347e-03,  ..., -1.0570e-01,\n",
            "          -2.2355e-01, -2.4562e-03],\n",
            "         [ 5.5166e-03,  1.8042e-01,  2.4634e-02,  ...,  1.3455e-03,\n",
            "          -2.8017e-01, -2.7994e-03],\n",
            "         [ 3.5690e-02,  1.1773e-01,  2.7025e-03,  ..., -1.0498e-01,\n",
            "          -2.2961e-01,  1.5587e-02]],\n",
            "\n",
            "        [[ 1.1654e-02, -9.1289e-01,  3.1211e-02,  ..., -3.3999e-01,\n",
            "          -4.9493e-02,  3.7307e-01],\n",
            "         [-1.2722e-03, -7.7254e-01,  5.3697e-02,  ..., -3.5358e-01,\n",
            "          -7.1569e-02,  3.4572e-01],\n",
            "         [-1.3236e-02, -6.0300e-01,  5.2105e-02,  ..., -2.7063e-01,\n",
            "          -6.9835e-02,  3.3409e-01],\n",
            "         ...,\n",
            "         [ 7.0052e-02, -3.1052e-02,  7.1895e-02,  ..., -3.6065e-01,\n",
            "          -7.6247e-02,  1.4502e-01],\n",
            "         [ 1.2315e-01, -2.2026e-02,  6.6264e-02,  ..., -4.0428e-01,\n",
            "          -1.1077e-01,  1.2822e-01],\n",
            "         [ 9.7143e-02,  3.1581e-02,  8.2342e-02,  ..., -4.0296e-01,\n",
            "          -7.7156e-02,  9.0247e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 3.6226e-03, -5.9183e-01, -2.4817e-02,  ..., -8.2757e-02,\n",
            "          -3.4849e-02, -4.4480e-02],\n",
            "         [ 1.2256e-02, -3.1250e-01, -3.3094e-02,  ..., -7.4962e-02,\n",
            "          -8.0144e-02, -5.8779e-02],\n",
            "         [ 1.7981e-02, -7.6022e-02, -4.7336e-02,  ..., -6.4375e-02,\n",
            "          -8.5688e-02, -7.0496e-02],\n",
            "         ...,\n",
            "         [ 5.6183e-03,  1.3523e-01, -9.6390e-02,  ..., -6.1194e-02,\n",
            "          -6.2959e-02, -5.8787e-02],\n",
            "         [ 5.6183e-03,  1.3523e-01, -9.6390e-02,  ..., -6.1194e-02,\n",
            "          -6.2959e-02, -5.8787e-02],\n",
            "         [ 5.6183e-03,  1.3523e-01, -9.6390e-02,  ..., -6.1194e-02,\n",
            "          -6.2959e-02, -5.8787e-02]],\n",
            "\n",
            "        [[ 3.6637e-02,  3.5768e-02, -4.1684e-02,  ..., -2.0899e-01,\n",
            "          -4.9983e-02,  9.4649e-02],\n",
            "         [-4.3328e-03,  8.5342e-02, -5.0213e-02,  ..., -1.5205e-01,\n",
            "          -5.0177e-02,  2.6902e-02],\n",
            "         [-7.5666e-03,  1.1290e-01, -6.0436e-02,  ..., -9.8404e-02,\n",
            "          -5.9617e-02, -2.1565e-02],\n",
            "         ...,\n",
            "         [-4.8410e-02,  5.8921e-03, -3.1124e-02,  ..., -6.0304e-04,\n",
            "          -1.8440e-01, -6.9420e-02],\n",
            "         [-8.8879e-02,  6.9196e-02,  1.6832e-02,  ...,  6.6703e-02,\n",
            "          -2.3260e-01, -1.1403e-01],\n",
            "         [-2.5589e-02,  2.4711e-02, -2.4409e-02,  ...,  2.7887e-02,\n",
            "          -3.0288e-01, -9.3313e-02]],\n",
            "\n",
            "        [[-2.5166e-03, -2.8175e-01, -5.4721e-04,  ..., -2.4439e-01,\n",
            "          -7.1422e-02,  1.8405e-01],\n",
            "         [-8.0568e-03, -7.7009e-02, -1.0933e-02,  ..., -2.0146e-01,\n",
            "          -7.7563e-02,  7.4974e-02],\n",
            "         [-2.6243e-03,  3.8426e-02, -3.0628e-02,  ..., -1.3320e-01,\n",
            "          -7.5958e-02,  2.3268e-04],\n",
            "         ...,\n",
            "         [-2.7689e-02,  1.2631e-01, -1.1274e-01,  ...,  1.1059e-01,\n",
            "          -1.4330e-01, -1.1550e-01],\n",
            "         [-8.6315e-03,  1.1840e-01,  2.5867e-02,  ...,  1.9566e-01,\n",
            "          -2.5840e-01, -1.7268e-01],\n",
            "         [-5.3841e-02,  9.4810e-02,  5.1430e-02,  ...,  2.2460e-01,\n",
            "          -3.2280e-01, -1.8855e-01]]], device='cuda:0',\n",
            "       grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 0.0096,  0.0238,  0.0216,  ..., -0.1631, -0.0606, -0.0220],\n",
            "         [ 0.0029,  0.0758, -0.0218,  ..., -0.1115, -0.0622, -0.0390],\n",
            "         [ 0.0064,  0.1001, -0.0519,  ..., -0.0843, -0.0699, -0.0472],\n",
            "         ...,\n",
            "         [ 0.0293,  0.1108, -0.0422,  ...,  0.0210, -0.2917, -0.0531],\n",
            "         [ 0.0695,  0.0773,  0.0101,  ..., -0.0046, -0.3231, -0.0562],\n",
            "         [ 0.0125,  0.0849, -0.0095,  ...,  0.0174, -0.3722, -0.0757]],\n",
            "\n",
            "        [[ 0.0501,  0.0985, -0.0263,  ..., -0.1350, -0.1279, -0.0039],\n",
            "         [ 0.0311,  0.1010, -0.0469,  ..., -0.1233, -0.0833, -0.0307],\n",
            "         [ 0.0196,  0.1080, -0.0594,  ..., -0.1023, -0.0732, -0.0425],\n",
            "         ...,\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [ 0.0011,  0.1423, -0.0887,  ..., -0.0967, -0.0085,  0.0414]],\n",
            "\n",
            "        [[ 0.0238,  0.0859,  0.0557,  ..., -0.2772, -0.0470,  0.0264],\n",
            "         [ 0.0036,  0.0996,  0.0179,  ..., -0.1886, -0.0610, -0.0084],\n",
            "         [ 0.0041,  0.1031, -0.0202,  ..., -0.1316, -0.0724, -0.0290],\n",
            "         ...,\n",
            "         [ 0.0568,  0.2007, -0.0381,  ..., -0.1636, -0.0766, -0.0731],\n",
            "         [ 0.0548,  0.2230,  0.0109,  ..., -0.2482, -0.0316, -0.0501],\n",
            "         [ 0.1136,  0.0374,  0.0381,  ..., -0.2516,  0.0541,  0.0672]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         ...,\n",
            "         [-0.0658,  0.0247, -0.0536,  ...,  0.0048, -0.2411, -0.0806],\n",
            "         [-0.1207,  0.0312, -0.0353,  ..., -0.0087, -0.1862, -0.1019],\n",
            "         [-0.0122,  0.0352, -0.0348,  ..., -0.0288, -0.1936, -0.0236]],\n",
            "\n",
            "        [[ 0.0161,  0.0226, -0.0308,  ..., -0.0552, -0.2304, -0.0538],\n",
            "         [ 0.0230,  0.0470, -0.0285,  ..., -0.0907, -0.1845, -0.0442],\n",
            "         [ 0.0242,  0.0694, -0.0335,  ..., -0.1030, -0.1438, -0.0362],\n",
            "         ...,\n",
            "         [ 0.0742, -0.1204, -0.1479,  ..., -0.2229,  0.0814,  0.1757],\n",
            "         [-0.0121,  0.0125, -0.0272,  ..., -0.1564,  0.0227,  0.1085],\n",
            "         [-0.0173, -0.1569, -0.0194,  ..., -0.1813,  0.0644,  0.0871]],\n",
            "\n",
            "        [[ 0.0232,  0.0271, -0.0147,  ...,  0.0272, -0.2756, -0.0579],\n",
            "         [ 0.0246,  0.0314, -0.0226,  ..., -0.0811, -0.2351, -0.0554],\n",
            "         [ 0.0263,  0.0510, -0.0264,  ..., -0.1116, -0.1759, -0.0455],\n",
            "         ...,\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [-0.0172,  0.1154, -0.0788,  ..., -0.0460, -0.0827, -0.0813]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 0.0499,  0.0505, -0.0375,  ..., -0.0877, -0.2368, -0.0572],\n",
            "         [ 0.0398,  0.0727, -0.0437,  ..., -0.1106, -0.1529, -0.0608],\n",
            "         [ 0.0316,  0.0895, -0.0502,  ..., -0.1068, -0.1118, -0.0557],\n",
            "         ...,\n",
            "         [ 0.0268, -0.0074, -0.0936,  ..., -0.3648,  0.0685,  0.1306],\n",
            "         [-0.0439,  0.0357, -0.0070,  ..., -0.2404, -0.0241,  0.1169],\n",
            "         [ 0.0599, -0.0642, -0.0017,  ..., -0.3326, -0.0195,  0.1348]],\n",
            "\n",
            "        [[ 0.0063,  0.1701, -0.0522,  ..., -0.0901, -0.0351,  0.0448],\n",
            "         [ 0.0008,  0.1424, -0.0682,  ..., -0.0850, -0.0343,  0.0043],\n",
            "         [-0.0024,  0.1400, -0.0794,  ..., -0.0690, -0.0409, -0.0272],\n",
            "         ...,\n",
            "         [-0.0062,  0.0358,  0.0298,  ...,  0.0881, -0.2837, -0.0498],\n",
            "         [-0.0026,  0.0306, -0.0237,  ...,  0.0462, -0.3259, -0.1131],\n",
            "         [-0.0164,  0.0309, -0.0052,  ...,  0.0430, -0.3178, -0.0782]],\n",
            "\n",
            "        [[-0.0062,  0.1074,  0.0436,  ..., -0.1447,  0.0186,  0.0794],\n",
            "         [-0.0200,  0.0926,  0.0072,  ..., -0.1151, -0.0203,  0.0352],\n",
            "         [-0.0149,  0.1025, -0.0328,  ..., -0.0863, -0.0441, -0.0043],\n",
            "         ...,\n",
            "         [-0.0495,  0.0626, -0.1816,  ..., -0.0678,  0.1273, -0.1012],\n",
            "         [-0.0127,  0.0328, -0.1552,  ..., -0.0296,  0.1047, -0.0650],\n",
            "         [ 0.0455, -0.0977, -0.2505,  ..., -0.0538,  0.2025, -0.0408]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0273,  0.0609, -0.0216,  ..., -0.0777, -0.1631, -0.0092],\n",
            "         [ 0.0240,  0.0852, -0.0387,  ..., -0.0950, -0.1178, -0.0233],\n",
            "         [ 0.0180,  0.1036, -0.0526,  ..., -0.0887, -0.0911, -0.0331],\n",
            "         ...,\n",
            "         [ 0.0462, -0.3566, -0.1373,  ..., -0.1819,  0.0741, -0.0140],\n",
            "         [ 0.0169, -0.2733, -0.0743,  ..., -0.1144,  0.0546, -0.0746],\n",
            "         [ 0.0184, -0.1765, -0.0565,  ..., -0.1099,  0.0159, -0.1156]],\n",
            "\n",
            "        [[-0.0083, -0.0420, -0.0159,  ..., -0.1256, -0.0294,  0.0521],\n",
            "         [-0.0009,  0.0545, -0.0360,  ..., -0.1021, -0.0532, -0.0070],\n",
            "         [ 0.0026,  0.1045, -0.0565,  ..., -0.0785, -0.0595, -0.0433],\n",
            "         ...,\n",
            "         [-0.0812,  0.0706, -0.1464,  ..., -0.0708, -0.0411, -0.1182],\n",
            "         [-0.1565,  0.0672, -0.1041,  ..., -0.0266,  0.0157, -0.0755],\n",
            "         [-0.2004,  0.0004, -0.1774,  ..., -0.0248,  0.0966, -0.0938]],\n",
            "\n",
            "        [[ 0.0166,  0.1070, -0.0716,  ..., -0.0532, -0.0920, -0.0630],\n",
            "         [ 0.0149,  0.1177, -0.0790,  ..., -0.0625, -0.0765, -0.0628],\n",
            "         [ 0.0112,  0.1261, -0.0840,  ..., -0.0636, -0.0673, -0.0610],\n",
            "         ...,\n",
            "         [ 0.0288,  0.0541, -0.0039,  ...,  0.0893, -0.3538, -0.0244],\n",
            "         [ 0.0597,  0.0819,  0.0223,  ...,  0.1062, -0.3747, -0.0487],\n",
            "         [ 0.0315,  0.0492, -0.0089,  ...,  0.1137, -0.3326, -0.0735]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 5.4519e-03,  3.7623e-02, -8.3845e-03,  ..., -2.1337e-01,\n",
            "          -3.1712e-02,  9.0011e-02],\n",
            "         [-1.1829e-02,  8.7304e-02, -3.6067e-02,  ..., -1.5580e-01,\n",
            "          -4.2657e-02,  2.6613e-02],\n",
            "         [-9.7243e-03,  1.1087e-01, -6.0272e-02,  ..., -1.0761e-01,\n",
            "          -5.3960e-02, -1.4570e-02],\n",
            "         ...,\n",
            "         [-4.1804e-02,  1.4469e-02, -2.9087e-02,  ..., -4.3920e-02,\n",
            "          -1.1021e-01, -1.1261e-01],\n",
            "         [-1.0804e-01, -1.1246e-02, -7.5579e-02,  ..., -8.9530e-03,\n",
            "           1.6001e-02, -1.5541e-01],\n",
            "         [-8.0557e-02,  1.3015e-02, -2.6340e-02,  ...,  4.8997e-02,\n",
            "          -2.8112e-02, -1.0278e-01]],\n",
            "\n",
            "        [[ 2.4882e-02,  2.9824e-02,  5.2725e-03,  ..., -4.4740e-02,\n",
            "          -2.2798e-01, -4.2237e-02],\n",
            "         [ 2.8223e-02,  5.1787e-02, -8.7647e-03,  ..., -8.8603e-02,\n",
            "          -1.8450e-01, -4.0786e-02],\n",
            "         [ 2.5808e-02,  7.3424e-02, -2.1679e-02,  ..., -1.0048e-01,\n",
            "          -1.3799e-01, -3.7062e-02],\n",
            "         ...,\n",
            "         [-3.0176e-02,  1.0572e-02,  6.8671e-04,  ...,  3.6530e-02,\n",
            "          -2.9802e-01, -1.4665e-01],\n",
            "         [-5.5535e-02,  5.0030e-02,  9.0304e-03,  ...,  7.5061e-02,\n",
            "          -3.3496e-01, -1.4381e-01],\n",
            "         [-1.0154e-01,  6.6774e-02, -4.5189e-03,  ...,  6.4496e-02,\n",
            "          -2.9998e-01, -1.0084e-01]],\n",
            "\n",
            "        [[ 3.7157e-02,  5.3905e-02, -8.7201e-02,  ..., -2.3044e-02,\n",
            "          -1.1570e-01, -7.3540e-02],\n",
            "         [ 4.0261e-02,  1.1942e-01, -9.1723e-02,  ..., -4.1368e-02,\n",
            "          -1.1067e-01, -8.0918e-02],\n",
            "         [ 3.4756e-02,  1.5287e-01, -8.6278e-02,  ..., -4.3718e-02,\n",
            "          -9.9483e-02, -8.3860e-02],\n",
            "         ...,\n",
            "         [-8.3152e-02,  5.8595e-02, -1.4140e-02,  ...,  8.6622e-02,\n",
            "          -3.0774e-01, -1.7582e-01],\n",
            "         [-2.3943e-02,  7.7069e-02, -2.2559e-02,  ...,  1.1714e-01,\n",
            "          -4.1961e-01, -1.4474e-01],\n",
            "         [-7.4197e-02,  8.6321e-02,  2.3948e-02,  ...,  1.0435e-01,\n",
            "          -2.8940e-01, -1.2218e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 8.8888e-03,  1.6428e-02, -4.3861e-02,  ..., -6.9662e-02,\n",
            "          -7.5228e-02, -9.2186e-02],\n",
            "         [ 2.4083e-03,  8.8310e-02, -6.0655e-02,  ..., -6.5987e-02,\n",
            "          -7.5707e-02, -9.1356e-02],\n",
            "         [ 4.6547e-03,  1.2069e-01, -7.2332e-02,  ..., -5.9611e-02,\n",
            "          -7.6077e-02, -8.8407e-02],\n",
            "         ...,\n",
            "         [ 2.1727e-01, -1.0821e-01, -4.4660e-03,  ..., -4.0341e-01,\n",
            "          -3.6279e-02,  2.3506e-01],\n",
            "         [ 2.2346e-01, -1.9116e-01,  1.0101e-01,  ..., -5.5562e-01,\n",
            "          -4.2829e-02,  5.2166e-01],\n",
            "         [ 2.6920e-01, -3.9760e-01,  1.7145e-01,  ..., -7.5383e-01,\n",
            "           3.3281e-03,  6.7297e-01]],\n",
            "\n",
            "        [[-2.8987e-02,  8.4257e-02, -8.7753e-02,  ..., -5.2257e-03,\n",
            "          -6.9426e-02, -7.7489e-02],\n",
            "         [ 8.5196e-03,  1.1547e-01, -9.5262e-02,  ..., -2.6380e-02,\n",
            "          -8.2606e-02, -7.9308e-02],\n",
            "         [ 1.6428e-02,  1.3541e-01, -9.4490e-02,  ..., -4.0556e-02,\n",
            "          -7.6649e-02, -7.8122e-02],\n",
            "         ...,\n",
            "         [ 1.4479e-02,  2.1924e-02,  6.5110e-02,  ..., -1.4348e-01,\n",
            "          -2.6929e-01, -1.3425e-01],\n",
            "         [ 2.1921e-02,  4.8778e-02,  8.1741e-02,  ..., -1.0482e-01,\n",
            "          -2.6067e-01, -1.4189e-01],\n",
            "         [-2.9567e-02,  6.4386e-02,  5.7083e-02,  ...,  1.3419e-01,\n",
            "          -2.7449e-01, -3.2923e-01]],\n",
            "\n",
            "        [[ 3.8815e-02,  2.9316e-02, -2.3980e-02,  ..., -1.1776e-02,\n",
            "          -2.5087e-01, -4.0683e-02],\n",
            "         [ 3.6406e-02,  4.1043e-02, -2.4303e-02,  ..., -7.8804e-02,\n",
            "          -2.0879e-01, -3.8104e-02],\n",
            "         [ 3.2818e-02,  6.1148e-02, -3.0524e-02,  ..., -1.0184e-01,\n",
            "          -1.5769e-01, -3.5949e-02],\n",
            "         ...,\n",
            "         [ 8.3906e-02,  1.2202e-01, -4.8885e-02,  ...,  5.4968e-03,\n",
            "          -1.9801e-01, -9.3079e-02],\n",
            "         [ 1.6306e-01,  6.8782e-02, -1.7241e-01,  ..., -8.9104e-02,\n",
            "          -8.6096e-02, -6.0656e-02],\n",
            "         [ 1.5372e-01,  1.1180e-01, -8.3847e-02,  ..., -9.3545e-02,\n",
            "          -5.4159e-02, -3.3167e-02]]], device='cuda:0',\n",
            "       grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 6.3047e-03,  5.4045e-02, -2.1323e-02,  ...,  1.8822e-02,\n",
            "          -8.5230e-02, -5.9028e-02],\n",
            "         [ 2.4918e-02,  8.6705e-02, -3.9433e-02,  ..., -2.4090e-02,\n",
            "          -8.2027e-02, -5.7381e-02],\n",
            "         [ 2.3741e-02,  1.1096e-01, -5.4165e-02,  ..., -4.6584e-02,\n",
            "          -7.3678e-02, -5.7545e-02],\n",
            "         ...,\n",
            "         [-3.9446e-02,  1.0425e-03, -6.3749e-02,  ..., -1.6624e-01,\n",
            "           9.3755e-02, -1.4049e-01],\n",
            "         [-3.8762e-02,  9.5001e-02, -6.1585e-02,  ..., -1.1128e-01,\n",
            "          -5.7338e-02, -1.6032e-01],\n",
            "         [-3.2702e-02,  3.0379e-02, -9.8626e-02,  ..., -1.2222e-01,\n",
            "          -5.8675e-02, -9.1765e-02]],\n",
            "\n",
            "        [[-4.9021e-02,  9.5913e-02, -2.9443e-03,  ...,  8.7904e-02,\n",
            "          -2.8096e-01, -5.2234e-02],\n",
            "         [-4.9322e-02,  5.8666e-02,  1.8009e-02,  ...,  1.3199e-01,\n",
            "          -2.5403e-01, -1.0282e-01],\n",
            "         [-8.2926e-03,  7.3354e-03,  1.0406e-02,  ...,  1.6956e-01,\n",
            "          -3.2302e-01, -9.2153e-02],\n",
            "         ...,\n",
            "         [ 3.8142e-02, -4.3354e-02, -1.0489e-01,  ..., -1.7041e-02,\n",
            "          -3.5558e-02, -1.4813e-01],\n",
            "         [ 1.5690e-02, -6.2753e-02, -9.3942e-02,  ..., -3.6950e-02,\n",
            "          -3.6776e-02, -2.2428e-01],\n",
            "         [ 4.2452e-02, -8.7994e-02, -6.8627e-02,  ..., -6.5415e-02,\n",
            "          -8.7470e-02, -2.0303e-01]],\n",
            "\n",
            "        [[-1.4815e-03,  4.5646e-02, -1.0607e-02,  ..., -1.9778e-02,\n",
            "          -2.3342e-01, -6.9672e-02],\n",
            "         [ 1.3433e-02,  5.9667e-02, -2.0512e-02,  ..., -7.8635e-02,\n",
            "          -1.8564e-01, -5.9220e-02],\n",
            "         [ 2.0949e-02,  7.5415e-02, -2.8085e-02,  ..., -9.8643e-02,\n",
            "          -1.4272e-01, -4.8237e-02],\n",
            "         ...,\n",
            "         [-1.3551e-01,  1.5625e-01,  5.3316e-03,  ...,  9.2934e-02,\n",
            "          -2.7916e-01, -1.4321e-01],\n",
            "         [-1.4000e-01,  6.6949e-02,  3.9316e-03,  ...,  1.0749e-01,\n",
            "          -4.0753e-01, -1.2410e-01],\n",
            "         [-1.2929e-01,  3.9770e-02, -9.0379e-03,  ...,  7.4264e-02,\n",
            "          -4.1518e-01, -9.0880e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.1972e-01, -2.7138e-01,  1.5581e-01,  ..., -5.7096e-01,\n",
            "          -8.0438e-03,  5.2848e-01],\n",
            "         [ 3.4423e-02, -1.1687e-01,  1.6736e-01,  ..., -5.1025e-01,\n",
            "          -1.6615e-02,  4.2174e-01],\n",
            "         [-2.9981e-02, -9.1713e-03,  1.2534e-01,  ..., -3.8064e-01,\n",
            "          -2.5647e-02,  3.4770e-01],\n",
            "         ...,\n",
            "         [ 3.0867e-02,  9.6568e-02, -8.5020e-02,  ...,  1.2940e-02,\n",
            "          -1.8562e-01, -9.9350e-02],\n",
            "         [-5.4179e-02,  6.4900e-02, -9.4848e-02,  ...,  1.5374e-02,\n",
            "          -1.6232e-01, -7.0889e-02],\n",
            "         [ 6.9753e-03,  3.4295e-02, -2.3866e-02,  ...,  1.7798e-02,\n",
            "          -2.0389e-01, -2.0618e-02]],\n",
            "\n",
            "        [[ 9.8960e-03,  2.4557e-02, -1.6833e-02,  ...,  1.5289e-02,\n",
            "          -1.9135e-01, -1.6812e-01],\n",
            "         [ 1.9997e-02,  4.0455e-02, -2.9038e-02,  ..., -6.2279e-02,\n",
            "          -1.6116e-01, -1.0556e-01],\n",
            "         [ 2.4884e-02,  5.9175e-02, -3.5449e-02,  ..., -9.2426e-02,\n",
            "          -1.3419e-01, -7.3806e-02],\n",
            "         ...,\n",
            "         [-2.3315e-02,  1.2102e-01, -4.7250e-02,  ..., -1.1650e-02,\n",
            "          -1.5507e-01, -7.4554e-02],\n",
            "         [-7.2255e-02,  8.2143e-02, -6.7559e-02,  ..., -2.9927e-02,\n",
            "          -1.1556e-01, -1.3960e-01],\n",
            "         [-6.7245e-02,  4.9744e-02, -5.5300e-02,  ..., -6.3677e-02,\n",
            "          -1.4722e-01, -1.9874e-01]],\n",
            "\n",
            "        [[ 9.7572e-02,  1.5559e-01, -8.6268e-02,  ..., -1.0053e-01,\n",
            "          -4.7928e-02, -3.7436e-02],\n",
            "         [ 4.4748e-02,  1.6106e-01, -9.2155e-02,  ..., -8.1410e-02,\n",
            "          -4.9638e-02, -5.0640e-02],\n",
            "         [ 2.0429e-02,  1.5281e-01, -9.6672e-02,  ..., -6.7330e-02,\n",
            "          -5.6921e-02, -5.7428e-02],\n",
            "         ...,\n",
            "         [ 3.7759e-02,  1.0094e-01, -5.1217e-02,  ..., -2.3882e-01,\n",
            "           8.0324e-02, -1.3998e-03],\n",
            "         [ 5.5840e-03,  2.2882e-01, -2.5002e-03,  ..., -1.7054e-01,\n",
            "           8.4063e-02, -6.9345e-02],\n",
            "         [-6.6705e-02,  2.1251e-01, -5.1150e-05,  ..., -1.4895e-01,\n",
            "           8.7374e-02, -1.3674e-01]]], device='cuda:0',\n",
            "       grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[-7.6777e-03,  1.1631e-01, -6.7363e-02,  ..., -7.5474e-02,\n",
            "          -7.6772e-02, -9.8877e-02],\n",
            "         [-4.8466e-03,  1.2687e-01, -8.4719e-02,  ..., -7.1110e-02,\n",
            "          -6.3998e-02, -9.0136e-02],\n",
            "         [-7.7634e-05,  1.3600e-01, -8.9506e-02,  ..., -6.3179e-02,\n",
            "          -6.2582e-02, -8.0432e-02],\n",
            "         ...,\n",
            "         [-1.3825e-01,  9.9189e-02,  4.3422e-02,  ...,  7.0129e-02,\n",
            "          -1.7526e-01, -2.2030e-01],\n",
            "         [-5.8901e-03,  4.2052e-02, -6.3452e-03,  ...,  4.5117e-02,\n",
            "          -1.7857e-01, -1.4748e-01],\n",
            "         [ 2.5970e-02,  4.9929e-02, -1.2517e-02,  ...,  1.6588e-02,\n",
            "          -1.6777e-01, -1.1582e-01]],\n",
            "\n",
            "        [[ 2.8172e-02, -8.5395e-04, -6.5573e-02,  ..., -4.0113e-02,\n",
            "          -7.6310e-02, -1.3938e-01],\n",
            "         [ 2.5488e-02,  7.3492e-02, -7.4918e-02,  ..., -4.1048e-02,\n",
            "          -7.6521e-02, -1.1510e-01],\n",
            "         [ 2.2777e-02,  1.1116e-01, -8.0346e-02,  ..., -4.4944e-02,\n",
            "          -7.8114e-02, -9.6158e-02],\n",
            "         ...,\n",
            "         [ 5.7890e-02, -1.7844e-01,  6.7513e-02,  ..., -4.6039e-01,\n",
            "          -9.5988e-03,  3.6838e-01],\n",
            "         [ 7.5616e-02, -1.8661e-01,  1.0832e-01,  ..., -5.5590e-01,\n",
            "          -4.0212e-02,  2.6473e-01],\n",
            "         [ 1.8311e-01, -3.8145e-01,  8.8980e-02,  ..., -5.7541e-01,\n",
            "           5.9363e-02,  3.6040e-01]],\n",
            "\n",
            "        [[-2.8766e-02,  3.1937e-02, -1.8407e-02,  ...,  1.9279e-02,\n",
            "          -3.0516e-01, -5.4353e-02],\n",
            "         [-2.6899e-03,  2.5827e-02, -2.1372e-02,  ..., -5.9773e-02,\n",
            "          -2.9865e-01, -4.8589e-02],\n",
            "         [ 1.1234e-02,  3.3777e-02, -2.1581e-02,  ..., -1.0138e-01,\n",
            "          -2.5359e-01, -3.8467e-02],\n",
            "         ...,\n",
            "         [ 1.1849e-02,  8.9429e-02, -1.3533e-01,  ...,  2.0745e-02,\n",
            "          -1.0194e-01, -7.8583e-02],\n",
            "         [ 1.5066e-02,  8.9003e-02, -1.6348e-01,  ...,  5.0453e-02,\n",
            "          -1.2172e-01, -9.7820e-02],\n",
            "         [ 5.0845e-02,  8.6494e-02, -1.1935e-01,  ..., -7.6110e-03,\n",
            "          -1.4180e-01,  2.2266e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.2617e-02,  4.1269e-02, -5.0512e-02,  ..., -5.4830e-02,\n",
            "          -1.0522e-01, -5.1574e-02],\n",
            "         [ 9.5304e-03,  7.7228e-02, -5.3615e-02,  ..., -7.2295e-02,\n",
            "          -7.9595e-02, -6.0143e-02],\n",
            "         [ 1.1589e-02,  9.5951e-02, -5.9914e-02,  ..., -7.6920e-02,\n",
            "          -7.6300e-02, -5.6875e-02],\n",
            "         ...,\n",
            "         [ 1.1185e-02,  1.3414e-01, -2.4356e-01,  ...,  5.4326e-03,\n",
            "          -1.5282e-01, -7.9835e-02],\n",
            "         [ 1.6330e-02,  1.5240e-01, -3.3335e-01,  ...,  9.0617e-03,\n",
            "          -1.0243e-01, -1.0302e-01],\n",
            "         [-7.1296e-04,  1.5928e-01, -2.7369e-01,  ..., -1.5582e-02,\n",
            "          -8.6360e-02, -7.6402e-02]],\n",
            "\n",
            "        [[ 5.3254e-03,  7.3291e-02, -3.3882e-02,  ..., -6.5498e-02,\n",
            "          -1.0523e-01, -1.1497e-01],\n",
            "         [ 1.3604e-02,  1.0308e-01, -4.3997e-02,  ..., -6.7343e-02,\n",
            "          -7.5939e-02, -9.2373e-02],\n",
            "         [ 1.3416e-02,  1.1852e-01, -5.7463e-02,  ..., -6.4868e-02,\n",
            "          -6.6230e-02, -7.6987e-02],\n",
            "         ...,\n",
            "         [-1.1356e-01,  5.8888e-02,  2.3883e-02,  ...,  1.0557e-01,\n",
            "          -3.7546e-01, -1.2177e-01],\n",
            "         [-9.2311e-02,  3.6377e-02,  2.4111e-02,  ...,  1.1706e-01,\n",
            "          -3.9996e-01, -1.3899e-01],\n",
            "         [-3.2231e-02,  1.2377e-02,  1.9123e-02,  ...,  6.2685e-02,\n",
            "          -5.3425e-01, -8.2621e-02]],\n",
            "\n",
            "        [[-9.8658e-03,  1.9207e-01, -2.5818e-02,  ..., -8.5923e-02,\n",
            "          -4.4343e-02, -9.6510e-02],\n",
            "         [-5.6564e-03,  1.6921e-01, -5.7212e-02,  ..., -7.3773e-02,\n",
            "          -5.6002e-02, -8.6730e-02],\n",
            "         [-1.8566e-03,  1.5714e-01, -7.6081e-02,  ..., -6.3330e-02,\n",
            "          -6.0980e-02, -8.1429e-02],\n",
            "         ...,\n",
            "         [-2.0913e-03, -1.1808e-01, -1.7818e-01,  ..., -1.3298e-01,\n",
            "           1.2870e-01, -6.2782e-02],\n",
            "         [ 3.9712e-03, -1.6062e-01, -1.9119e-01,  ..., -1.2064e-01,\n",
            "           5.1585e-02, -7.9076e-02],\n",
            "         [ 4.7032e-02, -6.1591e-02, -1.6829e-01,  ..., -1.2721e-01,\n",
            "          -4.5128e-02, -9.7965e-02]]], device='cuda:0',\n",
            "       grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 4.0946e-02,  5.8858e-02, -5.1080e-02,  ..., -6.7565e-02,\n",
            "          -1.1274e-01, -5.8791e-02],\n",
            "         [ 3.3975e-02,  7.9197e-02, -6.4822e-02,  ..., -8.8010e-02,\n",
            "          -8.9720e-02, -5.0493e-02],\n",
            "         [ 2.6422e-02,  9.5823e-02, -7.1157e-02,  ..., -8.7336e-02,\n",
            "          -8.1867e-02, -4.7513e-02],\n",
            "         ...,\n",
            "         [ 1.0982e-01, -6.7329e-01,  1.2507e-01,  ..., -6.9543e-01,\n",
            "          -6.2909e-03,  5.1635e-01],\n",
            "         [ 1.1909e-01, -7.4385e-01,  1.5573e-01,  ..., -7.0836e-01,\n",
            "          -3.6356e-02,  5.4627e-01],\n",
            "         [ 1.7286e-01, -8.4398e-01,  1.8059e-01,  ..., -7.7903e-01,\n",
            "           1.9115e-02,  5.1141e-01]],\n",
            "\n",
            "        [[ 3.8260e-02, -1.5045e-01,  1.1389e-01,  ..., -4.0853e-01,\n",
            "          -1.3103e-02,  3.4660e-01],\n",
            "         [-9.4027e-03, -2.8833e-03,  9.7600e-02,  ..., -3.4892e-01,\n",
            "          -3.3520e-02,  2.8439e-01],\n",
            "         [-2.0557e-02,  6.1603e-02,  5.8606e-02,  ..., -2.4027e-01,\n",
            "          -4.6184e-02,  1.9433e-01],\n",
            "         ...,\n",
            "         [-8.3384e-03,  8.7048e-02, -1.6644e-01,  ..., -1.3016e-01,\n",
            "          -5.3542e-02, -4.6930e-02],\n",
            "         [ 3.8820e-02,  8.7479e-02, -1.9871e-01,  ..., -1.4167e-01,\n",
            "          -5.1698e-02, -3.6450e-03],\n",
            "         [ 2.8366e-02,  1.4737e-01, -1.9588e-01,  ..., -1.0610e-01,\n",
            "          -1.5284e-01, -1.8785e-02]],\n",
            "\n",
            "        [[ 5.0259e-02,  1.2709e-01, -8.5055e-02,  ..., -4.8376e-02,\n",
            "          -9.4116e-02,  1.6575e-02],\n",
            "         [ 3.3946e-02,  1.2360e-01, -8.8919e-02,  ..., -6.4892e-02,\n",
            "          -5.8136e-02, -1.4804e-02],\n",
            "         [ 2.0443e-02,  1.2971e-01, -8.9656e-02,  ..., -6.3087e-02,\n",
            "          -4.9440e-02, -3.5546e-02],\n",
            "         ...,\n",
            "         [-1.2824e-01,  2.7854e-02, -2.4575e-01,  ...,  1.1485e-03,\n",
            "           5.0547e-02, -7.8750e-02],\n",
            "         [-1.3777e-01,  6.3176e-02, -2.1167e-01,  ...,  2.8770e-02,\n",
            "           1.2109e-02, -1.0144e-01],\n",
            "         [-4.4624e-02,  4.2311e-02, -2.3734e-01,  ...,  6.0991e-02,\n",
            "          -6.5396e-02, -1.3507e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 3.6646e-02,  1.9539e-01, -1.6606e-01,  ..., -2.9746e-02,\n",
            "          -1.0788e-01, -6.4450e-02],\n",
            "         [ 3.0359e-02,  1.8194e-01, -1.3280e-01,  ..., -4.4534e-02,\n",
            "          -8.7640e-02, -6.9452e-02],\n",
            "         [ 2.1422e-02,  1.6686e-01, -1.1429e-01,  ..., -5.0932e-02,\n",
            "          -7.5317e-02, -6.9111e-02],\n",
            "         ...,\n",
            "         [ 2.1254e-01, -5.2846e-01,  1.3370e-01,  ..., -6.3731e-01,\n",
            "          -6.7825e-02,  5.0405e-01],\n",
            "         [ 2.2251e-01, -5.2860e-01,  1.5616e-01,  ..., -6.8886e-01,\n",
            "          -3.6178e-02,  4.7423e-01],\n",
            "         [ 1.8335e-01, -7.0267e-01,  1.1519e-01,  ..., -7.5278e-01,\n",
            "           1.5480e-02,  4.8351e-01]],\n",
            "\n",
            "        [[-2.4844e-02,  1.3819e-02,  6.5663e-03,  ...,  1.1459e-02,\n",
            "          -3.7450e-01, -7.8456e-02],\n",
            "         [-1.4676e-02,  4.4540e-03, -2.1631e-03,  ..., -3.8875e-02,\n",
            "          -3.5022e-01, -6.5742e-02],\n",
            "         [ 9.3931e-04, -2.8159e-03, -8.1460e-03,  ..., -8.7208e-02,\n",
            "          -3.4653e-01, -4.4917e-02],\n",
            "         ...,\n",
            "         [ 5.1383e-02,  2.6072e-02,  4.5396e-02,  ...,  2.5553e-01,\n",
            "          -3.3645e-01, -1.4915e-01],\n",
            "         [ 2.1956e-02,  1.8529e-02,  1.8841e-02,  ...,  2.6583e-01,\n",
            "          -3.4965e-01, -9.3543e-02],\n",
            "         [ 1.8791e-02,  4.6979e-02,  4.3352e-02,  ...,  2.5024e-01,\n",
            "          -2.9943e-01, -1.0473e-01]],\n",
            "\n",
            "        [[ 3.9685e-02,  6.0167e-02, -9.9354e-02,  ..., -8.0946e-02,\n",
            "          -9.5443e-02, -8.5574e-02],\n",
            "         [ 3.1858e-02,  1.1715e-01, -9.0371e-02,  ..., -6.8794e-02,\n",
            "          -8.6403e-02, -8.5866e-02],\n",
            "         [ 2.4040e-02,  1.4067e-01, -8.7717e-02,  ..., -5.8851e-02,\n",
            "          -7.9358e-02, -8.4826e-02],\n",
            "         ...,\n",
            "         [-6.3749e-02,  9.7502e-02, -1.3387e-01,  ..., -2.1377e-02,\n",
            "          -3.2663e-02, -8.7765e-02],\n",
            "         [-2.9757e-03,  6.4638e-02, -6.4855e-02,  ..., -4.3510e-02,\n",
            "          -3.2871e-02,  1.8099e-02],\n",
            "         [ 9.3562e-05, -1.2854e-01, -4.3699e-02,  ..., -9.8300e-02,\n",
            "           5.7917e-02,  5.7181e-02]]], device='cuda:0',\n",
            "       grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 5.7479e-02, -7.3860e-01,  1.8093e-01,  ..., -5.2726e-01,\n",
            "          -2.4587e-02,  4.0528e-01],\n",
            "         [-6.1604e-03, -4.7862e-01,  1.9907e-01,  ..., -4.8560e-01,\n",
            "          -2.7135e-02,  4.0510e-01],\n",
            "         [-3.9528e-02, -2.2876e-01,  1.5813e-01,  ..., -3.9340e-01,\n",
            "          -3.4070e-02,  4.0887e-01],\n",
            "         ...,\n",
            "         [-1.2543e-01,  4.4271e-02,  3.1406e-03,  ..., -4.9233e-02,\n",
            "          -1.4908e-02, -6.4586e-02],\n",
            "         [-1.6277e-01, -3.1096e-02, -1.0936e-01,  ..., -4.3234e-02,\n",
            "           2.2166e-02, -6.9232e-04],\n",
            "         [-6.7881e-02, -5.0531e-02,  8.3621e-03,  ..., -5.3415e-02,\n",
            "          -1.7070e-02,  1.0521e-02]],\n",
            "\n",
            "        [[ 4.1297e-02,  1.5122e-01, -1.4137e-01,  ..., -9.3996e-02,\n",
            "          -1.0600e-01, -2.8138e-02],\n",
            "         [ 2.4668e-02,  1.5751e-01, -1.1948e-01,  ..., -7.5361e-02,\n",
            "          -7.0375e-02, -4.8165e-02],\n",
            "         [ 1.3423e-02,  1.5745e-01, -1.0806e-01,  ..., -6.0065e-02,\n",
            "          -5.7455e-02, -6.0141e-02],\n",
            "         ...,\n",
            "         [ 5.3010e-02,  1.2137e-01, -5.2636e-03,  ..., -3.7817e-02,\n",
            "          -1.4922e-01,  4.0154e-02],\n",
            "         [ 1.2540e-01,  1.2359e-01,  2.9710e-02,  ...,  2.4622e-02,\n",
            "          -1.5000e-01,  7.4775e-02],\n",
            "         [ 1.8360e-01,  2.9043e-02,  5.0149e-02,  ...,  9.3947e-04,\n",
            "          -1.7891e-01,  1.1238e-01]],\n",
            "\n",
            "        [[-1.7544e-02,  1.1906e-01, -1.6268e-01,  ...,  3.9899e-02,\n",
            "          -1.1353e-01, -1.1562e-01],\n",
            "         [ 1.5501e-02,  1.2972e-01, -1.3545e-01,  ...,  3.3828e-03,\n",
            "          -1.0848e-01, -8.9909e-02],\n",
            "         [ 2.2338e-02,  1.4168e-01, -1.1456e-01,  ..., -2.7031e-02,\n",
            "          -9.3019e-02, -7.8810e-02],\n",
            "         ...,\n",
            "         [ 5.6183e-03,  1.3523e-01, -9.6390e-02,  ..., -6.1194e-02,\n",
            "          -6.2959e-02, -5.8787e-02],\n",
            "         [ 5.6183e-03,  1.3523e-01, -9.6390e-02,  ..., -6.1194e-02,\n",
            "          -6.2959e-02, -5.8787e-02],\n",
            "         [-5.9082e-02,  1.7211e-01, -9.3523e-02,  ...,  1.7774e-02,\n",
            "          -2.1230e-02, -1.4201e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 8.1044e-03, -5.5028e-01,  1.5940e-01,  ..., -4.8019e-01,\n",
            "          -6.4680e-03,  4.2307e-01],\n",
            "         [-3.5651e-02, -2.6370e-01,  1.5020e-01,  ..., -4.1920e-01,\n",
            "          -2.7773e-02,  3.9933e-01],\n",
            "         [-3.7445e-02, -8.0849e-02,  1.0173e-01,  ..., -3.0551e-01,\n",
            "          -4.2543e-02,  3.4578e-01],\n",
            "         ...,\n",
            "         [-1.6055e-02,  8.1508e-02,  3.9549e-03,  ..., -1.1996e-02,\n",
            "          -4.3808e-01, -4.0943e-02],\n",
            "         [-2.8548e-02,  6.9059e-02, -1.1514e-02,  ...,  6.2093e-02,\n",
            "          -4.7394e-01, -8.1235e-02],\n",
            "         [-4.8595e-02,  2.0130e-02, -4.1490e-02,  ...,  7.0978e-02,\n",
            "          -4.8476e-01, -7.9618e-02]],\n",
            "\n",
            "        [[ 3.4354e-02,  2.1369e-02,  5.5446e-03,  ...,  8.4849e-02,\n",
            "          -3.1119e-01, -7.1868e-02],\n",
            "         [ 3.0307e-02,  1.8240e-02, -1.2249e-02,  ..., -6.0611e-02,\n",
            "          -2.4816e-01, -6.5248e-02],\n",
            "         [ 2.8591e-02,  3.6891e-02, -1.9685e-02,  ..., -1.0684e-01,\n",
            "          -1.8605e-01, -5.6788e-02],\n",
            "         ...,\n",
            "         [ 2.4703e-02, -4.5457e-03,  3.1205e-02,  ...,  3.5612e-02,\n",
            "          -2.9137e-01, -1.0119e-01],\n",
            "         [ 4.1555e-03,  1.2014e-01,  4.3869e-02,  ...,  6.7091e-02,\n",
            "          -2.8166e-01, -1.8116e-01],\n",
            "         [ 3.3573e-02,  6.9389e-02,  1.7354e-02,  ...,  3.1230e-02,\n",
            "          -2.6738e-01, -7.3738e-02]],\n",
            "\n",
            "        [[ 2.0790e-03, -1.8804e-02, -1.6149e-02,  ..., -6.3697e-02,\n",
            "          -2.4436e-02,  2.0746e-02],\n",
            "         [ 7.5157e-03,  4.6393e-02, -4.0883e-02,  ..., -6.6065e-02,\n",
            "          -4.5456e-02, -1.7001e-02],\n",
            "         [ 7.9617e-03,  9.0069e-02, -5.9294e-02,  ..., -6.1854e-02,\n",
            "          -5.3978e-02, -3.9985e-02],\n",
            "         ...,\n",
            "         [ 5.6183e-03,  1.3523e-01, -9.6390e-02,  ..., -6.1194e-02,\n",
            "          -6.2959e-02, -5.8787e-02],\n",
            "         [ 5.6183e-03,  1.3523e-01, -9.6390e-02,  ..., -6.1194e-02,\n",
            "          -6.2959e-02, -5.8787e-02],\n",
            "         [ 5.7349e-03,  1.3490e-01, -1.3094e-01,  ..., -4.5126e-02,\n",
            "          -6.8658e-02, -9.5773e-02]]], device='cuda:0',\n",
            "       grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[-0.0329,  0.0192, -0.0210,  ..., -0.0579, -0.0435, -0.0249],\n",
            "         [-0.0076,  0.0678, -0.0485,  ..., -0.0610, -0.0575, -0.0397],\n",
            "         [ 0.0027,  0.0979, -0.0670,  ..., -0.0619, -0.0637, -0.0474],\n",
            "         ...,\n",
            "         [-0.0612,  0.0985, -0.1072,  ...,  0.0128, -0.0705, -0.0884],\n",
            "         [-0.0636,  0.1118, -0.0362,  ...,  0.0810, -0.0209, -0.1277],\n",
            "         [ 0.0052,  0.0853, -0.0436,  ..., -0.0187, -0.0497, -0.0629]],\n",
            "\n",
            "        [[ 0.1226,  0.0308, -0.0031,  ..., -0.1035, -0.1103,  0.0305],\n",
            "         [ 0.0744,  0.0545, -0.0337,  ..., -0.1151, -0.0865, -0.0107],\n",
            "         [ 0.0471,  0.0750, -0.0505,  ..., -0.1048, -0.0839, -0.0291],\n",
            "         ...,\n",
            "         [ 0.0232,  0.0980, -0.0099,  ...,  0.0130, -0.1320, -0.0441],\n",
            "         [ 0.0066,  0.0806, -0.0371,  ..., -0.0432, -0.1203, -0.0635],\n",
            "         [ 0.0040,  0.0513, -0.0705,  ..., -0.0212, -0.1396, -0.0713]],\n",
            "\n",
            "        [[-0.0046,  0.1222, -0.0964,  ..., -0.0039, -0.0820, -0.0819],\n",
            "         [ 0.0124,  0.1226, -0.0953,  ..., -0.0391, -0.0885, -0.0701],\n",
            "         [ 0.0133,  0.1270, -0.0933,  ..., -0.0557, -0.0790, -0.0641],\n",
            "         ...,\n",
            "         [-0.0291,  0.0680, -0.0600,  ...,  0.0179, -0.0374, -0.2201],\n",
            "         [-0.0432,  0.0470, -0.1269,  ...,  0.0157,  0.0016, -0.2196],\n",
            "         [ 0.0818, -0.0803, -0.1392,  ..., -0.0321,  0.0390, -0.0327]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0153,  0.0166, -0.0336,  ..., -0.0224, -0.3085, -0.0520],\n",
            "         [ 0.0042,  0.0189, -0.0257,  ..., -0.0803, -0.3048, -0.0419],\n",
            "         [ 0.0143,  0.0343, -0.0242,  ..., -0.1120, -0.2516, -0.0342],\n",
            "         ...,\n",
            "         [ 0.1500, -0.1395, -0.0409,  ..., -0.3699,  0.1309,  0.0971],\n",
            "         [-0.0438, -0.3569,  0.0347,  ..., -0.2873, -0.0521,  0.0185],\n",
            "         [-0.0285, -0.5069,  0.0859,  ..., -0.3407,  0.0138,  0.0237]],\n",
            "\n",
            "        [[ 0.0341,  0.0635, -0.0376,  ..., -0.0785, -0.1663, -0.0845],\n",
            "         [ 0.0342,  0.0767, -0.0430,  ..., -0.1045, -0.1248, -0.0731],\n",
            "         [ 0.0301,  0.0875, -0.0496,  ..., -0.1057, -0.1037, -0.0634],\n",
            "         ...,\n",
            "         [ 0.0780, -0.5176, -0.0088,  ..., -0.3070,  0.0696,  0.0833],\n",
            "         [ 0.0230, -0.4704,  0.0150,  ..., -0.2870,  0.0050,  0.0529],\n",
            "         [ 0.0688, -0.6886,  0.0090,  ..., -0.4258,  0.1032,  0.1233]],\n",
            "\n",
            "        [[ 0.0262,  0.1365, -0.0972,  ..., -0.0642, -0.0824, -0.0734],\n",
            "         [ 0.0164,  0.1391, -0.0953,  ..., -0.0706, -0.0648, -0.0720],\n",
            "         [ 0.0075,  0.1414, -0.0941,  ..., -0.0659, -0.0581, -0.0691],\n",
            "         ...,\n",
            "         [-0.1152,  0.1800, -0.0318,  ...,  0.0845, -0.1874, -0.1256],\n",
            "         [-0.0716,  0.0868,  0.0037,  ...,  0.0804, -0.2617, -0.1316],\n",
            "         [-0.1226,  0.0332,  0.0701,  ...,  0.0524, -0.2985, -0.2407]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 7.6296e-03,  1.7323e-01, -5.2397e-02,  ..., -5.8877e-02,\n",
            "          -3.7983e-02, -8.1825e-02],\n",
            "         [-9.7070e-03,  1.7123e-01, -7.5603e-02,  ..., -6.2425e-02,\n",
            "          -3.2180e-02, -7.9883e-02],\n",
            "         [-1.2399e-02,  1.5787e-01, -8.9324e-02,  ..., -5.9902e-02,\n",
            "          -4.2499e-02, -7.1584e-02],\n",
            "         ...,\n",
            "         [ 4.0803e-02, -8.6243e-01,  1.6033e-01,  ..., -5.7137e-01,\n",
            "          -7.1557e-02,  2.2553e-01],\n",
            "         [ 1.5798e-01, -8.4435e-01,  1.0976e-01,  ..., -7.7514e-01,\n",
            "          -8.5546e-03,  2.9574e-01],\n",
            "         [ 1.8599e-01, -8.7165e-01,  9.2179e-02,  ..., -6.5667e-01,\n",
            "           6.0539e-02,  3.5049e-01]],\n",
            "\n",
            "        [[ 3.0793e-02,  6.8522e-02, -6.4439e-02,  ..., -5.5397e-02,\n",
            "          -1.3738e-01, -5.5424e-02],\n",
            "         [ 2.5519e-02,  8.7739e-02, -7.4090e-02,  ..., -7.4640e-02,\n",
            "          -1.1051e-01, -5.7589e-02],\n",
            "         [ 1.8369e-02,  1.0547e-01, -7.8853e-02,  ..., -7.5418e-02,\n",
            "          -9.0684e-02, -5.6128e-02],\n",
            "         ...,\n",
            "         [-6.3576e-02,  1.0629e-01, -1.3581e-01,  ..., -6.8068e-03,\n",
            "          -6.1133e-03, -8.5290e-02],\n",
            "         [-7.4328e-02,  1.2707e-01, -5.2093e-02,  ..., -1.4146e-02,\n",
            "          -9.1009e-03, -9.3349e-03],\n",
            "         [-7.5797e-02,  1.6661e-01, -2.5009e-02,  ...,  2.5886e-02,\n",
            "          -1.4395e-01, -5.3080e-02]],\n",
            "\n",
            "        [[ 3.5530e-02,  2.3196e-02, -7.2549e-02,  ..., -2.1808e-02,\n",
            "          -7.9829e-02, -3.4969e-02],\n",
            "         [ 3.3104e-02,  8.1830e-02, -8.3104e-02,  ..., -3.4734e-02,\n",
            "          -8.9224e-02, -5.1899e-02],\n",
            "         [ 2.8874e-02,  1.1888e-01, -8.4845e-02,  ..., -4.2420e-02,\n",
            "          -8.7909e-02, -6.3985e-02],\n",
            "         ...,\n",
            "         [ 1.7850e-02,  1.2554e-01, -2.4073e-01,  ..., -4.9804e-02,\n",
            "          -6.7572e-02, -6.1866e-02],\n",
            "         [ 8.5665e-03, -1.1163e-01, -3.9463e-01,  ..., -8.6056e-02,\n",
            "           1.2587e-01, -1.4573e-02],\n",
            "         [-6.8200e-02, -1.2526e-01, -2.1422e-01,  ..., -3.5790e-02,\n",
            "           9.2776e-02, -1.0388e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-3.0273e-02, -3.9615e-01,  4.6182e-02,  ..., -2.2812e-01,\n",
            "          -9.9614e-02,  1.4879e-03],\n",
            "         [-9.5850e-03, -1.6958e-01,  1.7420e-02,  ..., -1.9721e-01,\n",
            "          -9.3310e-02, -2.7474e-02],\n",
            "         [-7.0579e-05, -1.4400e-02, -1.4148e-02,  ..., -1.3841e-01,\n",
            "          -8.2395e-02, -5.3400e-02],\n",
            "         ...,\n",
            "         [ 4.3505e-02,  1.0801e-01, -3.1064e-02,  ...,  9.9372e-02,\n",
            "          -2.3979e-01, -8.7170e-02],\n",
            "         [-4.8540e-02,  5.8945e-02,  3.2222e-02,  ...,  1.6986e-01,\n",
            "          -2.9361e-01, -1.7860e-01],\n",
            "         [ 1.5020e-02,  1.7574e-02, -5.7594e-02,  ...,  2.3546e-01,\n",
            "          -3.0624e-01, -9.7273e-02]],\n",
            "\n",
            "        [[-3.7608e-02, -6.2076e-01,  2.1581e-02,  ..., -1.8871e-01,\n",
            "          -2.5124e-02,  1.3804e-01],\n",
            "         [-2.4471e-02, -3.5268e-01,  1.6620e-02,  ..., -1.5839e-01,\n",
            "          -5.1466e-02,  5.8126e-02],\n",
            "         [-1.1090e-02, -1.2546e-01, -6.0908e-03,  ..., -1.0534e-01,\n",
            "          -6.1742e-02, -6.2471e-03],\n",
            "         ...,\n",
            "         [-1.3206e-01,  1.0849e-01, -1.1243e-01,  ..., -3.9140e-02,\n",
            "          -4.9886e-03, -2.1121e-02],\n",
            "         [-1.2818e-01,  6.0940e-02, -1.0361e-01,  ..., -1.0029e-02,\n",
            "          -1.1894e-02, -4.6034e-02],\n",
            "         [-4.4092e-02,  2.0569e-02, -1.7768e-01,  ..., -3.6827e-02,\n",
            "           2.3753e-02, -2.2857e-02]],\n",
            "\n",
            "        [[-4.0031e-02,  1.8918e-02, -4.8115e-04,  ..., -1.2342e-02,\n",
            "          -2.8256e-01, -8.0681e-02],\n",
            "         [-5.1324e-03,  2.1017e-02, -1.6948e-02,  ..., -7.5064e-02,\n",
            "          -2.4969e-01, -4.8862e-02],\n",
            "         [ 1.0014e-02,  4.2364e-02, -2.1833e-02,  ..., -1.0157e-01,\n",
            "          -1.9706e-01, -3.6129e-02],\n",
            "         ...,\n",
            "         [ 4.9571e-02, -1.6401e-01, -1.1066e-01,  ..., -1.7412e-01,\n",
            "           7.3506e-02,  5.1902e-02],\n",
            "         [ 1.1965e-01, -1.7720e-01, -1.2314e-01,  ..., -2.6972e-01,\n",
            "           1.0923e-01,  3.5178e-02],\n",
            "         [ 1.7962e-02, -2.1275e-01, -9.6521e-02,  ..., -2.2462e-01,\n",
            "           1.0513e-01, -2.4891e-03]]], device='cuda:0',\n",
            "       grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 4.8009e-02, -8.3122e-01,  1.1211e-01,  ..., -4.6150e-01,\n",
            "          -3.0850e-02,  4.0012e-01],\n",
            "         [-9.8119e-03, -6.7758e-01,  1.2207e-01,  ..., -4.1560e-01,\n",
            "          -4.6074e-02,  4.0197e-01],\n",
            "         [-2.5435e-02, -4.2890e-01,  1.0639e-01,  ..., -3.4600e-01,\n",
            "          -5.0138e-02,  4.0538e-01],\n",
            "         ...,\n",
            "         [ 8.7530e-02, -1.5706e-01,  2.8217e-02,  ..., -1.9690e-01,\n",
            "           1.0098e-01, -3.6581e-02],\n",
            "         [ 1.3308e-01, -2.3171e-01,  7.6698e-02,  ..., -2.5794e-01,\n",
            "           7.7234e-02,  4.4155e-02],\n",
            "         [ 4.0092e-02, -1.7991e-01,  7.7169e-02,  ..., -2.2874e-01,\n",
            "           1.2181e-01,  1.4485e-02]],\n",
            "\n",
            "        [[ 4.5106e-03,  1.0090e-01, -6.5441e-02,  ..., -4.1574e-02,\n",
            "          -1.0724e-01, -5.1984e-02],\n",
            "         [ 7.1616e-03,  1.2066e-01, -7.6272e-02,  ..., -6.4326e-02,\n",
            "          -6.9538e-02, -6.6706e-02],\n",
            "         [ 8.3075e-03,  1.2850e-01, -7.8676e-02,  ..., -6.6451e-02,\n",
            "          -6.2149e-02, -6.3989e-02],\n",
            "         ...,\n",
            "         [-4.2280e-02,  9.7391e-02, -6.1689e-02,  ...,  4.4281e-02,\n",
            "          -2.7552e-02, -1.5996e-01],\n",
            "         [-1.2786e-01,  6.9384e-02, -4.2798e-02,  ...,  1.2845e-01,\n",
            "          -1.3061e-01, -1.9474e-01],\n",
            "         [-1.0241e-01,  5.9293e-02, -7.0483e-02,  ...,  1.4572e-01,\n",
            "          -1.6272e-01, -2.0492e-01]],\n",
            "\n",
            "        [[ 1.7231e-02, -2.5901e-02, -1.7187e-01,  ..., -3.5736e-02,\n",
            "          -7.3653e-02, -8.7548e-02],\n",
            "         [ 2.8024e-02,  6.9446e-02, -1.3566e-01,  ..., -4.5322e-02,\n",
            "          -9.3192e-02, -8.9160e-02],\n",
            "         [ 2.5244e-02,  1.1826e-01, -1.1180e-01,  ..., -4.9263e-02,\n",
            "          -8.8484e-02, -8.5933e-02],\n",
            "         ...,\n",
            "         [-1.3513e-01,  1.2632e-01, -1.5614e-01,  ..., -4.2971e-02,\n",
            "          -2.0992e-02, -1.1409e-01],\n",
            "         [-1.8035e-01,  1.3566e-01, -4.2622e-02,  ...,  1.2796e-02,\n",
            "          -2.8231e-02, -7.7275e-02],\n",
            "         [-1.2302e-01,  1.0465e-01, -2.9630e-02,  ...,  4.8545e-02,\n",
            "          -9.1570e-02, -6.2672e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 2.3472e-02,  2.7444e-02, -3.9332e-02,  ...,  5.0767e-02,\n",
            "          -2.6537e-01, -6.6420e-02],\n",
            "         [ 2.8723e-02,  3.1410e-02, -3.5671e-02,  ..., -5.9223e-02,\n",
            "          -2.3712e-01, -5.7259e-02],\n",
            "         [ 2.8966e-02,  4.9780e-02, -3.5195e-02,  ..., -1.0004e-01,\n",
            "          -1.8741e-01, -4.9290e-02],\n",
            "         ...,\n",
            "         [ 8.2601e-02, -9.5099e-01,  1.0635e-01,  ..., -4.9458e-01,\n",
            "           1.9544e-01,  5.0023e-01],\n",
            "         [ 1.5031e-02, -9.5507e-01,  1.0794e-01,  ..., -3.4436e-01,\n",
            "           1.4422e-01,  4.9937e-01],\n",
            "         [ 4.7933e-02, -9.4902e-01,  8.4145e-02,  ..., -3.5202e-01,\n",
            "           1.7439e-01,  4.2876e-01]],\n",
            "\n",
            "        [[ 1.9082e-02,  1.0434e-01, -6.1329e-02,  ..., -3.8054e-02,\n",
            "          -4.8175e-02, -3.5601e-02],\n",
            "         [ 1.8088e-02,  1.1933e-01, -7.6886e-02,  ..., -6.0587e-02,\n",
            "          -4.2396e-02, -4.9297e-02],\n",
            "         [ 8.3202e-03,  1.3456e-01, -8.5535e-02,  ..., -6.1488e-02,\n",
            "          -4.1397e-02, -5.7532e-02],\n",
            "         ...,\n",
            "         [ 8.3359e-02, -6.2114e-01, -1.8368e-01,  ..., -9.1386e-02,\n",
            "           1.1869e-01, -7.1563e-02],\n",
            "         [ 6.6757e-02, -6.8624e-01, -2.9534e-01,  ..., -1.0018e-01,\n",
            "           2.5346e-01, -8.8076e-02],\n",
            "         [ 8.3677e-02, -6.4488e-01, -2.2928e-01,  ..., -8.6431e-02,\n",
            "           1.7955e-01, -6.5376e-02]],\n",
            "\n",
            "        [[ 2.1488e-02, -5.1650e-02, -5.6030e-02,  ..., -1.5003e-01,\n",
            "          -6.4247e-02, -1.9965e-02],\n",
            "         [ 1.4106e-02,  5.1097e-02, -5.7410e-02,  ..., -1.2346e-01,\n",
            "          -7.7957e-02, -4.1700e-02],\n",
            "         [ 8.5614e-03,  1.0258e-01, -6.4580e-02,  ..., -9.1810e-02,\n",
            "          -7.5733e-02, -6.0101e-02],\n",
            "         ...,\n",
            "         [-1.6068e-02,  1.5539e-01, -3.8634e-03,  ...,  2.3162e-02,\n",
            "          -1.9402e-01, -1.1987e-01],\n",
            "         [-8.2217e-04,  1.2326e-01, -9.8313e-02,  ..., -1.7303e-02,\n",
            "          -1.5554e-01, -1.2764e-01],\n",
            "         [ 2.2124e-02,  1.5393e-01, -3.0281e-02,  ..., -2.5788e-02,\n",
            "          -2.1717e-01, -1.8133e-01]]], device='cuda:0',\n",
            "       grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 1.8806e-02, -4.6747e-03,  4.6582e-02,  ..., -1.9899e-01,\n",
            "          -1.0416e-02, -2.2531e-03],\n",
            "         [-7.2421e-04,  7.0724e-02,  6.4167e-03,  ..., -1.5001e-01,\n",
            "          -4.0649e-02, -2.8764e-02],\n",
            "         [-1.3219e-03,  1.0320e-01, -2.9647e-02,  ..., -1.0830e-01,\n",
            "          -5.7776e-02, -4.5403e-02],\n",
            "         ...,\n",
            "         [-9.3523e-02,  4.8095e-02,  2.4586e-02,  ...,  1.6635e-01,\n",
            "          -4.1337e-01, -1.6867e-01],\n",
            "         [-1.0230e-01,  2.4790e-02,  2.8260e-02,  ...,  1.7042e-01,\n",
            "          -4.0576e-01, -1.6988e-01],\n",
            "         [-2.0155e-01,  1.4607e-02,  1.0753e-02,  ...,  1.1447e-01,\n",
            "          -2.6781e-01, -1.1083e-01]],\n",
            "\n",
            "        [[-9.0139e-03,  6.8924e-02, -5.2143e-02,  ...,  4.7167e-02,\n",
            "          -1.6120e-01, -1.0870e-01],\n",
            "         [ 8.7663e-03,  8.8086e-02, -5.6849e-02,  ..., -2.6492e-02,\n",
            "          -1.3173e-01, -8.4079e-02],\n",
            "         [ 1.4668e-02,  1.0251e-01, -6.0379e-02,  ..., -5.8892e-02,\n",
            "          -1.0568e-01, -6.6002e-02],\n",
            "         ...,\n",
            "         [ 2.7437e-02,  5.9077e-02, -1.2788e-01,  ..., -8.1110e-02,\n",
            "          -3.5526e-02, -7.1130e-02],\n",
            "         [ 7.3780e-02,  5.1297e-03, -1.2070e-01,  ..., -1.4873e-01,\n",
            "          -3.0702e-02, -1.4500e-01],\n",
            "         [ 2.4265e-02, -1.6897e-01, -1.5936e-01,  ..., -1.9789e-01,\n",
            "           1.1785e-01, -1.5516e-01]],\n",
            "\n",
            "        [[-2.5023e-02,  8.1882e-02, -6.2960e-02,  ..., -2.4296e-02,\n",
            "          -9.4252e-02, -5.3802e-02],\n",
            "         [-7.3471e-03,  1.0639e-01, -6.7940e-02,  ..., -5.7654e-02,\n",
            "          -7.8260e-02, -6.1997e-02],\n",
            "         [ 2.4614e-03,  1.1613e-01, -7.0779e-02,  ..., -6.7667e-02,\n",
            "          -7.4088e-02, -5.8655e-02],\n",
            "         ...,\n",
            "         [-5.7834e-02,  1.7875e-01, -2.0476e-01,  ...,  5.4657e-02,\n",
            "          -1.3172e-01, -2.5249e-01],\n",
            "         [-1.0212e-01,  1.2920e-01, -9.1784e-02,  ...,  1.2551e-01,\n",
            "          -1.7343e-01, -2.4326e-01],\n",
            "         [-7.8525e-02,  1.1706e-01, -2.9000e-02,  ...,  1.4005e-01,\n",
            "          -2.2173e-01, -1.8195e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-6.0998e-02, -9.0144e-01,  7.0397e-02,  ..., -2.0810e-01,\n",
            "           5.9691e-03,  3.8040e-01],\n",
            "         [-3.6418e-02, -7.5949e-01,  6.2825e-02,  ..., -1.9583e-01,\n",
            "          -5.6253e-02,  3.2903e-01],\n",
            "         [-1.4750e-02, -6.3385e-01,  4.2370e-02,  ..., -1.4503e-01,\n",
            "          -7.0447e-02,  2.4655e-01],\n",
            "         ...,\n",
            "         [-4.0509e-02,  1.3101e-01,  9.8884e-03,  ...,  2.5996e-02,\n",
            "          -2.6744e-01, -1.0116e-01],\n",
            "         [-6.9919e-02,  1.3892e-01,  2.1440e-02,  ...,  1.1608e-01,\n",
            "          -3.3953e-01, -1.6222e-01],\n",
            "         [-6.6010e-02,  6.5582e-02, -4.4876e-02,  ...,  1.5717e-01,\n",
            "          -4.5076e-01, -1.6919e-01]],\n",
            "\n",
            "        [[ 4.9652e-02, -5.5522e-01, -1.2586e-01,  ..., -5.3969e-02,\n",
            "          -1.1002e-01, -7.7451e-02],\n",
            "         [ 4.1100e-02, -2.7881e-01, -1.1061e-01,  ..., -5.5198e-02,\n",
            "          -1.1493e-01, -8.4987e-02],\n",
            "         [ 3.4378e-02, -5.1519e-02, -9.1975e-02,  ..., -5.1367e-02,\n",
            "          -1.0314e-01, -8.7752e-02],\n",
            "         ...,\n",
            "         [-1.2723e-02,  1.5799e-01, -2.2449e-01,  ..., -3.3056e-02,\n",
            "          -3.3957e-02, -1.3564e-01],\n",
            "         [ 4.8966e-03,  1.9991e-01, -1.6521e-01,  ..., -2.4407e-02,\n",
            "          -9.5605e-02, -1.3853e-01],\n",
            "         [ 2.3809e-02,  1.5993e-01, -9.3332e-02,  ..., -1.0133e-02,\n",
            "          -9.1898e-02, -2.4535e-01]],\n",
            "\n",
            "        [[ 4.2949e-02,  1.0777e-01, -5.8840e-02,  ..., -7.9596e-02,\n",
            "          -1.2953e-01, -1.0508e-01],\n",
            "         [ 2.7181e-02,  1.2343e-01, -6.4126e-02,  ..., -7.8938e-02,\n",
            "          -8.8789e-02, -8.2647e-02],\n",
            "         [ 1.9015e-02,  1.2843e-01, -6.9332e-02,  ..., -7.3852e-02,\n",
            "          -7.4880e-02, -6.7959e-02],\n",
            "         ...,\n",
            "         [ 4.6982e-03,  9.7992e-02, -1.0951e-01,  ...,  6.3983e-02,\n",
            "          -1.3359e-02, -1.5004e-01],\n",
            "         [-7.5042e-04,  9.2288e-02, -4.3290e-02,  ..., -1.7469e-02,\n",
            "          -4.7668e-02, -1.9541e-01],\n",
            "         [-8.9945e-02,  1.8163e-02, -3.3602e-02,  ..., -1.8054e-01,\n",
            "           7.3402e-02, -1.4465e-01]]], device='cuda:0',\n",
            "       grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[-1.9843e-02, -2.4315e-04, -4.9195e-03,  ..., -1.0792e-02,\n",
            "          -2.6936e-01, -4.2308e-02],\n",
            "         [-1.4040e-03, -6.9685e-04, -1.2124e-02,  ..., -8.7170e-02,\n",
            "          -2.9463e-01, -4.5628e-02],\n",
            "         [ 1.0262e-02,  2.2589e-02, -1.6200e-02,  ..., -1.1692e-01,\n",
            "          -2.3666e-01, -3.7242e-02],\n",
            "         ...,\n",
            "         [ 9.4861e-02, -2.2367e-01, -6.8814e-02,  ..., -2.7907e-01,\n",
            "           9.2476e-03, -7.6259e-02],\n",
            "         [ 9.1185e-02, -3.4066e-01, -8.1182e-02,  ..., -2.2228e-01,\n",
            "          -5.0002e-02, -3.5222e-02],\n",
            "         [-9.3751e-03, -2.5987e-01, -6.5288e-02,  ..., -1.7642e-01,\n",
            "          -4.3119e-02, -2.7505e-03]],\n",
            "\n",
            "        [[-1.3470e-02, -7.9982e-02, -9.5357e-02,  ..., -1.0412e-01,\n",
            "          -4.0865e-02, -1.1544e-01],\n",
            "         [ 8.6693e-04,  3.6132e-02, -9.3187e-02,  ..., -8.5547e-02,\n",
            "          -7.8485e-02, -9.6536e-02],\n",
            "         [ 9.4179e-03,  1.0171e-01, -9.0370e-02,  ..., -7.0202e-02,\n",
            "          -8.2472e-02, -8.8307e-02],\n",
            "         ...,\n",
            "         [-6.5329e-02, -2.1617e-02, -3.5417e-02,  ..., -1.4980e-01,\n",
            "           1.7854e-03, -2.9773e-01],\n",
            "         [-3.4290e-02, -2.7109e-02, -1.0023e-02,  ..., -1.0590e-01,\n",
            "          -1.7199e-02, -3.0563e-01],\n",
            "         [-8.1038e-02, -1.0610e-01, -6.3778e-02,  ..., -1.2963e-01,\n",
            "           1.2814e-02, -3.1574e-01]],\n",
            "\n",
            "        [[-1.2467e-02,  7.0744e-02, -6.5140e-02,  ...,  6.2186e-03,\n",
            "          -1.8074e-01, -1.0184e-01],\n",
            "         [ 4.8177e-03,  8.5785e-02, -5.8964e-02,  ..., -5.7289e-02,\n",
            "          -1.3993e-01, -7.6114e-02],\n",
            "         [ 1.3946e-02,  9.7630e-02, -5.7881e-02,  ..., -7.8222e-02,\n",
            "          -1.1060e-01, -5.9225e-02],\n",
            "         ...,\n",
            "         [-6.3154e-02, -8.2510e-03,  6.2425e-02,  ..., -1.7462e-02,\n",
            "          -3.0564e-01, -1.9355e-01],\n",
            "         [-9.1983e-02,  3.9345e-02,  8.2794e-02,  ...,  5.1714e-02,\n",
            "          -3.3737e-01, -1.5416e-01],\n",
            "         [-1.4165e-01,  1.5563e-02,  5.6071e-02,  ...,  5.3604e-02,\n",
            "          -2.7313e-01, -1.2854e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 2.8193e-03,  2.9129e-02, -4.3950e-02,  ..., -1.5398e-02,\n",
            "          -3.0841e-01, -7.9550e-02],\n",
            "         [ 1.1416e-02,  4.0745e-02, -3.9649e-02,  ..., -8.8605e-02,\n",
            "          -2.4660e-01, -6.9165e-02],\n",
            "         [ 1.7949e-02,  5.9719e-02, -3.7722e-02,  ..., -1.0980e-01,\n",
            "          -1.8199e-01, -5.4316e-02],\n",
            "         ...,\n",
            "         [-1.8968e-02,  1.5290e-01, -3.8209e-02,  ..., -3.8364e-02,\n",
            "          -6.2543e-02, -9.1884e-02],\n",
            "         [-1.3509e-02,  1.7714e-01, -3.0708e-02,  ..., -4.5471e-02,\n",
            "          -1.2201e-01, -8.2887e-02],\n",
            "         [ 5.2100e-03,  1.7591e-01,  1.9104e-03,  ..., -3.7610e-03,\n",
            "          -2.1653e-01, -1.1047e-01]],\n",
            "\n",
            "        [[ 3.3298e-02,  1.4916e-01, -5.4534e-02,  ..., -2.3364e-02,\n",
            "          -1.1456e-01, -1.3600e-01],\n",
            "         [ 3.4472e-02,  1.4063e-01, -5.7502e-02,  ..., -4.9162e-02,\n",
            "          -1.0352e-01, -9.9160e-02],\n",
            "         [ 2.9349e-02,  1.3617e-01, -6.4241e-02,  ..., -6.3149e-02,\n",
            "          -8.9910e-02, -7.7314e-02],\n",
            "         ...,\n",
            "         [ 9.8353e-02, -7.2349e-02,  1.0503e-01,  ..., -4.1090e-01,\n",
            "          -2.2188e-02,  1.8920e-01],\n",
            "         [ 1.0014e-02,  3.7148e-02,  7.7530e-02,  ..., -3.1620e-01,\n",
            "           6.0505e-03,  1.3422e-01],\n",
            "         [ 7.2444e-02,  6.6734e-02,  3.1052e-02,  ..., -3.3528e-01,\n",
            "          -6.9034e-02,  1.0860e-01]],\n",
            "\n",
            "        [[-7.8387e-02,  1.1991e-01, -3.2026e-03,  ..., -1.0413e-01,\n",
            "          -4.1775e-02, -1.1812e-01],\n",
            "         [-3.4879e-02,  1.0920e-01, -3.2005e-02,  ..., -9.3788e-02,\n",
            "          -5.8816e-02, -9.1760e-02],\n",
            "         [-1.3623e-02,  1.2449e-01, -5.3829e-02,  ..., -7.5827e-02,\n",
            "          -6.3044e-02, -8.3411e-02],\n",
            "         ...,\n",
            "         [-8.5165e-02,  6.1686e-02, -1.4916e-01,  ..., -6.9998e-02,\n",
            "           1.0985e-01, -6.2839e-02],\n",
            "         [ 3.1972e-02,  9.5268e-02, -1.2924e-01,  ..., -7.8176e-02,\n",
            "          -2.2512e-02, -5.1082e-02],\n",
            "         [ 2.5770e-02,  1.1984e-01, -1.6827e-01,  ..., -6.7838e-02,\n",
            "           8.3782e-02, -3.3638e-02]]], device='cuda:0',\n",
            "       grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[-3.3822e-02, -7.3894e-02, -4.4001e-02,  ..., -1.3979e-01,\n",
            "          -7.5041e-02, -3.5015e-02],\n",
            "         [-1.7718e-02,  4.3180e-02, -5.1957e-02,  ..., -1.0935e-01,\n",
            "          -7.5158e-02, -5.9779e-02],\n",
            "         [-4.9029e-03,  9.8308e-02, -6.3233e-02,  ..., -8.1610e-02,\n",
            "          -7.3103e-02, -7.1853e-02],\n",
            "         ...,\n",
            "         [ 9.5405e-03, -3.0003e-02,  2.3225e-04,  ...,  6.2472e-02,\n",
            "          -1.2431e-01, -2.2213e-01],\n",
            "         [-4.9605e-02,  4.4778e-02, -3.9500e-03,  ...,  6.4311e-02,\n",
            "          -1.3903e-01, -2.0084e-01],\n",
            "         [-7.7043e-02,  2.1100e-02, -2.6395e-02,  ...,  4.7434e-02,\n",
            "          -7.8297e-02, -1.2959e-01]],\n",
            "\n",
            "        [[ 9.8363e-03, -2.5229e-02, -4.4422e-02,  ..., -6.9323e-02,\n",
            "          -8.6874e-02, -1.5788e-01],\n",
            "         [ 1.8845e-02,  6.0821e-02, -5.8997e-02,  ..., -7.1507e-02,\n",
            "          -9.2682e-02, -1.1144e-01],\n",
            "         [ 1.9297e-02,  1.0485e-01, -7.0175e-02,  ..., -7.0241e-02,\n",
            "          -8.5644e-02, -8.7691e-02],\n",
            "         ...,\n",
            "         [ 5.6183e-03,  1.3523e-01, -9.6390e-02,  ..., -6.1194e-02,\n",
            "          -6.2959e-02, -5.8787e-02],\n",
            "         [-9.2832e-02,  1.5382e-01, -8.8745e-02,  ...,  2.5584e-03,\n",
            "          -5.9845e-02, -4.2589e-02],\n",
            "         [ 1.0091e-02,  1.2905e-01, -1.3311e-01,  ..., -2.9501e-02,\n",
            "          -6.5838e-02, -4.4595e-03]],\n",
            "\n",
            "        [[-4.2858e-02,  5.2427e-03,  9.1431e-04,  ..., -9.1585e-03,\n",
            "          -2.6545e-01, -4.2981e-02],\n",
            "         [-7.1505e-03,  3.5882e-03, -9.0167e-03,  ..., -7.6530e-02,\n",
            "          -2.9103e-01, -3.5887e-02],\n",
            "         [ 7.9252e-03,  2.0668e-02, -1.2949e-02,  ..., -1.1168e-01,\n",
            "          -2.4373e-01, -2.9189e-02],\n",
            "         ...,\n",
            "         [ 3.7080e-02,  3.9731e-02, -1.3162e-01,  ..., -1.1370e-01,\n",
            "          -2.7378e-01, -2.8990e-02],\n",
            "         [ 3.1893e-02,  4.6293e-02, -1.1241e-01,  ..., -1.0728e-01,\n",
            "          -2.5149e-01, -4.3652e-02],\n",
            "         [ 4.7572e-02,  5.5553e-02, -5.4462e-02,  ..., -6.8898e-02,\n",
            "          -2.8381e-01, -5.6818e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 2.6679e-02,  1.1371e-01, -3.9810e-02,  ..., -6.8226e-02,\n",
            "          -1.5705e-01, -7.8006e-02],\n",
            "         [ 2.1982e-02,  1.1174e-01, -5.4191e-02,  ..., -8.3007e-02,\n",
            "          -1.0688e-01, -7.3517e-02],\n",
            "         [ 1.6653e-02,  1.1660e-01, -6.1839e-02,  ..., -8.0447e-02,\n",
            "          -8.6555e-02, -6.5896e-02],\n",
            "         ...,\n",
            "         [ 1.3703e-01, -7.0021e-01,  7.9464e-02,  ..., -5.9105e-01,\n",
            "          -5.8227e-03,  4.7782e-01],\n",
            "         [ 2.2847e-01, -7.7673e-01,  1.4444e-01,  ..., -7.0040e-01,\n",
            "          -2.3974e-03,  5.7309e-01],\n",
            "         [ 1.9390e-01, -8.9246e-01,  7.9644e-02,  ..., -8.4743e-01,\n",
            "          -2.9758e-02,  4.9340e-01]],\n",
            "\n",
            "        [[ 1.4296e-02,  1.3741e-01,  4.9448e-03,  ..., -2.4730e-01,\n",
            "          -6.0192e-02,  6.8517e-02],\n",
            "         [-1.1324e-02,  1.5005e-01, -2.0271e-02,  ..., -1.7632e-01,\n",
            "          -6.2969e-02,  2.0772e-02],\n",
            "         [-1.1311e-02,  1.4065e-01, -4.6891e-02,  ..., -1.2395e-01,\n",
            "          -6.7936e-02, -1.5437e-02],\n",
            "         ...,\n",
            "         [ 4.7242e-02,  1.5818e-01, -1.2258e-01,  ..., -1.9770e-02,\n",
            "           1.4362e-01, -1.9087e-01],\n",
            "         [ 4.8361e-02,  2.1215e-01, -1.3269e-01,  ..., -3.2054e-02,\n",
            "           6.7808e-02, -1.6601e-01],\n",
            "         [ 5.3582e-02,  2.2637e-01, -1.9377e-01,  ..., -5.4581e-02,\n",
            "           1.1048e-01, -2.1107e-01]],\n",
            "\n",
            "        [[ 2.4471e-02,  1.9150e-01, -1.0428e-01,  ..., -4.3652e-02,\n",
            "          -4.1711e-02, -5.4544e-02],\n",
            "         [ 2.1803e-02,  1.7936e-01, -1.1017e-01,  ..., -5.0244e-02,\n",
            "          -6.4284e-02, -6.7970e-02],\n",
            "         [ 1.6534e-02,  1.7060e-01, -1.0633e-01,  ..., -5.0826e-02,\n",
            "          -6.9145e-02, -7.4377e-02],\n",
            "         ...,\n",
            "         [ 1.0678e-01, -6.0716e-02, -4.3282e-02,  ..., -2.1084e-01,\n",
            "          -5.2191e-02,  2.0349e-01],\n",
            "         [ 1.1310e-01,  3.1341e-02, -6.9360e-03,  ..., -9.3055e-02,\n",
            "          -2.7022e-02,  1.3921e-01],\n",
            "         [ 1.3490e-01,  2.8604e-02,  4.8851e-03,  ..., -5.9888e-02,\n",
            "          -1.7834e-02,  9.9014e-02]]], device='cuda:0',\n",
            "       grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[-1.2623e-03,  6.3206e-02, -4.4593e-02,  ...,  2.8202e-03,\n",
            "          -1.1784e-01, -7.7142e-02],\n",
            "         [ 1.7109e-02,  9.1032e-02, -6.2444e-02,  ..., -3.3033e-02,\n",
            "          -1.0146e-01, -6.6353e-02],\n",
            "         [ 1.6791e-02,  1.1155e-01, -7.4482e-02,  ..., -5.1775e-02,\n",
            "          -8.2593e-02, -6.3032e-02],\n",
            "         ...,\n",
            "         [-3.4598e-02, -5.7227e-02, -1.8993e-01,  ..., -4.0318e-02,\n",
            "          -4.1798e-03, -6.6485e-02],\n",
            "         [-9.2840e-03, -7.1073e-03, -1.9746e-01,  ..., -2.8007e-02,\n",
            "          -6.5939e-02, -5.3426e-02],\n",
            "         [-3.3044e-02, -2.4388e-02, -2.2992e-01,  ...,  3.6477e-04,\n",
            "          -5.0487e-03, -8.3884e-02]],\n",
            "\n",
            "        [[ 4.2514e-02,  1.4347e-01, -1.0313e-01,  ..., -5.3411e-02,\n",
            "          -6.8085e-02, -2.8200e-02],\n",
            "         [ 2.4519e-02,  1.4752e-01, -1.0156e-01,  ..., -6.0734e-02,\n",
            "          -5.2040e-02, -5.0821e-02],\n",
            "         [ 1.0902e-02,  1.4807e-01, -1.0028e-01,  ..., -5.7785e-02,\n",
            "          -5.0414e-02, -5.9775e-02],\n",
            "         ...,\n",
            "         [ 5.6183e-03,  1.3523e-01, -9.6390e-02,  ..., -6.1194e-02,\n",
            "          -6.2959e-02, -5.8787e-02],\n",
            "         [-9.3746e-02,  1.2316e-01, -1.0157e-01,  ...,  1.7309e-02,\n",
            "           1.4059e-04, -1.2023e-01],\n",
            "         [-8.4353e-02,  1.0628e-01, -2.0550e-02,  ...,  7.8326e-02,\n",
            "          -1.1066e-01, -1.6828e-01]],\n",
            "\n",
            "        [[ 5.2068e-02,  6.7550e-02, -5.8188e-02,  ..., -9.3164e-02,\n",
            "          -1.8723e-01, -5.5011e-02],\n",
            "         [ 3.5111e-02,  8.6677e-02, -6.2506e-02,  ..., -9.6536e-02,\n",
            "          -1.2754e-01, -5.7744e-02],\n",
            "         [ 2.5390e-02,  1.0087e-01, -6.5536e-02,  ..., -9.0542e-02,\n",
            "          -9.9047e-02, -5.4191e-02],\n",
            "         ...,\n",
            "         [ 7.6058e-02, -9.7762e-02,  1.1774e-02,  ..., -2.5198e-01,\n",
            "          -3.4816e-04,  1.0557e-01],\n",
            "         [ 5.9417e-02, -2.0668e-01,  7.6839e-03,  ..., -2.0593e-01,\n",
            "           3.0615e-02,  1.0503e-01],\n",
            "         [-1.6820e-02, -2.8454e-01,  5.8151e-03,  ..., -1.8940e-01,\n",
            "           9.2373e-02,  3.2717e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.0543e-01, -8.4993e-01,  1.2893e-01,  ..., -5.7942e-01,\n",
            "          -7.3108e-02,  3.4317e-01],\n",
            "         [ 4.4757e-02, -6.3342e-01,  1.8479e-01,  ..., -5.2575e-01,\n",
            "          -4.0720e-02,  3.8595e-01],\n",
            "         [-1.4388e-02, -3.8278e-01,  1.6029e-01,  ..., -4.3407e-01,\n",
            "          -3.3176e-02,  4.1051e-01],\n",
            "         ...,\n",
            "         [ 9.3204e-03,  8.7353e-02, -1.2261e-01,  ..., -9.4918e-02,\n",
            "          -6.3948e-02, -5.9937e-02],\n",
            "         [-7.6739e-02,  6.9484e-02, -1.9123e-01,  ..., -8.5872e-02,\n",
            "          -1.0322e-01, -6.6539e-02],\n",
            "         [-1.4671e-02,  8.5489e-02, -1.4380e-01,  ..., -7.3582e-02,\n",
            "          -1.3066e-01, -7.5288e-02]],\n",
            "\n",
            "        [[ 4.6737e-02,  2.8242e-01, -1.3237e-01,  ..., -2.2766e-02,\n",
            "          -7.5371e-02, -1.1374e-01],\n",
            "         [ 4.7930e-02,  2.3823e-01, -1.1948e-01,  ..., -3.1152e-02,\n",
            "          -9.2395e-02, -9.7268e-02],\n",
            "         [ 3.9526e-02,  2.0329e-01, -1.0742e-01,  ..., -3.9854e-02,\n",
            "          -8.5556e-02, -8.7308e-02],\n",
            "         ...,\n",
            "         [ 1.4405e-01,  1.5941e-01, -5.2574e-02,  ..., -1.6566e-01,\n",
            "          -6.0588e-02, -4.1375e-02],\n",
            "         [ 5.2465e-02,  1.8953e-01,  1.5926e-02,  ..., -2.3962e-01,\n",
            "          -8.1927e-02,  1.1604e-01],\n",
            "         [ 9.9955e-02,  1.6383e-01,  7.4262e-02,  ..., -2.9017e-01,\n",
            "          -4.5157e-02,  1.8911e-01]],\n",
            "\n",
            "        [[ 9.4483e-02,  4.4472e-02, -3.0890e-02,  ..., -8.3468e-02,\n",
            "          -2.5078e-02,  3.9850e-02],\n",
            "         [ 4.6439e-02,  7.8363e-02, -5.7107e-02,  ..., -8.0530e-02,\n",
            "          -3.3336e-02, -3.1206e-03],\n",
            "         [ 2.2367e-02,  9.9360e-02, -7.3987e-02,  ..., -7.3202e-02,\n",
            "          -4.6624e-02, -2.8041e-02],\n",
            "         ...,\n",
            "         [-1.0803e-01,  1.0844e-01, -1.0093e-01,  ...,  8.8794e-02,\n",
            "          -1.0347e-01, -8.4315e-02],\n",
            "         [-6.3796e-02,  1.0015e-01, -1.2498e-01,  ...,  7.8253e-02,\n",
            "          -1.1928e-01, -1.0713e-01],\n",
            "         [ 2.4160e-04,  1.0345e-01, -1.3041e-01,  ...,  6.4994e-02,\n",
            "          -1.5052e-01, -1.0057e-01]]], device='cuda:0',\n",
            "       grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[-3.2772e-04,  7.1123e-02, -1.5171e-01,  ...,  3.0293e-03,\n",
            "          -8.1067e-02, -8.8986e-02],\n",
            "         [ 1.7252e-02,  1.1395e-01, -1.2712e-01,  ..., -1.5063e-02,\n",
            "          -8.9091e-02, -8.5129e-02],\n",
            "         [ 2.1271e-02,  1.3482e-01, -1.0985e-01,  ..., -3.2619e-02,\n",
            "          -8.4394e-02, -8.0074e-02],\n",
            "         ...,\n",
            "         [-5.8152e-02,  2.4472e-02,  2.3374e-02,  ..., -1.6221e-03,\n",
            "           7.0520e-03,  4.7329e-02],\n",
            "         [ 1.0624e-01, -7.7858e-02,  3.7710e-02,  ...,  1.5171e-02,\n",
            "          -4.4247e-02,  4.5323e-02],\n",
            "         [ 1.4497e-01, -1.3756e-01,  2.9965e-02,  ..., -1.1321e-01,\n",
            "           1.7635e-02,  6.9179e-02]],\n",
            "\n",
            "        [[ 1.0244e-02,  6.3212e-02, -5.8267e-02,  ..., -1.7940e-04,\n",
            "          -1.2672e-01, -8.3450e-02],\n",
            "         [ 2.0243e-02,  8.8997e-02, -6.3904e-02,  ..., -4.6541e-02,\n",
            "          -1.0723e-01, -7.2829e-02],\n",
            "         [ 1.8421e-02,  1.0775e-01, -6.9811e-02,  ..., -6.2263e-02,\n",
            "          -8.5976e-02, -6.3162e-02],\n",
            "         ...,\n",
            "         [ 1.4212e-02,  1.4105e-01, -1.2832e-02,  ..., -6.1398e-02,\n",
            "          -1.0263e-01, -1.8026e-02],\n",
            "         [ 4.5224e-03,  1.2968e-01,  2.7749e-02,  ..., -5.5509e-02,\n",
            "          -1.0021e-01, -1.8673e-02],\n",
            "         [ 7.4010e-02,  1.2896e-01,  4.2906e-02,  ..., -1.8482e-01,\n",
            "          -8.5265e-02,  6.7495e-03]],\n",
            "\n",
            "        [[ 1.8855e-02, -8.8897e-02,  4.7445e-03,  ..., -1.2163e-01,\n",
            "          -5.3421e-02, -1.5790e-02],\n",
            "         [ 1.9848e-02,  2.9512e-02, -2.9500e-02,  ..., -1.1366e-01,\n",
            "          -6.8502e-02, -4.1459e-02],\n",
            "         [ 1.4322e-02,  8.7165e-02, -5.4782e-02,  ..., -9.2887e-02,\n",
            "          -6.9140e-02, -5.4973e-02],\n",
            "         ...,\n",
            "         [ 6.3520e-03,  1.4283e-01,  9.2805e-03,  ..., -5.5762e-02,\n",
            "          -1.4450e-01, -1.0034e-01],\n",
            "         [ 6.2943e-02,  8.8764e-02,  6.4882e-02,  ..., -1.2336e-01,\n",
            "          -1.8376e-01, -1.2368e-01],\n",
            "         [ 1.1417e-01,  4.7798e-02,  4.1712e-02,  ..., -4.6647e-01,\n",
            "          -1.8271e-01,  3.0645e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 2.4559e-02,  1.2567e-01, -1.0859e-01,  ..., -5.8934e-02,\n",
            "          -1.3212e-01, -7.2016e-02],\n",
            "         [ 1.9212e-02,  1.3868e-01, -1.0680e-01,  ..., -5.9813e-02,\n",
            "          -9.4742e-02, -7.6969e-02],\n",
            "         [ 1.1183e-02,  1.4688e-01, -1.0304e-01,  ..., -5.5213e-02,\n",
            "          -7.3788e-02, -7.4722e-02],\n",
            "         ...,\n",
            "         [-1.1442e-01, -9.7273e-02, -1.5410e-01,  ..., -2.3505e-02,\n",
            "          -1.3910e-02, -1.5719e-01],\n",
            "         [-5.0333e-02, -1.2213e-01, -1.7995e-01,  ..., -4.1789e-02,\n",
            "          -1.9579e-02, -1.2631e-01],\n",
            "         [-5.9168e-02,  6.2595e-02, -1.1766e-01,  ..., -2.2822e-03,\n",
            "          -3.3112e-02, -1.6675e-01]],\n",
            "\n",
            "        [[ 3.4196e-02,  1.5825e-01,  7.3151e-02,  ..., -2.3664e-01,\n",
            "           6.7294e-03,  1.2603e-01],\n",
            "         [ 6.5555e-03,  1.2794e-01,  4.4713e-02,  ..., -1.7104e-01,\n",
            "          -1.9763e-02,  6.9828e-02],\n",
            "         [ 4.4666e-04,  1.1171e-01,  3.1251e-03,  ..., -1.2493e-01,\n",
            "          -4.6308e-02,  2.1140e-02],\n",
            "         ...,\n",
            "         [ 2.2491e-01, -1.7165e-01,  7.1434e-02,  ..., -5.8928e-01,\n",
            "          -6.6845e-02,  3.1412e-01],\n",
            "         [ 1.5766e-01, -4.3083e-02,  1.0748e-01,  ..., -5.5864e-01,\n",
            "          -1.2366e-01,  2.4803e-01],\n",
            "         [ 2.4838e-01, -2.3599e-01,  9.8685e-02,  ..., -6.8836e-01,\n",
            "          -9.6585e-02,  2.4856e-01]],\n",
            "\n",
            "        [[ 3.1684e-02,  1.0708e-01, -9.0082e-02,  ..., -2.1963e-03,\n",
            "          -1.4326e-01, -7.7043e-02],\n",
            "         [ 3.1178e-02,  1.1382e-01, -8.1714e-02,  ..., -4.8397e-02,\n",
            "          -1.1549e-01, -6.6800e-02],\n",
            "         [ 2.3642e-02,  1.2032e-01, -8.0454e-02,  ..., -6.5134e-02,\n",
            "          -9.3708e-02, -5.9608e-02],\n",
            "         ...,\n",
            "         [ 1.3983e-01, -5.2069e-01,  4.1134e-02,  ..., -5.7166e-01,\n",
            "          -4.6646e-02,  2.5358e-01],\n",
            "         [ 2.3723e-01, -7.5573e-01,  8.7204e-02,  ..., -6.1240e-01,\n",
            "           4.7722e-03,  4.3621e-01],\n",
            "         [ 2.1636e-01, -8.3659e-01,  8.4384e-02,  ..., -6.0757e-01,\n",
            "          -2.6504e-02,  4.3842e-01]]], device='cuda:0',\n",
            "       grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 5.4010e-02, -2.1348e-02,  7.7764e-03,  ..., -1.0810e-01,\n",
            "           6.3577e-03,  9.3454e-03],\n",
            "         [ 3.3459e-02,  5.4924e-02, -2.1441e-02,  ..., -9.3362e-02,\n",
            "          -3.6596e-02, -2.3170e-02],\n",
            "         [ 2.2235e-02,  9.0534e-02, -4.8540e-02,  ..., -8.1602e-02,\n",
            "          -5.8195e-02, -3.9983e-02],\n",
            "         ...,\n",
            "         [ 2.2661e-02,  1.0678e-02,  4.7211e-02,  ..., -2.0855e-02,\n",
            "          -2.1835e-01, -1.7853e-01],\n",
            "         [ 6.4794e-02,  2.9555e-02,  5.8377e-02,  ..., -8.6859e-02,\n",
            "          -4.8468e-01, -8.7722e-02],\n",
            "         [ 1.6666e-02,  3.1514e-02,  3.2064e-02,  ..., -7.2464e-02,\n",
            "          -5.1519e-01, -1.7136e-01]],\n",
            "\n",
            "        [[ 5.8721e-02,  1.3777e-01,  3.9819e-02,  ..., -1.4483e-01,\n",
            "          -7.3553e-02, -1.2279e-02],\n",
            "         [ 4.0708e-02,  1.1181e-01,  6.6781e-03,  ..., -1.2540e-01,\n",
            "          -6.6796e-02, -3.0948e-02],\n",
            "         [ 2.3847e-02,  1.1176e-01, -2.4106e-02,  ..., -1.0064e-01,\n",
            "          -7.1035e-02, -4.3388e-02],\n",
            "         ...,\n",
            "         [-1.7815e-02,  4.3815e-02,  7.7759e-03,  ...,  6.6860e-02,\n",
            "          -3.2490e-01, -1.2944e-01],\n",
            "         [-1.0372e-01,  6.6384e-02,  7.2768e-03,  ...,  1.3114e-01,\n",
            "          -3.2639e-01, -1.4517e-01],\n",
            "         [-3.0653e-02,  3.9497e-02,  2.1612e-02,  ...,  1.6737e-01,\n",
            "          -3.4148e-01, -9.4796e-02]],\n",
            "\n",
            "        [[ 7.6792e-02,  1.2641e-01,  5.5879e-02,  ..., -3.4381e-01,\n",
            "          -6.7531e-02,  2.1695e-02],\n",
            "         [ 4.8397e-02,  8.9426e-02,  3.6093e-02,  ..., -2.8566e-01,\n",
            "          -5.7991e-02,  3.7753e-03],\n",
            "         [ 2.5139e-02,  9.1510e-02,  5.1285e-03,  ..., -1.9739e-01,\n",
            "          -6.5592e-02, -1.6610e-02],\n",
            "         ...,\n",
            "         [ 1.3681e-02,  4.3446e-02,  5.2086e-02,  ...,  1.6091e-01,\n",
            "          -3.9521e-01, -2.5243e-01],\n",
            "         [ 3.2599e-02,  1.1182e-02, -4.6787e-05,  ...,  1.7653e-01,\n",
            "          -3.9432e-01, -1.6183e-01],\n",
            "         [ 1.2042e-02,  2.9172e-02,  1.6744e-02,  ...,  1.2501e-01,\n",
            "          -3.1500e-01, -1.6520e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.1060e-02,  1.1299e-01, -9.3778e-02,  ...,  2.2933e-03,\n",
            "          -1.0229e-01, -1.0773e-01],\n",
            "         [ 3.0261e-02,  1.3370e-01, -9.1735e-02,  ..., -2.0218e-02,\n",
            "          -1.0317e-01, -9.0467e-02],\n",
            "         [ 3.1142e-02,  1.4226e-01, -8.8084e-02,  ..., -4.0539e-02,\n",
            "          -9.1787e-02, -7.8352e-02],\n",
            "         ...,\n",
            "         [-7.3901e-02,  9.7262e-02, -1.2138e-01,  ...,  3.7333e-03,\n",
            "          -6.6043e-02, -1.2036e-01],\n",
            "         [-5.8138e-02,  6.8459e-02, -1.6933e-01,  ...,  3.2655e-02,\n",
            "          -1.6988e-02, -1.2467e-01],\n",
            "         [-7.6266e-02,  9.3693e-02,  2.0741e-02,  ...,  3.8689e-02,\n",
            "          -1.3349e-01, -1.1511e-01]],\n",
            "\n",
            "        [[ 1.5383e-01, -1.0411e-01,  1.2591e-01,  ..., -5.1454e-01,\n",
            "          -2.5838e-02,  3.4756e-01],\n",
            "         [ 6.8461e-02, -1.5519e-02,  1.2753e-01,  ..., -4.6943e-01,\n",
            "          -2.8625e-02,  3.5829e-01],\n",
            "         [ 1.2537e-02,  3.8844e-02,  9.4737e-02,  ..., -3.6053e-01,\n",
            "          -4.0738e-02,  3.0188e-01],\n",
            "         ...,\n",
            "         [ 8.0815e-02, -7.3046e-02, -1.6080e-01,  ..., -6.7344e-02,\n",
            "           5.9052e-02, -1.6107e-01],\n",
            "         [ 4.0922e-02,  3.0948e-02, -5.1768e-02,  ..., -1.1356e-02,\n",
            "          -4.1987e-02, -1.0246e-01],\n",
            "         [ 3.0953e-02, -3.7544e-02, -1.0672e-01,  ..., -1.0155e-01,\n",
            "          -4.3003e-02, -7.0707e-02]],\n",
            "\n",
            "        [[ 8.1509e-02, -7.1256e-01,  1.2444e-01,  ..., -4.1044e-01,\n",
            "          -4.8945e-02,  3.4242e-01],\n",
            "         [ 1.9046e-02, -4.7549e-01,  1.2956e-01,  ..., -3.4648e-01,\n",
            "          -4.5349e-02,  3.4659e-01],\n",
            "         [-5.0683e-04, -2.4108e-01,  9.3700e-02,  ..., -2.3413e-01,\n",
            "          -4.8544e-02,  3.0353e-01],\n",
            "         ...,\n",
            "         [-2.5100e-02,  1.2142e-01, -1.0624e-01,  ..., -8.8153e-02,\n",
            "          -1.2739e-01, -1.3690e-01],\n",
            "         [-2.0740e-03,  1.6588e-01, -6.7176e-02,  ..., -3.3614e-02,\n",
            "          -1.1574e-01, -1.5453e-01],\n",
            "         [ 8.8386e-02,  1.0092e-01, -1.2874e-01,  ..., -6.4627e-02,\n",
            "          -6.6913e-02, -9.0595e-02]]], device='cuda:0',\n",
            "       grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 1.8787e-02,  1.2729e-02, -6.2680e-03,  ..., -1.4122e-01,\n",
            "          -3.1746e-01, -1.0355e-01],\n",
            "         [ 1.4726e-02,  2.3011e-02, -1.5948e-02,  ..., -1.3980e-01,\n",
            "          -2.3838e-01, -6.9173e-02],\n",
            "         [ 1.8346e-02,  4.3409e-02, -2.0585e-02,  ..., -1.3565e-01,\n",
            "          -1.7844e-01, -4.9007e-02],\n",
            "         ...,\n",
            "         [ 1.6041e-03,  1.5638e-01, -7.8118e-02,  ..., -1.9568e-02,\n",
            "          -9.3173e-02, -8.7884e-02],\n",
            "         [ 5.8601e-02,  4.8792e-02, -5.0030e-02,  ..., -7.0478e-02,\n",
            "          -7.6778e-02, -1.2589e-01],\n",
            "         [ 2.4415e-02,  6.0149e-02, -1.0333e-01,  ..., -1.1569e-01,\n",
            "          -2.7031e-02, -1.5784e-01]],\n",
            "\n",
            "        [[-1.5365e-02,  2.2176e-02, -1.4177e-02,  ..., -1.8535e-03,\n",
            "          -2.6251e-01, -7.1559e-02],\n",
            "         [ 3.4119e-03,  2.9250e-02, -2.0274e-02,  ..., -7.7659e-02,\n",
            "          -2.3662e-01, -5.2334e-02],\n",
            "         [ 1.6461e-02,  4.4808e-02, -2.3831e-02,  ..., -1.1190e-01,\n",
            "          -1.8895e-01, -4.0873e-02],\n",
            "         ...,\n",
            "         [ 3.8345e-02,  1.5312e-01, -3.1107e-02,  ...,  7.4566e-03,\n",
            "          -1.0967e-01, -5.5816e-02],\n",
            "         [-1.1191e-02,  1.9101e-01, -2.9604e-02,  ...,  5.6242e-03,\n",
            "          -6.7408e-02, -7.5383e-02],\n",
            "         [-1.0132e-01,  5.7154e-02, -1.8010e-01,  ..., -4.3320e-02,\n",
            "           6.7634e-02, -6.3076e-02]],\n",
            "\n",
            "        [[-1.3188e-03,  1.5468e-02, -4.7733e-03,  ...,  6.4385e-02,\n",
            "          -3.5061e-01, -1.0522e-01],\n",
            "         [ 6.8450e-03,  1.1950e-03, -9.7222e-03,  ..., -4.3251e-02,\n",
            "          -3.4379e-01, -6.5554e-02],\n",
            "         [ 1.3874e-02,  3.7965e-03, -1.3056e-02,  ..., -9.9338e-02,\n",
            "          -3.3726e-01, -4.8350e-02],\n",
            "         ...,\n",
            "         [-7.3372e-02,  7.6536e-02, -3.7405e-02,  ..., -3.7649e-03,\n",
            "          -9.1038e-02, -2.4835e-02],\n",
            "         [-3.6484e-02,  1.1031e-01, -7.9507e-02,  ...,  3.9301e-02,\n",
            "          -1.2424e-01, -7.9723e-02],\n",
            "         [-4.0539e-02,  1.3367e-01, -9.6419e-02,  ...,  9.1790e-02,\n",
            "          -1.3299e-01, -7.3636e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 7.1537e-03,  5.2851e-02, -2.5339e-02,  ..., -8.3542e-03,\n",
            "          -9.7227e-02, -7.7729e-02],\n",
            "         [ 2.2547e-02,  7.7415e-02, -3.4939e-02,  ..., -4.7218e-02,\n",
            "          -8.9031e-02, -7.3390e-02],\n",
            "         [ 2.6581e-02,  9.6525e-02, -4.3028e-02,  ..., -6.8830e-02,\n",
            "          -8.4810e-02, -6.0569e-02],\n",
            "         ...,\n",
            "         [ 1.2699e-01,  8.8321e-02,  3.2777e-02,  ..., -1.8851e-01,\n",
            "          -3.6479e-01,  3.6639e-02],\n",
            "         [ 6.3823e-02,  8.3546e-02,  5.5401e-02,  ..., -2.5044e-02,\n",
            "          -3.6660e-01, -3.1220e-02],\n",
            "         [ 1.7225e-03,  8.0601e-02,  8.9520e-02,  ...,  1.4460e-01,\n",
            "          -3.4047e-01, -8.4172e-02]],\n",
            "\n",
            "        [[ 5.3976e-02,  4.9912e-02, -6.3307e-02,  ..., -8.7210e-02,\n",
            "          -8.5638e-02, -6.8033e-02],\n",
            "         [ 3.6526e-02,  9.2323e-02, -7.1028e-02,  ..., -9.1000e-02,\n",
            "          -7.4460e-02, -7.0646e-02],\n",
            "         [ 2.0687e-02,  1.1667e-01, -7.7147e-02,  ..., -8.0363e-02,\n",
            "          -6.8546e-02, -6.8080e-02],\n",
            "         ...,\n",
            "         [ 3.2595e-04, -2.7688e-01, -1.9527e-01,  ..., -1.8453e-02,\n",
            "           2.0540e-01, -9.3769e-02],\n",
            "         [-3.0582e-02, -2.6630e-01, -2.3029e-01,  ..., -1.3724e-02,\n",
            "           2.4132e-01, -8.9180e-02],\n",
            "         [-1.8532e-03, -2.4106e-01, -2.5973e-01,  ..., -1.4725e-02,\n",
            "           2.3189e-01, -1.0195e-01]],\n",
            "\n",
            "        [[ 4.9530e-02,  1.8005e-01, -6.4776e-02,  ..., -5.1777e-02,\n",
            "          -6.5194e-02, -8.1085e-02],\n",
            "         [ 2.9685e-02,  1.5255e-01, -7.4740e-02,  ..., -6.0017e-02,\n",
            "          -6.4517e-02, -6.7879e-02],\n",
            "         [ 1.9955e-02,  1.4258e-01, -8.2345e-02,  ..., -6.3678e-02,\n",
            "          -6.6024e-02, -6.1313e-02],\n",
            "         ...,\n",
            "         [-1.8475e-02,  8.0163e-02, -1.0046e-02,  ...,  1.2580e-01,\n",
            "          -2.1268e-01, -1.9301e-01],\n",
            "         [-3.6781e-02,  4.1438e-02, -2.2681e-02,  ...,  1.0362e-01,\n",
            "          -2.6579e-01, -1.0041e-01],\n",
            "         [-4.3887e-02,  3.1394e-02,  1.3970e-02,  ...,  6.2889e-02,\n",
            "          -3.5633e-01, -8.0880e-02]]], device='cuda:0',\n",
            "       grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 0.0353,  0.1279, -0.0694,  ..., -0.1028, -0.0906, -0.1038],\n",
            "         [ 0.0093,  0.1347, -0.0846,  ..., -0.0951, -0.0686, -0.0917],\n",
            "         [-0.0012,  0.1401, -0.0895,  ..., -0.0762, -0.0633, -0.0790],\n",
            "         ...,\n",
            "         [-0.1242,  0.1939, -0.1703,  ..., -0.1992,  0.1829, -0.1722],\n",
            "         [-0.0355,  0.0708, -0.1947,  ..., -0.2403,  0.1770, -0.0948],\n",
            "         [-0.0092, -0.0114, -0.1701,  ..., -0.2031,  0.1363, -0.0640]],\n",
            "\n",
            "        [[ 0.0195,  0.1022, -0.0762,  ..., -0.0306, -0.0700, -0.0574],\n",
            "         [ 0.0323,  0.1259, -0.0861,  ..., -0.0425, -0.0732, -0.0650],\n",
            "         [ 0.0256,  0.1405, -0.0890,  ..., -0.0472, -0.0663, -0.0677],\n",
            "         ...,\n",
            "         [ 0.0181,  0.1410, -0.0899,  ...,  0.0226, -0.1202, -0.0917],\n",
            "         [-0.0053,  0.0991,  0.0333,  ..., -0.0047, -0.1288, -0.0780],\n",
            "         [-0.0468,  0.0662,  0.1210,  ..., -0.0106, -0.1629, -0.0042]],\n",
            "\n",
            "        [[ 0.0328,  0.0979, -0.0996,  ...,  0.0219, -0.1216, -0.0715],\n",
            "         [ 0.0350,  0.1157, -0.0947,  ..., -0.0306, -0.0936, -0.0804],\n",
            "         [ 0.0278,  0.1255, -0.0895,  ..., -0.0523, -0.0788, -0.0754],\n",
            "         ...,\n",
            "         [-0.0191,  0.1948, -0.0620,  ...,  0.0074, -0.1655, -0.1320],\n",
            "         [-0.0173,  0.1702, -0.0441,  ...,  0.0266, -0.1270, -0.1809],\n",
            "         [-0.0361,  0.1333, -0.0436,  ...,  0.0345, -0.0529, -0.2337]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0454,  0.0142,  0.0078,  ..., -0.0400, -0.2794, -0.0487],\n",
            "         [ 0.0372,  0.0188, -0.0142,  ..., -0.1115, -0.2232, -0.0529],\n",
            "         [ 0.0340,  0.0411, -0.0226,  ..., -0.1289, -0.1668, -0.0459],\n",
            "         ...,\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588]],\n",
            "\n",
            "        [[ 0.0311, -0.0449, -0.1091,  ..., -0.0055, -0.1011, -0.0914],\n",
            "         [ 0.0405,  0.0764, -0.1023,  ..., -0.0197, -0.1200, -0.0877],\n",
            "         [ 0.0399,  0.1301, -0.0917,  ..., -0.0308, -0.1090, -0.0842],\n",
            "         ...,\n",
            "         [ 0.1260, -0.0039, -0.1438,  ..., -0.3785, -0.0410,  0.0921],\n",
            "         [ 0.1854, -0.0802, -0.0596,  ..., -0.4753,  0.0429,  0.2414],\n",
            "         [ 0.1238, -0.0337, -0.0008,  ..., -0.4986,  0.0569,  0.3918]],\n",
            "\n",
            "        [[ 0.0134,  0.0228, -0.0126,  ..., -0.0433, -0.2862, -0.0483],\n",
            "         [ 0.0196,  0.0388, -0.0210,  ..., -0.0955, -0.2257, -0.0445],\n",
            "         [ 0.0209,  0.0607, -0.0280,  ..., -0.1101, -0.1645, -0.0381],\n",
            "         ...,\n",
            "         [-0.0459,  0.0464, -0.1903,  ...,  0.0827, -0.0594, -0.1993],\n",
            "         [-0.0019,  0.0727, -0.1366,  ...,  0.0864, -0.2398, -0.1602],\n",
            "         [-0.1208,  0.0682, -0.2756,  ...,  0.0569, -0.1287, -0.1704]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[-3.7439e-04,  7.2186e-02, -1.0745e-01,  ..., -1.1983e-01,\n",
            "          -6.5444e-02, -6.8018e-02],\n",
            "         [ 1.7243e-02,  1.2336e-01, -8.5436e-02,  ..., -1.1360e-01,\n",
            "          -8.9633e-02, -6.5226e-02],\n",
            "         [ 1.5918e-02,  1.4983e-01, -7.4186e-02,  ..., -8.8980e-02,\n",
            "          -8.3887e-02, -7.3016e-02],\n",
            "         ...,\n",
            "         [-1.2877e-01, -1.9225e-04, -1.0149e-01,  ..., -2.3152e-02,\n",
            "           5.0669e-03, -3.8176e-02],\n",
            "         [-1.3719e-01, -7.4156e-03, -4.9675e-02,  ..., -1.7171e-02,\n",
            "           3.9251e-02, -1.0860e-01],\n",
            "         [-5.7763e-02,  1.4365e-02, -1.1485e-02,  ..., -2.5527e-02,\n",
            "          -6.1383e-02, -1.3447e-01]],\n",
            "\n",
            "        [[ 9.5937e-03,  2.8429e-02,  2.4320e-02,  ..., -7.4585e-02,\n",
            "          -9.6093e-02, -3.2030e-02],\n",
            "         [ 1.8110e-02,  5.0679e-02, -1.3699e-02,  ..., -9.5887e-02,\n",
            "          -7.7770e-02, -4.2024e-02],\n",
            "         [ 2.0347e-02,  7.4621e-02, -3.2184e-02,  ..., -9.9064e-02,\n",
            "          -7.7151e-02, -4.3266e-02],\n",
            "         ...,\n",
            "         [-8.7059e-02,  7.3407e-02, -2.9920e-01,  ..., -9.4149e-02,\n",
            "           1.2534e-01, -1.8679e-01],\n",
            "         [-1.1064e-01,  1.0381e-01, -2.9088e-01,  ..., -2.9257e-02,\n",
            "           1.6738e-01, -1.4004e-01],\n",
            "         [-2.3724e-02,  1.7278e-01, -1.6644e-01,  ..., -1.8453e-02,\n",
            "           6.1662e-02, -9.3418e-02]],\n",
            "\n",
            "        [[ 1.8607e-02,  1.3290e-01, -4.4424e-02,  ..., -3.8765e-03,\n",
            "          -1.1684e-01, -1.0466e-01],\n",
            "         [ 2.5393e-02,  1.2478e-01, -6.5740e-02,  ..., -4.9845e-02,\n",
            "          -1.0182e-01, -7.8531e-02],\n",
            "         [ 2.0971e-02,  1.2761e-01, -7.4100e-02,  ..., -6.7274e-02,\n",
            "          -8.5965e-02, -6.5034e-02],\n",
            "         ...,\n",
            "         [ 5.6183e-03,  1.3523e-01, -9.6390e-02,  ..., -6.1194e-02,\n",
            "          -6.2959e-02, -5.8787e-02],\n",
            "         [ 5.6183e-03,  1.3523e-01, -9.6390e-02,  ..., -6.1194e-02,\n",
            "          -6.2959e-02, -5.8787e-02],\n",
            "         [ 5.6183e-03,  1.3523e-01, -9.6390e-02,  ..., -6.1194e-02,\n",
            "          -6.2959e-02, -5.8787e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 5.6183e-03,  1.3523e-01, -9.6390e-02,  ..., -6.1194e-02,\n",
            "          -6.2959e-02, -5.8787e-02],\n",
            "         [ 5.6183e-03,  1.3523e-01, -9.6390e-02,  ..., -6.1194e-02,\n",
            "          -6.2959e-02, -5.8787e-02],\n",
            "         [ 5.6183e-03,  1.3523e-01, -9.6390e-02,  ..., -6.1194e-02,\n",
            "          -6.2959e-02, -5.8787e-02],\n",
            "         ...,\n",
            "         [ 6.9477e-02,  7.6261e-02, -7.0794e-02,  ..., -2.3189e-02,\n",
            "          -1.7454e-01, -1.1861e-01],\n",
            "         [ 1.5538e-02,  8.3548e-02, -1.0264e-01,  ...,  2.8179e-02,\n",
            "          -1.4401e-01, -1.2769e-01],\n",
            "         [-7.4444e-02,  1.3637e-01, -8.8582e-02,  ...,  8.9971e-02,\n",
            "          -8.2255e-02, -1.3144e-01]],\n",
            "\n",
            "        [[ 4.8003e-02,  7.5119e-02,  5.5031e-02,  ..., -3.7628e-01,\n",
            "          -3.1090e-02,  3.2457e-01],\n",
            "         [-1.3860e-02,  5.9792e-02,  6.3280e-02,  ..., -3.1398e-01,\n",
            "          -2.9450e-02,  2.7583e-01],\n",
            "         [-3.2216e-02,  7.3451e-02,  3.8000e-02,  ..., -2.1227e-01,\n",
            "          -3.8612e-02,  1.9865e-01],\n",
            "         ...,\n",
            "         [ 5.8665e-02,  3.0645e-02, -6.1096e-02,  ..., -2.9792e-01,\n",
            "          -8.5746e-02,  7.0489e-02],\n",
            "         [ 4.1054e-02,  7.8027e-02,  6.2686e-02,  ..., -3.8018e-01,\n",
            "          -9.1546e-02,  1.7485e-01],\n",
            "         [ 1.5923e-01, -2.2429e-01,  1.0752e-01,  ..., -5.8937e-01,\n",
            "           2.8226e-03,  3.8236e-01]],\n",
            "\n",
            "        [[ 2.6552e-03,  9.6918e-02, -1.1941e-01,  ...,  1.7590e-02,\n",
            "          -1.5417e-01, -8.3763e-02],\n",
            "         [ 2.5259e-02,  1.1337e-01, -9.7644e-02,  ..., -3.0686e-02,\n",
            "          -1.2160e-01, -7.5426e-02],\n",
            "         [ 2.4643e-02,  1.2707e-01, -8.7876e-02,  ..., -5.2177e-02,\n",
            "          -9.0440e-02, -6.9182e-02],\n",
            "         ...,\n",
            "         [-5.0787e-02,  8.1907e-02, -2.1463e-02,  ...,  4.7874e-02,\n",
            "          -1.3904e-01, -1.0283e-01],\n",
            "         [-3.5006e-02,  3.5397e-02,  5.8260e-03,  ...,  8.7265e-02,\n",
            "          -1.6093e-01, -1.1073e-01],\n",
            "         [ 4.7250e-03,  2.8914e-02,  2.5765e-02,  ...,  1.0975e-01,\n",
            "          -2.3029e-01, -1.3059e-01]]], device='cuda:0',\n",
            "       grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[-0.0475,  0.0657, -0.0411,  ..., -0.0379, -0.0531, -0.1305],\n",
            "         [-0.0143,  0.0941, -0.0557,  ..., -0.0462, -0.0676, -0.0903],\n",
            "         [ 0.0015,  0.1117, -0.0669,  ..., -0.0556, -0.0727, -0.0713],\n",
            "         ...,\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [-0.1262,  0.2022, -0.0538,  ..., -0.0268, -0.0974, -0.1294],\n",
            "         [-0.0500,  0.1041, -0.1183,  ..., -0.0433, -0.1087, -0.1324]],\n",
            "\n",
            "        [[ 0.0364,  0.2071, -0.0958,  ..., -0.0214, -0.0886, -0.0769],\n",
            "         [ 0.0444,  0.1955, -0.0920,  ..., -0.0314, -0.0975, -0.0749],\n",
            "         [ 0.0384,  0.1795, -0.0892,  ..., -0.0377, -0.0907, -0.0727],\n",
            "         ...,\n",
            "         [-0.0349,  0.0161,  0.0238,  ...,  0.1536, -0.3320, -0.1641],\n",
            "         [-0.0658,  0.0308,  0.0125,  ...,  0.0922, -0.2639, -0.0823],\n",
            "         [-0.1198,  0.0221,  0.0125,  ...,  0.0727, -0.3246, -0.0910]],\n",
            "\n",
            "        [[ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         ...,\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0102,  0.0982, -0.0877,  ...,  0.0529, -0.0966, -0.1029],\n",
            "         [ 0.0277,  0.1176, -0.0858,  ..., -0.0020, -0.0894, -0.0934],\n",
            "         [ 0.0252,  0.1286, -0.0874,  ..., -0.0346, -0.0777, -0.0816],\n",
            "         ...,\n",
            "         [-0.0790,  0.1072, -0.2420,  ..., -0.0468, -0.0319, -0.0927],\n",
            "         [-0.1252,  0.1074, -0.3649,  ...,  0.0038,  0.0405, -0.1478],\n",
            "         [-0.0734,  0.1371, -0.1469,  ...,  0.0056, -0.0140, -0.1018]],\n",
            "\n",
            "        [[ 0.0744, -0.1017,  0.1188,  ..., -0.4649, -0.0108,  0.4158],\n",
            "         [ 0.0133,  0.0036,  0.1185,  ..., -0.3996, -0.0235,  0.3266],\n",
            "         [-0.0181,  0.0557,  0.0805,  ..., -0.2884, -0.0373,  0.2336],\n",
            "         ...,\n",
            "         [-0.0366,  0.0264, -0.0224,  ..., -0.0608, -0.1922, -0.0301],\n",
            "         [ 0.0098,  0.0470, -0.1522,  ..., -0.0573, -0.1430, -0.0578],\n",
            "         [-0.0494,  0.1143, -0.0933,  ...,  0.0221, -0.0962, -0.0715]],\n",
            "\n",
            "        [[ 0.0300,  0.0272, -0.0116,  ...,  0.0236, -0.2100, -0.0772],\n",
            "         [ 0.0303,  0.0409, -0.0258,  ..., -0.0526, -0.1699, -0.0634],\n",
            "         [ 0.0296,  0.0621, -0.0336,  ..., -0.0851, -0.1372, -0.0509],\n",
            "         ...,\n",
            "         [ 0.1070, -0.8472, -0.0224,  ..., -0.5444,  0.0669,  0.2881],\n",
            "         [ 0.0558, -0.8709, -0.0119,  ..., -0.4741,  0.0107,  0.3118],\n",
            "         [ 0.0381, -0.8423,  0.0122,  ..., -0.4894, -0.0203,  0.3267]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 0.0043,  0.1071, -0.0718,  ..., -0.0694, -0.1122, -0.0830],\n",
            "         [ 0.0039,  0.1129, -0.0715,  ..., -0.0808, -0.0866, -0.0743],\n",
            "         [ 0.0056,  0.1181, -0.0721,  ..., -0.0787, -0.0775, -0.0626],\n",
            "         ...,\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588]],\n",
            "\n",
            "        [[-0.0133,  0.0124, -0.0011,  ...,  0.0417, -0.2805, -0.0287],\n",
            "         [ 0.0015,  0.0113, -0.0091,  ..., -0.0413, -0.3191, -0.0372],\n",
            "         [ 0.0120,  0.0196, -0.0138,  ..., -0.0920, -0.2904, -0.0323],\n",
            "         ...,\n",
            "         [ 0.0110, -0.0727, -0.1903,  ..., -0.1961,  0.1121, -0.1041],\n",
            "         [ 0.0859, -0.1666, -0.0710,  ..., -0.3099,  0.0826, -0.0732],\n",
            "         [ 0.0261, -0.0632, -0.0297,  ..., -0.1960, -0.0662, -0.1246]],\n",
            "\n",
            "        [[ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         ...,\n",
            "         [-0.2378,  0.0782, -0.0017,  ...,  0.1571, -0.3384, -0.1529],\n",
            "         [-0.1554,  0.0217,  0.0150,  ...,  0.0711, -0.4008, -0.0763],\n",
            "         [-0.1007,  0.0473,  0.0134,  ...,  0.0546, -0.4233, -0.1327]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0208,  0.1628, -0.1292,  ..., -0.0063, -0.0607, -0.0735],\n",
            "         [ 0.0237,  0.1649, -0.1143,  ..., -0.0284, -0.0614, -0.0740],\n",
            "         [ 0.0173,  0.1573, -0.1065,  ..., -0.0423, -0.0581, -0.0705],\n",
            "         ...,\n",
            "         [ 0.0572, -0.0954, -0.0779,  ..., -0.3312,  0.0427, -0.0816],\n",
            "         [ 0.1183, -0.3078, -0.0956,  ..., -0.3293,  0.0216,  0.0396],\n",
            "         [ 0.0516, -0.3604, -0.0603,  ..., -0.2860,  0.0423,  0.0733]],\n",
            "\n",
            "        [[ 0.0354,  0.0833, -0.0546,  ..., -0.0308, -0.1144, -0.0339],\n",
            "         [ 0.0401,  0.0967, -0.0574,  ..., -0.0666, -0.1030, -0.0362],\n",
            "         [ 0.0302,  0.1086, -0.0666,  ..., -0.0748, -0.0851, -0.0407],\n",
            "         ...,\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [-0.0057,  0.1486, -0.0833,  ..., -0.0188, -0.0714, -0.0763],\n",
            "         [-0.1301,  0.1482, -0.0470,  ...,  0.0030, -0.1154, -0.0825]],\n",
            "\n",
            "        [[-0.0147, -0.7805,  0.0300,  ..., -0.3047, -0.0511,  0.3008],\n",
            "         [-0.0268, -0.5963,  0.0341,  ..., -0.2156, -0.0605,  0.2585],\n",
            "         [-0.0180, -0.3310,  0.0196,  ..., -0.1413, -0.0615,  0.1593],\n",
            "         ...,\n",
            "         [-0.0566,  0.1776, -0.0738,  ...,  0.0277, -0.1119, -0.0941],\n",
            "         [-0.0603,  0.0875, -0.0812,  ...,  0.0136, -0.0724, -0.2203],\n",
            "         [ 0.0082,  0.0733, -0.1306,  ..., -0.0341, -0.0527, -0.1557]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         ...,\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [-0.0272,  0.0885, -0.2383,  ..., -0.0678, -0.0066, -0.0568],\n",
            "         [-0.0383,  0.1072, -0.1455,  ...,  0.0111, -0.0174, -0.0787]],\n",
            "\n",
            "        [[ 0.0229,  0.0212, -0.0464,  ..., -0.1666, -0.0803, -0.0868],\n",
            "         [ 0.0100,  0.0773, -0.0535,  ..., -0.1271, -0.0713, -0.0827],\n",
            "         [ 0.0075,  0.1033, -0.0625,  ..., -0.0998, -0.0714, -0.0734],\n",
            "         ...,\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [-0.0411,  0.1920, -0.0530,  ...,  0.0218, -0.0802, -0.1123]],\n",
            "\n",
            "        [[-0.0186,  0.0345, -0.0051,  ..., -0.0376, -0.3114, -0.0602],\n",
            "         [ 0.0038,  0.0360, -0.0160,  ..., -0.0951, -0.2540, -0.0491],\n",
            "         [ 0.0162,  0.0499, -0.0217,  ..., -0.1166, -0.1907, -0.0398],\n",
            "         ...,\n",
            "         [ 0.0975,  0.0623,  0.0242,  ..., -0.2235, -0.0997,  0.0664],\n",
            "         [ 0.0487,  0.0449,  0.0481,  ..., -0.2236, -0.0370,  0.0972],\n",
            "         [ 0.1263, -0.1480,  0.0235,  ..., -0.4599,  0.0156,  0.1122]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0283, -0.2516, -0.0226,  ..., -0.2190, -0.0909,  0.0495],\n",
            "         [ 0.0162, -0.0853, -0.0187,  ..., -0.1746, -0.0825,  0.0127],\n",
            "         [ 0.0058,  0.0359, -0.0309,  ..., -0.1179, -0.0740, -0.0310],\n",
            "         ...,\n",
            "         [-0.0486,  0.1150,  0.0185,  ...,  0.1537, -0.1965, -0.1753],\n",
            "         [-0.1038,  0.0878,  0.0306,  ...,  0.0989, -0.2775, -0.1705],\n",
            "         [-0.0939,  0.0509,  0.0537,  ...,  0.1245, -0.3908, -0.1936]],\n",
            "\n",
            "        [[-0.0066,  0.0691, -0.0757,  ..., -0.0604, -0.1320, -0.0535],\n",
            "         [ 0.0097,  0.0889, -0.0766,  ..., -0.0839, -0.1065, -0.0597],\n",
            "         [ 0.0125,  0.1054, -0.0771,  ..., -0.0839, -0.0872, -0.0562],\n",
            "         ...,\n",
            "         [-0.0090,  0.1345, -0.0585,  ...,  0.0123, -0.0952, -0.0696],\n",
            "         [-0.1285,  0.1526, -0.0363,  ...,  0.0332, -0.0415, -0.1604],\n",
            "         [-0.0244,  0.0951, -0.0524,  ...,  0.0068, -0.0367, -0.0921]],\n",
            "\n",
            "        [[ 0.0177,  0.1536, -0.0635,  ..., -0.0683, -0.0594, -0.0762],\n",
            "         [ 0.0011,  0.1430, -0.0777,  ..., -0.0839, -0.0423, -0.0593],\n",
            "         [-0.0075,  0.1437, -0.0833,  ..., -0.0750, -0.0438, -0.0568],\n",
            "         ...,\n",
            "         [-0.0313, -0.0980,  0.0008,  ..., -0.0117, -0.1677, -0.1433],\n",
            "         [ 0.0199, -0.1519, -0.0530,  ..., -0.1048, -0.0807, -0.1533],\n",
            "         [ 0.0796, -0.1716, -0.1215,  ..., -0.1422, -0.0876, -0.1877]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 0.0253,  0.1224, -0.0896,  ..., -0.0138, -0.0593, -0.0652],\n",
            "         [ 0.0200,  0.1343, -0.0886,  ..., -0.0415, -0.0575, -0.0685],\n",
            "         [ 0.0125,  0.1374, -0.0900,  ..., -0.0530, -0.0566, -0.0662],\n",
            "         ...,\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [-0.2135,  0.2077, -0.0118,  ...,  0.0355, -0.0606, -0.0802]],\n",
            "\n",
            "        [[ 0.0190,  0.1294, -0.0677,  ..., -0.0267, -0.1049, -0.0623],\n",
            "         [ 0.0182,  0.1254, -0.0831,  ..., -0.0581, -0.0873, -0.0601],\n",
            "         [ 0.0119,  0.1278, -0.0875,  ..., -0.0646, -0.0737, -0.0578],\n",
            "         ...,\n",
            "         [ 0.0604,  0.1044, -0.0488,  ..., -0.2861, -0.0065,  0.0097],\n",
            "         [ 0.0226,  0.1539,  0.0050,  ..., -0.1537, -0.0684,  0.0287],\n",
            "         [ 0.0207,  0.0078,  0.0033,  ..., -0.1310,  0.0015,  0.0284]],\n",
            "\n",
            "        [[ 0.0395, -0.0228,  0.0455,  ..., -0.2915, -0.0095,  0.1557],\n",
            "         [ 0.0008,  0.0520,  0.0275,  ..., -0.2222, -0.0229,  0.1119],\n",
            "         [-0.0090,  0.0898, -0.0038,  ..., -0.1450, -0.0415,  0.0546],\n",
            "         ...,\n",
            "         [-0.0217,  0.1762, -0.0826,  ..., -0.0038, -0.0873, -0.0025],\n",
            "         [-0.0113,  0.1121, -0.0707,  ..., -0.0731,  0.0200, -0.0088],\n",
            "         [ 0.0050,  0.0545, -0.1173,  ..., -0.1131,  0.1040, -0.0341]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0189,  0.0131,  0.0014,  ..., -0.0202, -0.3194, -0.0912],\n",
            "         [ 0.0047,  0.0157, -0.0130,  ..., -0.1020, -0.2615, -0.0645],\n",
            "         [ 0.0159,  0.0361, -0.0189,  ..., -0.1262, -0.1979, -0.0493],\n",
            "         ...,\n",
            "         [ 0.0361, -0.0130, -0.0791,  ..., -0.1058, -0.1052, -0.0964],\n",
            "         [ 0.0160,  0.0147, -0.0724,  ..., -0.0935, -0.0203, -0.0703],\n",
            "         [-0.0154, -0.0328, -0.0359,  ..., -0.0366, -0.0356, -0.1182]],\n",
            "\n",
            "        [[ 0.0267,  0.0964, -0.0577,  ..., -0.0244, -0.0921, -0.0589],\n",
            "         [ 0.0305,  0.1122, -0.0674,  ..., -0.0520, -0.0919, -0.0524],\n",
            "         [ 0.0243,  0.1246, -0.0751,  ..., -0.0616, -0.0823, -0.0508],\n",
            "         ...,\n",
            "         [-0.0076,  0.0345,  0.0700,  ...,  0.0792, -0.2963, -0.1046],\n",
            "         [-0.0873,  0.0624,  0.0529,  ...,  0.0997, -0.2581, -0.1212],\n",
            "         [-0.0736,  0.0498,  0.0392,  ...,  0.1075, -0.3249, -0.1294]],\n",
            "\n",
            "        [[ 0.0370, -0.0471, -0.0673,  ..., -0.0960, -0.0610, -0.1384],\n",
            "         [ 0.0187,  0.0549, -0.0675,  ..., -0.0757, -0.0577, -0.0961],\n",
            "         [ 0.0126,  0.1013, -0.0738,  ..., -0.0637, -0.0607, -0.0751],\n",
            "         ...,\n",
            "         [ 0.0653,  0.1029, -0.0342,  ...,  0.0849, -0.2668, -0.1632],\n",
            "         [ 0.1167,  0.0266, -0.0468,  ...,  0.2045, -0.3765, -0.2433],\n",
            "         [ 0.1212,  0.0678,  0.0297,  ...,  0.2267, -0.2601, -0.2830]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[-2.2061e-02,  8.8117e-02, -5.1148e-02,  ..., -5.6052e-03,\n",
            "          -1.1144e-01, -4.5996e-02],\n",
            "         [ 6.3300e-03,  9.5338e-02, -6.7634e-02,  ..., -5.6632e-02,\n",
            "          -1.0091e-01, -5.4896e-02],\n",
            "         [ 9.8956e-03,  1.1001e-01, -7.2705e-02,  ..., -7.0969e-02,\n",
            "          -8.2989e-02, -5.5503e-02],\n",
            "         ...,\n",
            "         [-9.4609e-02,  3.3216e-02, -5.8356e-03,  ...,  1.3125e-01,\n",
            "          -4.4559e-01, -9.5793e-02],\n",
            "         [ 1.4441e-02,  2.3930e-02, -5.6104e-03,  ...,  9.7108e-02,\n",
            "          -3.8143e-01, -6.2982e-02],\n",
            "         [-4.3732e-02,  2.5264e-02, -1.1747e-03,  ...,  5.8105e-02,\n",
            "          -3.1030e-01, -6.1618e-02]],\n",
            "\n",
            "        [[ 1.4886e-02,  6.0982e-02, -2.1389e-02,  ..., -1.1240e-01,\n",
            "          -2.2118e-02,  2.5806e-03],\n",
            "         [ 4.6886e-03,  9.6783e-02, -5.0565e-02,  ..., -9.4908e-02,\n",
            "          -3.7110e-02, -2.7423e-02],\n",
            "         [ 2.2817e-03,  1.1501e-01, -7.0407e-02,  ..., -7.8797e-02,\n",
            "          -5.0430e-02, -4.3771e-02],\n",
            "         ...,\n",
            "         [ 5.6183e-03,  1.3523e-01, -9.6390e-02,  ..., -6.1194e-02,\n",
            "          -6.2959e-02, -5.8787e-02],\n",
            "         [ 5.6183e-03,  1.3523e-01, -9.6390e-02,  ..., -6.1194e-02,\n",
            "          -6.2959e-02, -5.8787e-02],\n",
            "         [-6.4545e-02,  1.5384e-01, -9.7448e-02,  ..., -3.3231e-02,\n",
            "          -6.7008e-04, -9.3234e-02]],\n",
            "\n",
            "        [[-1.4421e-02,  1.3551e-01, -9.2587e-02,  ..., -9.2066e-02,\n",
            "          -7.5387e-03, -7.1386e-02],\n",
            "         [-2.1512e-02,  1.5036e-01, -9.2268e-02,  ..., -7.4441e-02,\n",
            "          -3.8226e-02, -8.0155e-02],\n",
            "         [-1.3682e-02,  1.4962e-01, -9.2144e-02,  ..., -6.2978e-02,\n",
            "          -5.3552e-02, -7.7330e-02],\n",
            "         ...,\n",
            "         [-6.6235e-03,  5.6382e-02,  8.3303e-02,  ..., -7.7713e-02,\n",
            "          -1.8709e-01, -8.5895e-02],\n",
            "         [ 6.6670e-02,  7.4393e-02,  1.2788e-01,  ..., -7.8079e-02,\n",
            "          -2.4772e-01, -2.2458e-02],\n",
            "         [ 5.4164e-02,  5.4278e-02,  1.0364e-01,  ..., -1.6728e-01,\n",
            "          -2.5864e-01, -2.3336e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.2456e-02,  3.2176e-03, -7.9426e-02,  ..., -7.5335e-02,\n",
            "          -5.5442e-02, -7.7366e-02],\n",
            "         [ 8.3695e-03,  7.1776e-02, -8.7819e-02,  ..., -7.3111e-02,\n",
            "          -6.1152e-02, -7.3755e-02],\n",
            "         [ 7.8053e-03,  1.0432e-01, -9.2851e-02,  ..., -6.9595e-02,\n",
            "          -6.5744e-02, -6.7257e-02],\n",
            "         ...,\n",
            "         [ 8.7976e-02, -1.3673e-01, -4.4562e-02,  ..., -1.3898e-01,\n",
            "          -2.3467e-01, -8.0572e-02],\n",
            "         [ 1.0426e-01, -2.1149e-01, -6.7680e-02,  ..., -1.9654e-01,\n",
            "          -1.9113e-02, -7.8914e-02],\n",
            "         [ 4.3803e-02, -2.8244e-01, -5.0370e-02,  ..., -7.0582e-02,\n",
            "           2.2302e-02, -1.6934e-01]],\n",
            "\n",
            "        [[-1.6560e-02,  2.0930e-02, -8.5772e-03,  ..., -8.6197e-05,\n",
            "          -2.6694e-01, -7.3462e-02],\n",
            "         [ 1.0489e-02,  2.5758e-02, -1.5717e-02,  ..., -7.3168e-02,\n",
            "          -2.3130e-01, -5.3644e-02],\n",
            "         [ 2.0825e-02,  4.3708e-02, -1.9847e-02,  ..., -1.0627e-01,\n",
            "          -1.8118e-01, -4.4141e-02],\n",
            "         ...,\n",
            "         [-1.4164e-01,  2.3266e-01, -2.4233e-01,  ..., -6.6427e-03,\n",
            "           2.5184e-02, -1.1734e-01],\n",
            "         [-5.9305e-03,  1.9300e-01, -1.7345e-01,  ...,  1.0605e-03,\n",
            "          -1.5070e-01, -1.0102e-01],\n",
            "         [-1.1914e-01,  1.7023e-01, -2.3647e-01,  ...,  8.3949e-03,\n",
            "          -8.6418e-02, -1.7736e-01]],\n",
            "\n",
            "        [[ 9.2773e-02,  1.7294e-02, -8.7665e-03,  ...,  3.7758e-02,\n",
            "          -2.6740e-01, -1.1863e-01],\n",
            "         [ 5.7562e-02,  1.9134e-02, -2.1511e-02,  ..., -7.5506e-02,\n",
            "          -2.3678e-01, -8.5367e-02],\n",
            "         [ 4.4235e-02,  4.5981e-02, -2.6843e-02,  ..., -1.0589e-01,\n",
            "          -1.8031e-01, -6.4273e-02],\n",
            "         ...,\n",
            "         [-7.4325e-02,  5.0827e-02, -3.4369e-03,  ...,  3.4645e-02,\n",
            "          -1.7958e-01, -1.3408e-01],\n",
            "         [-9.8200e-03,  3.4236e-02, -3.3069e-02,  ...,  2.5269e-02,\n",
            "          -3.3054e-01, -1.2842e-01],\n",
            "         [-7.0506e-03,  6.3583e-02,  4.2121e-03,  ...,  9.8303e-02,\n",
            "          -3.2365e-01, -1.1199e-01]]], device='cuda:0',\n",
            "       grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[-0.0037,  0.0155, -0.0243,  ..., -0.0249, -0.2758, -0.0620],\n",
            "         [ 0.0194,  0.0268, -0.0241,  ..., -0.0879, -0.2226, -0.0531],\n",
            "         [ 0.0265,  0.0502, -0.0272,  ..., -0.1122, -0.1683, -0.0468],\n",
            "         ...,\n",
            "         [-0.0023, -0.4716, -0.1018,  ..., -0.1831,  0.2244,  0.1511],\n",
            "         [ 0.0267, -0.6735, -0.0651,  ..., -0.2022,  0.1533,  0.2084],\n",
            "         [-0.0230, -0.7742, -0.0469,  ..., -0.1528,  0.1602,  0.1910]],\n",
            "\n",
            "        [[ 0.0029,  0.1550, -0.0620,  ..., -0.0321, -0.0650, -0.0529],\n",
            "         [ 0.0118,  0.1458, -0.0799,  ..., -0.0513, -0.0656, -0.0549],\n",
            "         [ 0.0089,  0.1443, -0.0889,  ..., -0.0571, -0.0607, -0.0578],\n",
            "         ...,\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [-0.0750,  0.1502, -0.1042,  ..., -0.0160, -0.0381, -0.0795],\n",
            "         [-0.2070,  0.1442, -0.1561,  ...,  0.0449, -0.0119, -0.1276]],\n",
            "\n",
            "        [[ 0.0659,  0.0501,  0.0495,  ..., -0.1612, -0.1164, -0.0491],\n",
            "         [ 0.0519,  0.0641,  0.0066,  ..., -0.1374, -0.0787, -0.0594],\n",
            "         [ 0.0380,  0.0798, -0.0215,  ..., -0.1137, -0.0744, -0.0597],\n",
            "         ...,\n",
            "         [ 0.0851, -0.1022,  0.0396,  ..., -0.2098, -0.0276,  0.1785],\n",
            "         [ 0.0852, -0.1544,  0.0238,  ..., -0.2004,  0.0234,  0.1199],\n",
            "         [ 0.0989, -0.0291,  0.0269,  ..., -0.0904, -0.0296,  0.0603]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0416, -0.0961, -0.0570,  ..., -0.0666, -0.0578, -0.0888],\n",
            "         [ 0.0320,  0.0087, -0.0673,  ..., -0.0715, -0.0729, -0.0743],\n",
            "         [ 0.0243,  0.0706, -0.0747,  ..., -0.0710, -0.0750, -0.0636],\n",
            "         ...,\n",
            "         [-0.0727,  0.2543, -0.2310,  ...,  0.0075, -0.0269, -0.0953],\n",
            "         [-0.0500,  0.1769, -0.2498,  ..., -0.0050, -0.0188, -0.0787],\n",
            "         [-0.0622,  0.1281, -0.2499,  ..., -0.0044, -0.0731, -0.0953]],\n",
            "\n",
            "        [[ 0.0156,  0.1587, -0.1139,  ..., -0.0063, -0.1316, -0.1131],\n",
            "         [ 0.0267,  0.1615, -0.1000,  ..., -0.0353, -0.1096, -0.0999],\n",
            "         [ 0.0250,  0.1542, -0.0906,  ..., -0.0507, -0.0907, -0.0840],\n",
            "         ...,\n",
            "         [-0.0341,  0.1159, -0.0662,  ..., -0.0866, -0.1170, -0.2452],\n",
            "         [-0.1438,  0.0876, -0.1324,  ..., -0.0201, -0.0458, -0.1209],\n",
            "         [-0.0916,  0.1112, -0.2196,  ...,  0.0124,  0.0063, -0.1019]],\n",
            "\n",
            "        [[ 0.0267,  0.0199, -0.0200,  ..., -0.0424, -0.2760, -0.0572],\n",
            "         [ 0.0258,  0.0303, -0.0267,  ..., -0.1033, -0.2188, -0.0539],\n",
            "         [ 0.0258,  0.0541, -0.0306,  ..., -0.1177, -0.1622, -0.0449],\n",
            "         ...,\n",
            "         [-0.0860,  0.0665, -0.0753,  ...,  0.0458, -0.1769, -0.0760],\n",
            "         [-0.0806,  0.0540, -0.0846,  ...,  0.0106, -0.2806, -0.0913],\n",
            "         [-0.0774,  0.0383, -0.0850,  ..., -0.0202, -0.2320, -0.1146]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[-1.5713e-02, -6.7560e-01, -4.8118e-02,  ..., -1.0066e-01,\n",
            "          -3.2168e-03,  6.0767e-02],\n",
            "         [-1.5257e-03, -4.4543e-01, -5.1599e-02,  ..., -7.7901e-02,\n",
            "          -6.3154e-02, -1.7710e-02],\n",
            "         [ 1.1634e-02, -1.7724e-01, -5.6822e-02,  ..., -6.3785e-02,\n",
            "          -7.8801e-02, -5.7692e-02],\n",
            "         ...,\n",
            "         [-3.5570e-03, -6.1446e-03, -2.7346e-01,  ..., -6.6042e-02,\n",
            "           6.9370e-03, -1.0546e-01],\n",
            "         [ 4.4174e-02, -6.3078e-03, -1.4210e-01,  ..., -7.4780e-02,\n",
            "          -6.8323e-02, -5.3525e-02],\n",
            "         [ 1.2754e-01, -7.3832e-02, -1.0313e-01,  ..., -2.6343e-01,\n",
            "          -3.0561e-02, -3.0400e-02]],\n",
            "\n",
            "        [[-3.3039e-02,  1.0630e-01, -1.0075e-01,  ...,  1.7065e-02,\n",
            "          -8.1032e-02, -6.4101e-02],\n",
            "         [ 7.2010e-03,  1.1762e-01, -8.8003e-02,  ..., -2.7371e-02,\n",
            "          -9.0231e-02, -6.1805e-02],\n",
            "         [ 1.6263e-02,  1.2430e-01, -8.3392e-02,  ..., -5.0596e-02,\n",
            "          -8.0489e-02, -5.8405e-02],\n",
            "         ...,\n",
            "         [-2.2337e-02,  6.4243e-02, -2.4431e-01,  ..., -1.4731e-02,\n",
            "          -2.9200e-01, -8.5465e-02],\n",
            "         [ 2.0753e-02,  8.7388e-02, -1.2108e-01,  ...,  9.0703e-03,\n",
            "          -3.7192e-01, -1.7811e-01],\n",
            "         [-1.3511e-02,  9.3580e-02, -1.9585e-02,  ...,  5.8866e-02,\n",
            "          -3.5878e-01, -1.9907e-01]],\n",
            "\n",
            "        [[ 4.4239e-02,  6.2833e-02, -8.9025e-03,  ..., -1.0367e-01,\n",
            "          -5.9418e-02,  5.4738e-03],\n",
            "         [ 9.3459e-03,  1.0091e-01, -4.1864e-02,  ..., -9.2608e-02,\n",
            "          -6.7469e-02, -2.8332e-02],\n",
            "         [ 1.5180e-03,  1.1460e-01, -6.5025e-02,  ..., -8.1131e-02,\n",
            "          -7.2673e-02, -4.3792e-02],\n",
            "         ...,\n",
            "         [ 3.0512e-02,  2.9688e-02, -1.1168e-01,  ..., -1.2026e-01,\n",
            "           4.2610e-02,  3.0284e-02],\n",
            "         [-5.4391e-03, -6.8506e-02, -1.4164e-01,  ..., -1.3545e-01,\n",
            "           8.4735e-02,  2.7490e-02],\n",
            "         [ 6.5386e-03, -1.8440e-01, -9.5636e-02,  ..., -1.9400e-01,\n",
            "           7.6664e-02,  4.9686e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.5220e-02,  1.6313e-01, -1.3835e-01,  ..., -4.1365e-03,\n",
            "          -1.2850e-01, -8.4793e-02],\n",
            "         [ 3.1874e-02,  1.6308e-01, -1.1408e-01,  ..., -2.1506e-02,\n",
            "          -1.1749e-01, -8.1383e-02],\n",
            "         [ 3.1707e-02,  1.5658e-01, -1.0091e-01,  ..., -3.8413e-02,\n",
            "          -9.9152e-02, -7.4997e-02],\n",
            "         ...,\n",
            "         [ 5.6183e-03,  1.3523e-01, -9.6390e-02,  ..., -6.1194e-02,\n",
            "          -6.2959e-02, -5.8787e-02],\n",
            "         [ 5.6183e-03,  1.3523e-01, -9.6390e-02,  ..., -6.1194e-02,\n",
            "          -6.2959e-02, -5.8787e-02],\n",
            "         [-2.5440e-02,  1.8240e-01, -7.7506e-02,  ..., -3.9560e-02,\n",
            "          -4.2204e-02, -9.1743e-02]],\n",
            "\n",
            "        [[-7.0694e-04,  1.5767e-01, -1.2641e-01,  ...,  1.1991e-02,\n",
            "          -6.5726e-02, -8.2980e-02],\n",
            "         [ 2.0608e-02,  1.5753e-01, -1.1261e-01,  ..., -1.5607e-02,\n",
            "          -7.1799e-02, -8.0489e-02],\n",
            "         [ 2.1269e-02,  1.5457e-01, -1.0481e-01,  ..., -3.6587e-02,\n",
            "          -6.6705e-02, -7.6529e-02],\n",
            "         ...,\n",
            "         [ 1.6176e-01, -4.6059e-01,  1.1070e-01,  ..., -4.6049e-01,\n",
            "           9.3636e-02,  2.8541e-01],\n",
            "         [ 6.7515e-02, -7.4763e-01,  1.3853e-01,  ..., -4.9576e-01,\n",
            "           9.1776e-02,  4.1993e-01],\n",
            "         [ 1.8445e-01, -8.7809e-01,  7.4017e-02,  ..., -6.2731e-01,\n",
            "           6.2716e-02,  3.8345e-01]],\n",
            "\n",
            "        [[-1.5901e-03,  7.2900e-02, -3.1050e-02,  ..., -2.4935e-02,\n",
            "          -2.0007e-01, -6.0942e-02],\n",
            "         [ 2.4321e-02,  8.3617e-02, -4.2320e-02,  ..., -5.3606e-02,\n",
            "          -1.6364e-01, -4.4579e-02],\n",
            "         [ 2.5804e-02,  1.0346e-01, -5.0923e-02,  ..., -6.5837e-02,\n",
            "          -1.2423e-01, -4.0162e-02],\n",
            "         ...,\n",
            "         [-1.7859e-02, -3.4235e-01,  1.3419e-02,  ..., -2.4814e-01,\n",
            "           1.1092e-02,  1.1626e-01],\n",
            "         [ 2.8945e-02, -3.5564e-01,  2.3532e-02,  ..., -3.2704e-01,\n",
            "           2.8417e-02,  1.1084e-01],\n",
            "         [ 3.2641e-02, -3.4718e-01,  2.6794e-02,  ..., -3.1766e-01,\n",
            "           7.8634e-02,  1.1109e-01]]], device='cuda:0',\n",
            "       grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 6.2865e-02,  7.9223e-02, -5.7094e-02,  ..., -1.3497e-01,\n",
            "          -7.4296e-02, -4.8766e-02],\n",
            "         [ 3.0718e-02,  1.0946e-01, -6.6768e-02,  ..., -1.0523e-01,\n",
            "          -6.8827e-02, -6.3391e-02],\n",
            "         [ 1.7309e-02,  1.2887e-01, -7.4193e-02,  ..., -7.7370e-02,\n",
            "          -7.0152e-02, -7.0629e-02],\n",
            "         ...,\n",
            "         [ 7.9189e-02,  6.1367e-02,  2.5244e-02,  ..., -1.9236e-01,\n",
            "          -1.7265e-01,  4.9326e-02],\n",
            "         [ 3.3424e-02,  5.2246e-02,  2.7439e-02,  ..., -3.2297e-02,\n",
            "          -1.6448e-01, -1.7811e-02],\n",
            "         [-1.8735e-02,  7.0400e-02,  4.9590e-02,  ...,  9.9488e-02,\n",
            "          -2.6030e-01, -6.5540e-02]],\n",
            "\n",
            "        [[ 2.4202e-02,  5.1055e-02, -4.9162e-02,  ..., -1.9212e-02,\n",
            "          -2.6820e-01, -1.0082e-01],\n",
            "         [ 2.3199e-02,  6.4805e-02, -4.7347e-02,  ..., -6.8618e-02,\n",
            "          -1.9143e-01, -8.0188e-02],\n",
            "         [ 2.3672e-02,  8.1597e-02, -4.7207e-02,  ..., -8.8241e-02,\n",
            "          -1.3948e-01, -6.2805e-02],\n",
            "         ...,\n",
            "         [-5.5971e-02,  1.1469e-02, -6.3847e-02,  ...,  6.6627e-02,\n",
            "          -5.0826e-01, -7.7841e-02],\n",
            "         [-8.3029e-02,  1.0574e-02, -2.1543e-02,  ...,  9.4990e-02,\n",
            "          -4.2717e-01, -1.1283e-01],\n",
            "         [-3.9304e-02,  1.5810e-02, -4.8671e-03,  ...,  1.6705e-01,\n",
            "          -4.2017e-01, -1.4726e-01]],\n",
            "\n",
            "        [[-1.8699e-02, -9.3609e-02, -7.2977e-02,  ..., -1.3186e-01,\n",
            "          -2.7013e-02,  3.2955e-02],\n",
            "         [-1.3467e-02,  2.4326e-02, -6.9352e-02,  ..., -1.0120e-01,\n",
            "          -5.7004e-02, -1.6812e-02],\n",
            "         [-5.1997e-03,  8.9339e-02, -7.2461e-02,  ..., -7.8299e-02,\n",
            "          -6.5853e-02, -4.7723e-02],\n",
            "         ...,\n",
            "         [-1.7128e-01,  1.1037e-01, -1.2475e-01,  ...,  4.3470e-04,\n",
            "          -6.6791e-03, -5.7958e-02],\n",
            "         [-2.4631e-02,  1.3182e-01, -1.2743e-01,  ..., -1.8989e-02,\n",
            "          -5.4473e-02, -6.6519e-02],\n",
            "         [ 4.6444e-02,  1.1310e-01, -2.2215e-01,  ..., -1.4278e-01,\n",
            "          -8.9150e-02, -7.0921e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 3.1813e-02,  1.4151e-01, -7.1894e-02,  ..., -5.2376e-02,\n",
            "          -1.0158e-01, -5.6870e-02],\n",
            "         [ 2.2140e-02,  1.3715e-01, -8.5132e-02,  ..., -6.9604e-02,\n",
            "          -7.7568e-02, -6.2318e-02],\n",
            "         [ 9.8229e-03,  1.3958e-01, -9.0199e-02,  ..., -6.7151e-02,\n",
            "          -6.3216e-02, -6.3445e-02],\n",
            "         ...,\n",
            "         [-3.5850e-02,  7.6889e-02, -5.2878e-02,  ...,  4.9139e-02,\n",
            "          -3.2147e-01, -6.9502e-02],\n",
            "         [-4.1403e-02,  5.6753e-02, -5.3595e-02,  ...,  7.9954e-02,\n",
            "          -3.7789e-01, -1.0592e-01],\n",
            "         [-3.1720e-02,  6.0152e-02, -2.7963e-02,  ...,  3.9903e-02,\n",
            "          -3.5050e-01, -5.7761e-02]],\n",
            "\n",
            "        [[-3.5539e-02, -8.4353e-01,  9.9749e-02,  ..., -3.7131e-01,\n",
            "           8.2035e-03,  3.7470e-01],\n",
            "         [-3.2634e-02, -6.2555e-01,  1.1069e-01,  ..., -3.2286e-01,\n",
            "          -4.4677e-02,  3.3855e-01],\n",
            "         [-2.2628e-02, -3.6525e-01,  8.2596e-02,  ..., -2.3386e-01,\n",
            "          -5.5774e-02,  2.8889e-01],\n",
            "         ...,\n",
            "         [ 5.2005e-02, -1.0817e-04, -2.0906e-01,  ..., -5.3598e-02,\n",
            "           1.5441e-03, -1.1703e-01],\n",
            "         [-1.6284e-03, -3.3514e-02, -1.6341e-01,  ..., -5.6263e-02,\n",
            "           4.6247e-02, -9.2140e-02],\n",
            "         [ 4.0950e-02,  3.6844e-02, -1.1264e-01,  ..., -1.1412e-01,\n",
            "          -4.9359e-02, -8.8646e-02]],\n",
            "\n",
            "        [[-2.3254e-02, -1.7072e-01,  2.9603e-02,  ..., -1.9013e-01,\n",
            "          -1.8622e-02,  9.3790e-02],\n",
            "         [-2.5856e-02, -1.8582e-02,  5.1553e-03,  ..., -1.4638e-01,\n",
            "          -4.7003e-02,  2.8561e-02],\n",
            "         [-1.4401e-02,  6.3370e-02, -2.6543e-02,  ..., -1.0051e-01,\n",
            "          -5.8889e-02, -1.9338e-02],\n",
            "         ...,\n",
            "         [-1.2784e-02,  3.7065e-02, -3.8110e-02,  ...,  3.4048e-02,\n",
            "          -1.4778e-01, -8.8181e-02],\n",
            "         [-8.8699e-03,  7.9340e-02, -1.1024e-02,  ...,  5.0639e-02,\n",
            "          -1.4749e-01, -1.1093e-01],\n",
            "         [-2.4711e-03,  6.4601e-02, -4.7506e-02,  ...,  3.8822e-02,\n",
            "          -2.6107e-01, -7.7412e-02]]], device='cuda:0',\n",
            "       grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 5.2729e-03,  2.9968e-02, -1.9867e-02,  ..., -6.9830e-03,\n",
            "          -2.3620e-01, -5.8363e-02],\n",
            "         [ 2.4794e-02,  2.7046e-02, -2.9583e-02,  ..., -8.2487e-02,\n",
            "          -2.1529e-01, -5.0821e-02],\n",
            "         [ 2.7436e-02,  4.4421e-02, -3.5471e-02,  ..., -1.0976e-01,\n",
            "          -1.7173e-01, -4.5383e-02],\n",
            "         ...,\n",
            "         [-1.7888e-01,  1.7567e-01, -4.8522e-02,  ...,  1.3589e-01,\n",
            "          -1.7126e-01, -2.1987e-01],\n",
            "         [-1.0420e-01,  1.0250e-01, -2.1180e-02,  ...,  1.6047e-01,\n",
            "          -2.4724e-01, -1.7631e-01],\n",
            "         [-6.7749e-02,  4.4211e-02,  6.0212e-04,  ...,  1.2543e-01,\n",
            "          -3.1818e-01, -7.5560e-02]],\n",
            "\n",
            "        [[ 1.6117e-03,  1.9014e-02, -3.8625e-03,  ...,  6.7644e-02,\n",
            "          -3.2899e-01, -7.9761e-02],\n",
            "         [ 1.5815e-02,  1.9297e-02, -1.3451e-02,  ..., -4.7738e-02,\n",
            "          -3.3731e-01, -6.2512e-02],\n",
            "         [ 2.0834e-02,  3.3054e-02, -1.7760e-02,  ..., -9.9944e-02,\n",
            "          -2.8954e-01, -4.8444e-02],\n",
            "         ...,\n",
            "         [-5.6658e-02,  1.4975e-01, -3.0560e-02,  ...,  1.1015e-01,\n",
            "          -1.4190e-01, -9.8285e-02],\n",
            "         [-1.4841e-02,  1.1831e-01, -6.5593e-02,  ...,  9.3512e-02,\n",
            "          -1.4197e-01, -6.6146e-02],\n",
            "         [-2.3363e-02,  1.2903e-01, -4.1997e-02,  ...,  6.8794e-02,\n",
            "          -1.2131e-01, -9.1536e-02]],\n",
            "\n",
            "        [[ 3.9033e-02,  1.9874e-01, -1.1641e-01,  ..., -9.9498e-02,\n",
            "          -1.0218e-01, -7.8308e-02],\n",
            "         [ 1.9455e-02,  1.6267e-01, -1.1307e-01,  ..., -9.0555e-02,\n",
            "          -7.3928e-02, -7.4926e-02],\n",
            "         [ 8.0955e-03,  1.5531e-01, -1.0514e-01,  ..., -7.2241e-02,\n",
            "          -6.3593e-02, -7.0936e-02],\n",
            "         ...,\n",
            "         [-1.8460e-02,  8.4543e-02,  1.1889e-02,  ...,  1.2838e-01,\n",
            "          -4.0474e-01, -1.7184e-01],\n",
            "         [-1.0701e-02,  8.4945e-02, -2.5932e-04,  ...,  1.5602e-01,\n",
            "          -4.3629e-01, -1.4412e-01],\n",
            "         [ 2.3659e-02,  4.4924e-02,  6.8010e-04,  ...,  1.6028e-01,\n",
            "          -3.5734e-01, -9.5697e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.8185e-02,  5.3199e-02, -4.4993e-02,  ..., -4.3507e-02,\n",
            "          -2.6909e-01, -5.1067e-02],\n",
            "         [ 3.2459e-02,  6.6454e-02, -4.8803e-02,  ..., -8.5333e-02,\n",
            "          -2.0180e-01, -4.7006e-02],\n",
            "         [ 3.0214e-02,  8.8433e-02, -5.3329e-02,  ..., -9.2001e-02,\n",
            "          -1.4235e-01, -4.5820e-02],\n",
            "         ...,\n",
            "         [ 7.7095e-02,  6.5314e-02, -6.1535e-03,  ..., -4.1784e-01,\n",
            "           6.0815e-02,  1.8279e-01],\n",
            "         [ 2.1488e-02, -2.7922e-02,  5.0426e-02,  ..., -5.3594e-01,\n",
            "           6.2838e-02,  2.4222e-01],\n",
            "         [-2.7136e-02, -3.0246e-01,  1.2820e-01,  ..., -5.5598e-01,\n",
            "           4.6479e-02,  2.9873e-01]],\n",
            "\n",
            "        [[ 1.7481e-02,  1.1277e-01, -1.0358e-01,  ..., -8.7858e-02,\n",
            "          -6.5925e-02, -8.5717e-02],\n",
            "         [ 3.5124e-03,  1.3350e-01, -1.0547e-01,  ..., -6.8460e-02,\n",
            "          -5.9146e-02, -7.9728e-02],\n",
            "         [ 2.0037e-03,  1.4065e-01, -1.0370e-01,  ..., -5.7897e-02,\n",
            "          -5.7570e-02, -7.4094e-02],\n",
            "         ...,\n",
            "         [-1.2213e-01,  1.3614e-01, -1.5929e-01,  ..., -1.9562e-02,\n",
            "          -5.4865e-02, -1.5799e-01],\n",
            "         [-3.6204e-02,  8.2865e-02, -2.5873e-01,  ..., -2.4547e-02,\n",
            "          -1.0318e-01, -6.3008e-02],\n",
            "         [ 2.3294e-02,  5.5064e-02, -3.0864e-01,  ..., -8.6134e-02,\n",
            "          -1.6562e-02, -7.2685e-02]],\n",
            "\n",
            "        [[ 5.1546e-02,  6.2273e-02, -2.5732e-02,  ..., -4.3992e-02,\n",
            "          -2.3672e-01, -2.8309e-02],\n",
            "         [ 4.6920e-02,  8.9180e-02, -4.4006e-02,  ..., -8.5031e-02,\n",
            "          -1.7085e-01, -4.1258e-02],\n",
            "         [ 3.1253e-02,  1.0988e-01, -5.3394e-02,  ..., -8.6912e-02,\n",
            "          -1.1939e-01, -4.2920e-02],\n",
            "         ...,\n",
            "         [ 7.5996e-02,  1.0594e-01, -9.0511e-02,  ..., -1.1149e-02,\n",
            "          -1.4659e-02, -3.4077e-02],\n",
            "         [ 9.8563e-02,  1.1533e-01, -1.3816e-01,  ..., -1.0517e-01,\n",
            "           3.0776e-02,  2.7656e-03],\n",
            "         [ 6.7656e-02,  1.2175e-01, -1.1983e-01,  ..., -8.9632e-02,\n",
            "           4.4761e-02,  3.8609e-02]]], device='cuda:0',\n",
            "       grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 1.2958e-02,  2.9744e-02, -1.3880e-02,  ...,  6.2277e-03,\n",
            "          -2.5903e-01, -5.0826e-02],\n",
            "         [ 2.1773e-02,  4.4669e-02, -2.0912e-02,  ..., -6.6808e-02,\n",
            "          -2.0627e-01, -5.2051e-02],\n",
            "         [ 2.4172e-02,  6.4343e-02, -2.6269e-02,  ..., -9.4645e-02,\n",
            "          -1.5497e-01, -4.4380e-02],\n",
            "         ...,\n",
            "         [-8.3068e-02,  1.3044e-01, -1.9777e-01,  ...,  7.3180e-02,\n",
            "          -1.6894e-01, -1.1409e-01],\n",
            "         [-3.6477e-02,  9.9366e-02, -1.6536e-01,  ...,  2.7239e-02,\n",
            "          -1.9596e-01, -5.5166e-02],\n",
            "         [ 9.1146e-03,  1.0861e-01, -1.1858e-01,  ..., -3.1775e-02,\n",
            "          -1.3519e-01, -2.6962e-02]],\n",
            "\n",
            "        [[ 2.3036e-02,  9.4763e-02, -6.1601e-02,  ..., -5.3903e-03,\n",
            "          -1.2933e-01, -5.9297e-02],\n",
            "         [ 2.9310e-02,  1.0552e-01, -6.2823e-02,  ..., -5.2887e-02,\n",
            "          -1.1260e-01, -5.0635e-02],\n",
            "         [ 2.4215e-02,  1.1656e-01, -6.8432e-02,  ..., -6.8804e-02,\n",
            "          -9.2101e-02, -4.8272e-02],\n",
            "         ...,\n",
            "         [-1.0085e-01, -6.0845e-02, -7.1293e-03,  ...,  1.7418e-02,\n",
            "           4.9770e-02, -1.5033e-01],\n",
            "         [-5.8441e-02,  3.5963e-02, -3.6969e-02,  ...,  7.5944e-04,\n",
            "          -1.8918e-02, -1.6754e-01],\n",
            "         [-4.6036e-02,  5.4143e-02, -5.7850e-02,  ...,  1.6506e-03,\n",
            "          -1.7335e-03, -1.7215e-01]],\n",
            "\n",
            "        [[ 1.9593e-02,  3.6223e-02, -3.1600e-02,  ...,  6.2267e-03,\n",
            "          -2.8653e-01, -8.2193e-02],\n",
            "         [ 2.5263e-02,  4.5094e-02, -3.1406e-02,  ..., -6.7196e-02,\n",
            "          -2.3211e-01, -6.0665e-02],\n",
            "         [ 2.8730e-02,  5.9264e-02, -3.2737e-02,  ..., -1.0091e-01,\n",
            "          -1.7860e-01, -4.7903e-02],\n",
            "         ...,\n",
            "         [ 1.0290e-01, -3.0180e-01,  4.7122e-02,  ..., -4.9184e-01,\n",
            "          -1.4604e-01,  2.9813e-01],\n",
            "         [ 1.1956e-01, -4.3468e-01,  6.3387e-02,  ..., -4.3456e-01,\n",
            "          -9.7079e-02,  4.0969e-01],\n",
            "         [ 2.0739e-01, -4.8679e-01,  1.0591e-01,  ..., -4.9589e-01,\n",
            "          -1.4081e-01,  4.2679e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-5.7121e-02, -1.5028e-01,  1.6297e-01,  ..., -4.3793e-01,\n",
            "          -1.0893e-02,  3.2274e-01],\n",
            "         [-6.5083e-02, -3.2513e-02,  1.3937e-01,  ..., -3.5596e-01,\n",
            "          -2.0550e-02,  2.8338e-01],\n",
            "         [-5.0312e-02,  2.9522e-02,  8.4248e-02,  ..., -2.5459e-01,\n",
            "          -3.6837e-02,  2.0339e-01],\n",
            "         ...,\n",
            "         [ 5.6183e-03,  1.3523e-01, -9.6390e-02,  ..., -6.1194e-02,\n",
            "          -6.2959e-02, -5.8787e-02],\n",
            "         [ 1.0899e-01,  5.8875e-02, -1.4547e-01,  ..., -1.7530e-01,\n",
            "          -7.5781e-02,  1.0123e-01],\n",
            "         [ 1.2260e-01, -7.6482e-02, -6.4589e-02,  ..., -3.6046e-01,\n",
            "           9.2256e-02,  2.2194e-01]],\n",
            "\n",
            "        [[ 3.1874e-02,  1.4230e-01, -1.4392e-01,  ..., -5.3038e-02,\n",
            "          -6.9638e-02, -7.9503e-02],\n",
            "         [ 2.6849e-02,  1.4018e-01, -1.2869e-01,  ..., -6.0725e-02,\n",
            "          -6.3205e-02, -7.3406e-02],\n",
            "         [ 1.7480e-02,  1.4754e-01, -1.1484e-01,  ..., -5.7742e-02,\n",
            "          -5.8590e-02, -7.0395e-02],\n",
            "         ...,\n",
            "         [ 1.8540e-01, -6.9083e-01,  1.3765e-01,  ..., -6.7041e-01,\n",
            "           4.0836e-03,  4.1477e-01],\n",
            "         [ 2.1768e-01, -7.9655e-01,  9.3169e-02,  ..., -7.7027e-01,\n",
            "           4.6960e-02,  4.7162e-01],\n",
            "         [ 2.3061e-01, -8.5675e-01,  8.4960e-02,  ..., -7.2287e-01,\n",
            "           5.9156e-02,  4.0574e-01]],\n",
            "\n",
            "        [[ 2.7522e-02,  1.6625e-01, -8.8955e-02,  ..., -8.4818e-02,\n",
            "          -3.0116e-02,  1.0351e-03],\n",
            "         [ 7.0907e-04,  1.5860e-01, -9.5901e-02,  ..., -7.3277e-02,\n",
            "          -4.3842e-02, -3.4716e-02],\n",
            "         [-3.9495e-03,  1.5153e-01, -9.8838e-02,  ..., -6.2367e-02,\n",
            "          -5.2763e-02, -5.2401e-02],\n",
            "         ...,\n",
            "         [-4.7187e-02,  3.4624e-02, -4.5825e-02,  ...,  1.4507e-01,\n",
            "          -2.3971e-01, -1.1389e-01],\n",
            "         [-3.7752e-02,  3.4303e-02, -4.1653e-02,  ...,  1.0731e-01,\n",
            "          -2.7521e-01, -7.9283e-02],\n",
            "         [-1.7737e-02,  3.1172e-02, -3.3497e-02,  ...,  1.1296e-01,\n",
            "          -2.6301e-01, -7.8435e-02]]], device='cuda:0',\n",
            "       grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 4.3844e-02,  1.1007e-01, -8.1801e-02,  ..., -7.9177e-02,\n",
            "          -1.1028e-01, -2.0136e-02],\n",
            "         [ 3.4091e-02,  1.2290e-01, -8.2423e-02,  ..., -8.6242e-02,\n",
            "          -8.0679e-02, -3.3642e-02],\n",
            "         [ 2.1366e-02,  1.3054e-01, -8.6423e-02,  ..., -7.7071e-02,\n",
            "          -6.7564e-02, -4.4545e-02],\n",
            "         ...,\n",
            "         [-8.3680e-02,  1.3388e-01, -1.2139e-01,  ...,  6.4232e-02,\n",
            "           1.9631e-02, -2.1843e-01],\n",
            "         [-3.1103e-02,  6.9905e-02, -1.2394e-01,  ..., -5.5884e-03,\n",
            "          -3.2597e-02, -1.2488e-01],\n",
            "         [-1.7811e-02,  6.0625e-02, -1.3755e-01,  ..., -1.4703e-02,\n",
            "          -2.6414e-02, -8.8140e-02]],\n",
            "\n",
            "        [[-2.1429e-02,  1.0871e-01, -5.8348e-02,  ...,  8.1017e-03,\n",
            "          -5.7973e-02, -1.1570e-01],\n",
            "         [ 2.3816e-02,  1.2218e-01, -6.2341e-02,  ..., -1.4188e-02,\n",
            "          -8.8914e-02, -8.8539e-02],\n",
            "         [ 3.0901e-02,  1.3428e-01, -7.1761e-02,  ..., -3.8131e-02,\n",
            "          -8.3737e-02, -8.1511e-02],\n",
            "         ...,\n",
            "         [ 4.7481e-02, -1.7813e-01,  4.5111e-02,  ..., -4.1166e-01,\n",
            "          -7.6422e-02, -1.5858e-01],\n",
            "         [ 1.2898e-01, -3.3823e-01,  5.5022e-02,  ..., -5.9674e-01,\n",
            "          -8.1890e-02,  3.6640e-02],\n",
            "         [ 1.2344e-01, -5.5625e-01,  1.3296e-02,  ..., -5.9046e-01,\n",
            "           5.3912e-02,  1.3459e-01]],\n",
            "\n",
            "        [[ 1.3909e-01, -2.8911e-01,  1.6271e-01,  ..., -4.2807e-01,\n",
            "          -2.0111e-02,  3.6864e-01],\n",
            "         [ 7.0211e-02, -1.1170e-01,  1.2831e-01,  ..., -3.4507e-01,\n",
            "          -2.5154e-02,  3.3628e-01],\n",
            "         [ 3.2177e-02, -1.4981e-02,  7.6822e-02,  ..., -2.4891e-01,\n",
            "          -4.5546e-02,  2.4196e-01],\n",
            "         ...,\n",
            "         [ 1.2280e-01, -3.8644e-03, -1.0920e-02,  ..., -4.2677e-01,\n",
            "          -1.0561e-01,  1.1269e-01],\n",
            "         [ 9.9269e-02,  1.5139e-01,  7.0000e-02,  ..., -5.1495e-01,\n",
            "          -2.1557e-01,  1.8675e-01],\n",
            "         [ 1.4458e-01, -3.3640e-02,  6.4396e-02,  ..., -5.8503e-01,\n",
            "          -6.3551e-02,  2.5810e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 4.3425e-02,  8.9089e-02,  2.5244e-02,  ..., -2.8966e-01,\n",
            "          -3.2890e-03,  2.6541e-01],\n",
            "         [-6.5683e-03,  9.4544e-02,  2.4655e-02,  ..., -2.5402e-01,\n",
            "          -1.3765e-02,  1.7622e-01],\n",
            "         [-2.5235e-02,  1.1054e-01,  4.3602e-04,  ..., -1.7401e-01,\n",
            "          -3.3580e-02,  9.6203e-02],\n",
            "         ...,\n",
            "         [ 8.8524e-02,  1.1456e-01,  2.0412e-02,  ..., -9.4081e-03,\n",
            "          -1.8383e-01,  5.2707e-04],\n",
            "         [ 9.2287e-03,  1.1226e-01,  1.8813e-02,  ..., -1.9642e-02,\n",
            "          -1.8516e-01, -2.9156e-02],\n",
            "         [-1.0089e-01,  5.9751e-02,  6.6196e-02,  ...,  9.9558e-02,\n",
            "          -2.5903e-01, -1.5739e-01]],\n",
            "\n",
            "        [[ 7.3209e-02, -7.0923e-01,  1.2145e-01,  ..., -5.0853e-01,\n",
            "          -2.4933e-03,  3.7490e-01],\n",
            "         [-1.1141e-02, -4.1509e-01,  1.3675e-01,  ..., -4.5845e-01,\n",
            "          -3.1686e-02,  3.9996e-01],\n",
            "         [-4.0164e-02, -1.5246e-01,  1.1692e-01,  ..., -3.7732e-01,\n",
            "          -4.1431e-02,  3.8584e-01],\n",
            "         ...,\n",
            "         [-2.3594e-02,  3.1830e-03, -1.1542e-01,  ..., -2.0729e-01,\n",
            "           2.3607e-02,  2.0775e-03],\n",
            "         [-1.5467e-02,  4.5377e-02, -1.7473e-02,  ..., -2.2590e-01,\n",
            "          -2.4084e-02,  4.0829e-02],\n",
            "         [-3.2010e-02,  7.4105e-02,  1.7518e-02,  ..., -2.3469e-01,\n",
            "          -5.5181e-02,  2.0664e-02]],\n",
            "\n",
            "        [[ 1.2124e-02,  2.9452e-02, -4.3723e-02,  ...,  1.0024e-02,\n",
            "          -2.3446e-01, -5.8252e-02],\n",
            "         [ 2.3660e-02,  4.2705e-02, -4.0035e-02,  ..., -6.5336e-02,\n",
            "          -2.0223e-01, -4.9594e-02],\n",
            "         [ 2.6261e-02,  6.3022e-02, -4.1832e-02,  ..., -9.4135e-02,\n",
            "          -1.5799e-01, -4.2655e-02],\n",
            "         ...,\n",
            "         [ 5.1937e-02, -6.5758e-01, -8.2701e-02,  ..., -9.3530e-02,\n",
            "           7.0941e-02, -2.1526e-02],\n",
            "         [-1.6517e-03, -6.1487e-01, -9.7303e-02,  ..., -9.7124e-02,\n",
            "           6.1837e-02, -3.6108e-02],\n",
            "         [ 8.5324e-02, -7.1039e-01, -7.0615e-02,  ..., -1.7247e-01,\n",
            "           7.7424e-02, -5.3753e-02]]], device='cuda:0',\n",
            "       grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[-2.0232e-02,  1.3533e-01, -8.7818e-02,  ..., -1.9259e-02,\n",
            "          -7.1436e-02, -8.1228e-02],\n",
            "         [-1.9449e-03,  1.3220e-01, -9.0595e-02,  ..., -3.9713e-02,\n",
            "          -7.7907e-02, -7.3731e-02],\n",
            "         [ 6.1184e-03,  1.3656e-01, -9.1195e-02,  ..., -5.2809e-02,\n",
            "          -7.4503e-02, -7.0893e-02],\n",
            "         ...,\n",
            "         [-3.2564e-02,  1.6150e-01, -5.4813e-04,  ...,  6.4979e-02,\n",
            "          -2.5740e-01, -8.8522e-02],\n",
            "         [-1.1491e-01,  1.7004e-01,  5.0727e-02,  ...,  9.9832e-02,\n",
            "          -2.9203e-01, -9.0339e-02],\n",
            "         [-1.5643e-01,  7.3531e-02,  4.8968e-02,  ...,  1.0800e-01,\n",
            "          -3.1315e-01, -1.1422e-01]],\n",
            "\n",
            "        [[ 2.0954e-02, -3.7229e-01,  2.9116e-02,  ..., -3.5080e-01,\n",
            "          -5.1015e-02,  9.9117e-02],\n",
            "         [-8.2874e-03, -1.3956e-01,  1.3409e-02,  ..., -2.7180e-01,\n",
            "          -5.7294e-02,  3.7678e-02],\n",
            "         [-1.2016e-02,  5.0968e-03, -1.2929e-02,  ..., -1.7758e-01,\n",
            "          -6.2212e-02, -1.3338e-02],\n",
            "         ...,\n",
            "         [-1.3872e-02,  9.9198e-02, -1.1227e-01,  ..., -3.4228e-02,\n",
            "          -5.7078e-03, -6.5517e-02],\n",
            "         [ 7.5493e-03,  5.1468e-02, -2.2327e-01,  ..., -1.0754e-02,\n",
            "          -1.1881e-01, -1.3170e-01],\n",
            "         [-1.0917e-02,  7.9850e-02, -1.5712e-01,  ...,  1.2424e-03,\n",
            "          -1.5737e-01, -1.2491e-01]],\n",
            "\n",
            "        [[ 7.6792e-02,  3.4658e-02,  7.3058e-02,  ..., -4.2635e-01,\n",
            "          -3.6835e-02,  2.6700e-01],\n",
            "         [ 2.3665e-02,  5.9074e-02,  5.8348e-02,  ..., -3.1810e-01,\n",
            "          -3.7281e-02,  1.8999e-01],\n",
            "         [ 7.4050e-03,  7.4140e-02,  2.2224e-02,  ..., -2.0883e-01,\n",
            "          -5.3105e-02,  9.6120e-02],\n",
            "         ...,\n",
            "         [-2.7688e-02, -1.3493e-01, -3.9828e-01,  ..., -5.4701e-02,\n",
            "           1.9062e-01,  1.8678e-02],\n",
            "         [-6.8761e-02, -2.3761e-01, -3.7943e-01,  ..., -2.6186e-02,\n",
            "           1.5200e-01, -6.6932e-02],\n",
            "         [-1.0374e-01, -1.9761e-01, -1.7720e-01,  ..., -2.9239e-03,\n",
            "          -2.5186e-02, -1.9717e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-8.5442e-03,  1.9927e-02, -1.4274e-02,  ..., -2.1822e-02,\n",
            "          -2.1214e-01, -9.9997e-02],\n",
            "         [ 1.1283e-02,  3.2923e-02, -2.6389e-02,  ..., -8.1956e-02,\n",
            "          -1.7992e-01, -8.0093e-02],\n",
            "         [ 2.0274e-02,  5.5268e-02, -3.2768e-02,  ..., -1.0375e-01,\n",
            "          -1.4350e-01, -6.1732e-02],\n",
            "         ...,\n",
            "         [-7.5780e-02,  2.0074e-01, -8.1942e-02,  ...,  3.6579e-02,\n",
            "          -1.0331e-01, -1.1006e-01],\n",
            "         [-7.0242e-02,  1.7335e-01, -2.4813e-02,  ...,  4.9150e-02,\n",
            "          -1.2078e-01, -1.3100e-01],\n",
            "         [-1.8774e-02,  1.5625e-01, -2.8069e-02,  ...,  1.5036e-02,\n",
            "          -2.1990e-01, -1.0793e-01]],\n",
            "\n",
            "        [[-6.2631e-04,  7.7883e-02, -1.3708e-02,  ..., -1.8350e-01,\n",
            "          -4.4849e-02, -1.3172e-02],\n",
            "         [-5.3025e-04,  1.0689e-01, -4.3285e-02,  ..., -1.2876e-01,\n",
            "          -4.5536e-02, -4.0964e-02],\n",
            "         [ 2.0296e-03,  1.2124e-01, -6.3579e-02,  ..., -9.2433e-02,\n",
            "          -5.5148e-02, -5.3767e-02],\n",
            "         ...,\n",
            "         [-1.5493e-02,  7.7468e-02,  4.7213e-02,  ..., -7.0856e-02,\n",
            "          -2.0037e-01, -3.0530e-02],\n",
            "         [-5.9200e-03,  4.0299e-02,  1.3942e-03,  ..., -2.0103e-01,\n",
            "          -1.4542e-01, -1.8262e-02],\n",
            "         [-3.6870e-02,  1.5303e-01,  5.6315e-02,  ..., -9.3177e-02,\n",
            "          -9.8876e-02, -8.8785e-02]],\n",
            "\n",
            "        [[ 2.5015e-03, -6.3191e-01, -5.5521e-02,  ..., -8.1119e-02,\n",
            "          -1.8492e-02, -7.4736e-02],\n",
            "         [ 1.1244e-02, -3.5467e-01, -5.6980e-02,  ..., -6.9194e-02,\n",
            "          -7.7454e-02, -8.0590e-02],\n",
            "         [ 1.8450e-02, -1.0505e-01, -6.0779e-02,  ..., -6.2382e-02,\n",
            "          -8.7299e-02, -8.3257e-02],\n",
            "         ...,\n",
            "         [-7.8617e-02,  1.0408e-01,  4.6845e-03,  ...,  9.9459e-02,\n",
            "          -1.5655e-01, -1.6008e-01],\n",
            "         [-9.8696e-02,  7.8544e-02, -2.3332e-02,  ...,  9.5656e-02,\n",
            "          -1.8325e-01, -1.2025e-01],\n",
            "         [-9.3540e-02,  3.6465e-02, -8.0196e-02,  ...,  1.6483e-02,\n",
            "          -1.5097e-01, -9.7761e-02]]], device='cuda:0',\n",
            "       grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[-0.0164,  0.0315,  0.0019,  ...,  0.0084, -0.2936, -0.0399],\n",
            "         [ 0.0032,  0.0206, -0.0131,  ..., -0.0757, -0.2849, -0.0387],\n",
            "         [ 0.0137,  0.0295, -0.0179,  ..., -0.1091, -0.2390, -0.0310],\n",
            "         ...,\n",
            "         [-0.0037,  0.0800, -0.0235,  ..., -0.0128, -0.0332, -0.0748],\n",
            "         [-0.0016,  0.0575, -0.0352,  ..., -0.0051,  0.0104, -0.1364],\n",
            "         [-0.0250,  0.0921,  0.0350,  ...,  0.0604, -0.0723, -0.1694]],\n",
            "\n",
            "        [[ 0.0168,  0.1218, -0.0930,  ..., -0.0130, -0.1284, -0.0927],\n",
            "         [ 0.0256,  0.1332, -0.0860,  ..., -0.0375, -0.0968, -0.0803],\n",
            "         [ 0.0233,  0.1393, -0.0845,  ..., -0.0516, -0.0761, -0.0722],\n",
            "         ...,\n",
            "         [-0.0714,  0.1014, -0.0921,  ...,  0.0061, -0.0510, -0.1441],\n",
            "         [-0.1125,  0.0797, -0.0275,  ...,  0.0231, -0.0334, -0.1446],\n",
            "         [-0.0539,  0.0569, -0.0633,  ...,  0.0023, -0.1958, -0.0991]],\n",
            "\n",
            "        [[ 0.0075, -0.0722, -0.1125,  ..., -0.0191, -0.1155, -0.1065],\n",
            "         [ 0.0245,  0.0315, -0.0983,  ..., -0.0419, -0.1100, -0.0969],\n",
            "         [ 0.0262,  0.0933, -0.0904,  ..., -0.0538, -0.0953, -0.0838],\n",
            "         ...,\n",
            "         [ 0.0174,  0.1374, -0.0326,  ...,  0.0310, -0.1913, -0.1052],\n",
            "         [ 0.0522,  0.1171, -0.0028,  ..., -0.0445, -0.1933, -0.0498],\n",
            "         [ 0.0589,  0.0806,  0.0046,  ..., -0.0540, -0.2461, -0.0549]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0063,  0.1394, -0.0460,  ..., -0.0477, -0.1500, -0.0599],\n",
            "         [-0.0013,  0.1350, -0.0634,  ..., -0.0712, -0.1007, -0.0527],\n",
            "         [-0.0016,  0.1295, -0.0733,  ..., -0.0739, -0.0804, -0.0483],\n",
            "         ...,\n",
            "         [-0.0667,  0.0693, -0.0429,  ...,  0.0384, -0.0815, -0.0972],\n",
            "         [-0.0552,  0.0538, -0.0489,  ...,  0.0165, -0.0447, -0.1234],\n",
            "         [-0.0860,  0.0806, -0.1647,  ...,  0.0165, -0.0253, -0.1625]],\n",
            "\n",
            "        [[ 0.0311,  0.0854,  0.0050,  ..., -0.1705, -0.0852, -0.0430],\n",
            "         [ 0.0124,  0.1006, -0.0266,  ..., -0.1360, -0.0678, -0.0501],\n",
            "         [ 0.0080,  0.1076, -0.0482,  ..., -0.1074, -0.0694, -0.0489],\n",
            "         ...,\n",
            "         [-0.0520, -0.0188, -0.0258,  ..., -0.0539, -0.3295, -0.0641],\n",
            "         [-0.0557, -0.0053, -0.0197,  ..., -0.0461, -0.2549, -0.0465],\n",
            "         [-0.0579,  0.0076, -0.0117,  ..., -0.0583, -0.2100, -0.0711]],\n",
            "\n",
            "        [[ 0.0051,  0.0527, -0.0489,  ..., -0.0361, -0.1441, -0.0591],\n",
            "         [ 0.0194,  0.0817, -0.0490,  ..., -0.0655, -0.1274, -0.0503],\n",
            "         [ 0.0210,  0.1021, -0.0551,  ..., -0.0739, -0.1066, -0.0444],\n",
            "         ...,\n",
            "         [-0.0136,  0.0904, -0.0167,  ...,  0.0584, -0.0321, -0.1191],\n",
            "         [-0.0221,  0.0740,  0.0108,  ...,  0.1258, -0.1469, -0.1968],\n",
            "         [-0.0792,  0.0505,  0.0044,  ...,  0.0788, -0.2593, -0.0936]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 0.0355,  0.0704, -0.0121,  ..., -0.0088, -0.0893, -0.0729],\n",
            "         [ 0.0298,  0.0944, -0.0363,  ..., -0.0515, -0.0763, -0.0613],\n",
            "         [ 0.0224,  0.1063, -0.0541,  ..., -0.0666, -0.0689, -0.0542],\n",
            "         ...,\n",
            "         [-0.1540,  0.0517, -0.1346,  ...,  0.0289, -0.1664, -0.1110],\n",
            "         [-0.1004,  0.0428, -0.1044,  ...,  0.0155, -0.1541, -0.1015],\n",
            "         [-0.1044,  0.0967,  0.0023,  ...,  0.0687, -0.1435, -0.1623]],\n",
            "\n",
            "        [[ 0.0105,  0.0656, -0.0576,  ..., -0.0676, -0.1626, -0.0633],\n",
            "         [ 0.0130,  0.0938, -0.0588,  ..., -0.0866, -0.1217, -0.0622],\n",
            "         [ 0.0142,  0.1072, -0.0615,  ..., -0.0870, -0.0986, -0.0531],\n",
            "         ...,\n",
            "         [-0.0437,  0.1797,  0.0137,  ...,  0.0468, -0.2196, -0.0542],\n",
            "         [-0.0206,  0.1018,  0.0591,  ...,  0.0843, -0.2892, -0.0665],\n",
            "         [-0.0102,  0.0861,  0.0549,  ...,  0.0066, -0.3711, -0.0394]],\n",
            "\n",
            "        [[ 0.0703,  0.0652, -0.0385,  ..., -0.1122, -0.1553, -0.0700],\n",
            "         [ 0.0488,  0.0862, -0.0559,  ..., -0.1062, -0.1011, -0.0742],\n",
            "         [ 0.0323,  0.1032, -0.0620,  ..., -0.0926, -0.0812, -0.0679],\n",
            "         ...,\n",
            "         [-0.1263,  0.1117, -0.1274,  ...,  0.0231, -0.2477, -0.1801],\n",
            "         [-0.0705,  0.1103, -0.0275,  ...,  0.0959, -0.2237, -0.1808],\n",
            "         [-0.0610,  0.0847, -0.0441,  ...,  0.0623, -0.1714, -0.1456]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0323,  0.1164, -0.0766,  ..., -0.0070, -0.1137, -0.0684],\n",
            "         [ 0.0380,  0.1345, -0.0830,  ..., -0.0394, -0.1028, -0.0661],\n",
            "         [ 0.0286,  0.1400, -0.0839,  ..., -0.0527, -0.0844, -0.0619],\n",
            "         ...,\n",
            "         [-0.0385,  0.0245,  0.0300,  ...,  0.0402, -0.3205, -0.1116],\n",
            "         [-0.0232,  0.0551,  0.0130,  ...,  0.0251, -0.4024, -0.1058],\n",
            "         [-0.0462,  0.0389,  0.0071,  ...,  0.0208, -0.3382, -0.0847]],\n",
            "\n",
            "        [[ 0.0062,  0.0406, -0.0313,  ..., -0.0906, -0.1603, -0.0365],\n",
            "         [ 0.0142,  0.0749, -0.0496,  ..., -0.0978, -0.1181, -0.0362],\n",
            "         [ 0.0138,  0.0986, -0.0620,  ..., -0.0899, -0.0930, -0.0393],\n",
            "         ...,\n",
            "         [ 0.1331,  0.1032,  0.1365,  ..., -0.5145, -0.0938,  0.1189],\n",
            "         [ 0.1343,  0.0966,  0.1244,  ..., -0.4530, -0.0354,  0.0844],\n",
            "         [ 0.1463,  0.0745,  0.0946,  ..., -0.5101,  0.0141,  0.1174]],\n",
            "\n",
            "        [[ 0.0175,  0.0254, -0.0133,  ..., -0.0350, -0.2581, -0.0399],\n",
            "         [ 0.0197,  0.0305, -0.0225,  ..., -0.0975, -0.2284, -0.0481],\n",
            "         [ 0.0221,  0.0486, -0.0266,  ..., -0.1161, -0.1783, -0.0370],\n",
            "         ...,\n",
            "         [-0.0041,  0.0980,  0.0907,  ...,  0.1087, -0.0828, -0.1668],\n",
            "         [-0.0006,  0.0301,  0.0972,  ...,  0.1222, -0.1475, -0.2804],\n",
            "         [-0.0357, -0.0011,  0.0290,  ...,  0.0356, -0.1175, -0.2406]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 0.0005,  0.0781, -0.0293,  ...,  0.0007, -0.1873, -0.0692],\n",
            "         [ 0.0213,  0.0844, -0.0446,  ..., -0.0630, -0.1656, -0.0532],\n",
            "         [ 0.0234,  0.0979, -0.0526,  ..., -0.0842, -0.1275, -0.0461],\n",
            "         ...,\n",
            "         [-0.1579,  0.0866, -0.0211,  ..., -0.0946,  0.0741, -0.2242],\n",
            "         [-0.0832,  0.1004, -0.0410,  ..., -0.2034,  0.1147, -0.1762],\n",
            "         [-0.0897,  0.2147, -0.0156,  ..., -0.1354,  0.0467, -0.0920]],\n",
            "\n",
            "        [[ 0.0183,  0.0659, -0.0207,  ..., -0.1004, -0.2138, -0.0502],\n",
            "         [ 0.0201,  0.0772, -0.0384,  ..., -0.1133, -0.1480, -0.0517],\n",
            "         [ 0.0208,  0.0893, -0.0477,  ..., -0.1084, -0.1162, -0.0475],\n",
            "         ...,\n",
            "         [-0.0147,  0.0528, -0.0568,  ..., -0.0240, -0.2453, -0.0738],\n",
            "         [-0.0512, -0.0085, -0.0952,  ..., -0.1173, -0.0834, -0.1386],\n",
            "         [-0.0117, -0.0381, -0.0833,  ..., -0.0749, -0.1259, -0.1108]],\n",
            "\n",
            "        [[ 0.0082,  0.0754, -0.0358,  ...,  0.0133, -0.1792, -0.0647],\n",
            "         [ 0.0264,  0.0803, -0.0421,  ..., -0.0384, -0.1626, -0.0427],\n",
            "         [ 0.0293,  0.0950, -0.0474,  ..., -0.0639, -0.1330, -0.0360],\n",
            "         ...,\n",
            "         [-0.1094,  0.1365, -0.1581,  ..., -0.0764,  0.0469, -0.0845],\n",
            "         [-0.0281,  0.1461, -0.1535,  ..., -0.0525, -0.0092, -0.1068],\n",
            "         [-0.0380,  0.1687, -0.0477,  ...,  0.0373, -0.1927, -0.1783]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0202,  0.0382, -0.0124,  ..., -0.0798, -0.2497, -0.0496],\n",
            "         [ 0.0250,  0.0586, -0.0239,  ..., -0.1183, -0.1756, -0.0435],\n",
            "         [ 0.0253,  0.0757, -0.0334,  ..., -0.1197, -0.1280, -0.0387],\n",
            "         ...,\n",
            "         [-0.1544,  0.0476, -0.2112,  ...,  0.0104, -0.0208, -0.0588],\n",
            "         [-0.2134,  0.1094, -0.1533,  ...,  0.0173,  0.0045, -0.0652],\n",
            "         [-0.2088,  0.1024, -0.2307,  ...,  0.0179,  0.0253, -0.0741]],\n",
            "\n",
            "        [[ 0.0822,  0.1489,  0.0894,  ..., -0.3853, -0.0151,  0.1240],\n",
            "         [ 0.0251,  0.1390,  0.0540,  ..., -0.2960, -0.0311,  0.0795],\n",
            "         [ 0.0022,  0.1300,  0.0107,  ..., -0.2004, -0.0479,  0.0302],\n",
            "         ...,\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [-0.1957,  0.1501, -0.0492,  ..., -0.0211, -0.1052, -0.1945],\n",
            "         [-0.3725,  0.1061, -0.0230,  ...,  0.0526, -0.2251, -0.2078]],\n",
            "\n",
            "        [[ 0.0263,  0.0074,  0.0076,  ..., -0.0212, -0.1639, -0.0774],\n",
            "         [ 0.0329,  0.0240, -0.0068,  ..., -0.0787, -0.1682, -0.0554],\n",
            "         [ 0.0310,  0.0493, -0.0167,  ..., -0.1017, -0.1408, -0.0402],\n",
            "         ...,\n",
            "         [-0.2066,  0.0296, -0.1152,  ..., -0.0999,  0.0853, -0.0216],\n",
            "         [-0.0433,  0.1103, -0.0683,  ..., -0.0915,  0.0797, -0.0036],\n",
            "         [-0.0032,  0.1318, -0.0577,  ..., -0.0725,  0.0536,  0.0035]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[-0.0465,  0.1540, -0.0484,  ..., -0.1030, -0.0110, -0.0675],\n",
            "         [-0.0246,  0.1495, -0.0622,  ..., -0.0773, -0.0347, -0.0690],\n",
            "         [-0.0108,  0.1434, -0.0747,  ..., -0.0662, -0.0479, -0.0684],\n",
            "         ...,\n",
            "         [-0.0262,  0.0179, -0.0006,  ...,  0.1255, -0.3039, -0.0412],\n",
            "         [-0.0557,  0.0251, -0.0012,  ...,  0.1016, -0.3388, -0.0530],\n",
            "         [-0.0456,  0.0243,  0.0110,  ...,  0.0876, -0.3365, -0.0470]],\n",
            "\n",
            "        [[-0.0050,  0.0147, -0.0809,  ..., -0.0941, -0.0928, -0.0832],\n",
            "         [-0.0038,  0.0720, -0.0825,  ..., -0.0890, -0.0736, -0.0697],\n",
            "         [-0.0036,  0.1046, -0.0859,  ..., -0.0786, -0.0654, -0.0632],\n",
            "         ...,\n",
            "         [-0.0700,  0.0311, -0.0769,  ...,  0.0247, -0.0872, -0.1262],\n",
            "         [-0.0329, -0.0202, -0.0790,  ...,  0.0211, -0.1179, -0.1012],\n",
            "         [-0.0346, -0.0638, -0.0743,  ...,  0.0246, -0.0515, -0.1632]],\n",
            "\n",
            "        [[-0.0095,  0.1250, -0.0957,  ...,  0.0036, -0.1244, -0.1109],\n",
            "         [ 0.0041,  0.1351, -0.0924,  ..., -0.0292, -0.0922, -0.0874],\n",
            "         [ 0.0094,  0.1342, -0.0905,  ..., -0.0489, -0.0762, -0.0724],\n",
            "         ...,\n",
            "         [ 0.0606, -0.1836, -0.0617,  ..., -0.2143, -0.0380,  0.0047],\n",
            "         [ 0.0501, -0.3217, -0.0228,  ..., -0.2044,  0.0075,  0.0128],\n",
            "         [ 0.0429, -0.4784, -0.0222,  ..., -0.1793,  0.0954,  0.0088]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0032,  0.1570, -0.0913,  ...,  0.0207, -0.1257, -0.0580],\n",
            "         [ 0.0386,  0.1567, -0.0905,  ..., -0.0027, -0.1327, -0.0665],\n",
            "         [ 0.0413,  0.1494, -0.0821,  ..., -0.0277, -0.1142, -0.0635],\n",
            "         ...,\n",
            "         [ 0.0006,  0.1433, -0.2076,  ..., -0.0027, -0.0365, -0.0891],\n",
            "         [-0.0027,  0.1624, -0.1627,  ..., -0.0088, -0.0634, -0.1122],\n",
            "         [-0.0059,  0.1361, -0.1462,  ..., -0.0252, -0.2094, -0.1476]],\n",
            "\n",
            "        [[-0.0860,  0.0556, -0.0451,  ..., -0.0309, -0.1958, -0.0979],\n",
            "         [-0.0279,  0.0761, -0.0490,  ..., -0.0764, -0.1400, -0.0831],\n",
            "         [-0.0017,  0.0955, -0.0485,  ..., -0.0851, -0.1058, -0.0646],\n",
            "         ...,\n",
            "         [-0.0494,  0.1372, -0.1335,  ..., -0.1005,  0.0238, -0.1030],\n",
            "         [-0.0991,  0.1763, -0.2381,  ..., -0.0338,  0.0380, -0.1844],\n",
            "         [-0.1116,  0.1563, -0.3659,  ...,  0.0037, -0.0606, -0.1181]],\n",
            "\n",
            "        [[ 0.0111,  0.1658, -0.0618,  ..., -0.0579, -0.0252, -0.0195],\n",
            "         [ 0.0056,  0.1604, -0.0760,  ..., -0.0538, -0.0441, -0.0480],\n",
            "         [ 0.0045,  0.1538, -0.0849,  ..., -0.0522, -0.0539, -0.0624],\n",
            "         ...,\n",
            "         [-0.0622,  0.1208, -0.1509,  ..., -0.0102, -0.0423, -0.1354],\n",
            "         [-0.0705,  0.1349, -0.0654,  ...,  0.0036, -0.0085, -0.1071],\n",
            "         [-0.0546,  0.0906, -0.1614,  ..., -0.0215,  0.0077, -0.1057]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 0.0037,  0.0124, -0.0090,  ..., -0.0308, -0.3215, -0.0336],\n",
            "         [ 0.0084,  0.0150, -0.0157,  ..., -0.0934, -0.3294, -0.0417],\n",
            "         [ 0.0155,  0.0298, -0.0175,  ..., -0.1217, -0.2744, -0.0350],\n",
            "         ...,\n",
            "         [ 0.0545, -0.7520, -0.1583,  ..., -0.1655,  0.1909,  0.1190],\n",
            "         [ 0.0192, -0.8109, -0.1466,  ..., -0.1012,  0.1963,  0.0579],\n",
            "         [-0.0036, -0.7869, -0.1255,  ..., -0.0907,  0.1980,  0.0082]],\n",
            "\n",
            "        [[ 0.0181, -0.0018, -0.0535,  ...,  0.0110, -0.1089, -0.0926],\n",
            "         [ 0.0369,  0.0580, -0.0579,  ..., -0.0194, -0.1118, -0.0744],\n",
            "         [ 0.0357,  0.0996, -0.0653,  ..., -0.0423, -0.0949, -0.0648],\n",
            "         ...,\n",
            "         [ 0.0943,  0.0103,  0.0017,  ..., -0.1830, -0.1674, -0.0235],\n",
            "         [ 0.1101, -0.0847, -0.0717,  ..., -0.2845, -0.1392, -0.1762],\n",
            "         [ 0.0613, -0.0335, -0.0583,  ..., -0.2080, -0.1008, -0.1863]],\n",
            "\n",
            "        [[ 0.0233, -0.3128, -0.0217,  ..., -0.0956, -0.0580, -0.0264],\n",
            "         [ 0.0243, -0.0949, -0.0402,  ..., -0.0848, -0.0775, -0.0546],\n",
            "         [ 0.0203,  0.0385, -0.0560,  ..., -0.0692, -0.0784, -0.0728],\n",
            "         ...,\n",
            "         [-0.0141,  0.0353, -0.0640,  ...,  0.0349, -0.4042, -0.0607],\n",
            "         [-0.0058,  0.0324, -0.0438,  ...,  0.0632, -0.4348, -0.0995],\n",
            "         [-0.0298,  0.0565, -0.0337,  ...,  0.0886, -0.3884, -0.0664]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0081,  0.1703, -0.1107,  ..., -0.0408, -0.1338, -0.1285],\n",
            "         [ 0.0143,  0.1632, -0.1057,  ..., -0.0498, -0.0994, -0.0962],\n",
            "         [ 0.0156,  0.1562, -0.0983,  ..., -0.0545, -0.0814, -0.0784],\n",
            "         ...,\n",
            "         [-0.0217,  0.0173, -0.0026,  ...,  0.0839, -0.3571, -0.1357],\n",
            "         [-0.0157,  0.0112, -0.0233,  ...,  0.0711, -0.4232, -0.0570],\n",
            "         [-0.0393,  0.0317,  0.0101,  ...,  0.0804, -0.3845, -0.0729]],\n",
            "\n",
            "        [[-0.0015,  0.1749, -0.1664,  ..., -0.0089, -0.1295, -0.0738],\n",
            "         [ 0.0194,  0.1586, -0.1371,  ..., -0.0378, -0.1032, -0.0679],\n",
            "         [ 0.0182,  0.1557, -0.1152,  ..., -0.0498, -0.0795, -0.0668],\n",
            "         ...,\n",
            "         [ 0.0041,  0.1302, -0.2137,  ..., -0.0143, -0.0625, -0.0661],\n",
            "         [ 0.0108,  0.1698, -0.1014,  ...,  0.0159, -0.1825, -0.0828],\n",
            "         [ 0.0460,  0.1229, -0.0254,  ...,  0.0622, -0.1386, -0.1501]],\n",
            "\n",
            "        [[ 0.0217,  0.1323, -0.0835,  ..., -0.0171, -0.0902, -0.0665],\n",
            "         [ 0.0297,  0.1364, -0.0891,  ..., -0.0392, -0.0835, -0.0665],\n",
            "         [ 0.0229,  0.1436, -0.0905,  ..., -0.0480, -0.0702, -0.0651],\n",
            "         ...,\n",
            "         [-0.1065,  0.0821, -0.1662,  ...,  0.0871, -0.1458, -0.1831],\n",
            "         [-0.1050,  0.0694, -0.0734,  ...,  0.0545, -0.2421, -0.1162],\n",
            "         [-0.0963,  0.0620, -0.0165,  ...,  0.0622, -0.2857, -0.0797]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 1.0101e-02, -6.4856e-01, -9.3244e-02,  ..., -5.2375e-02,\n",
            "          -2.2366e-02, -3.9939e-02],\n",
            "         [ 2.3085e-02, -4.1934e-01, -7.8071e-02,  ..., -5.4864e-02,\n",
            "          -8.2659e-02, -6.5601e-02],\n",
            "         [ 2.6936e-02, -1.5429e-01, -6.9574e-02,  ..., -5.4068e-02,\n",
            "          -9.0310e-02, -7.8202e-02],\n",
            "         ...,\n",
            "         [ 6.3466e-02, -6.7743e-02,  4.6510e-02,  ..., -4.0014e-01,\n",
            "          -3.0412e-02,  3.2343e-01],\n",
            "         [ 1.6232e-01, -3.1994e-01,  7.0066e-02,  ..., -6.4017e-01,\n",
            "          -9.0711e-02,  3.7383e-01],\n",
            "         [ 1.6399e-01, -4.3822e-01,  8.9308e-02,  ..., -6.2671e-01,\n",
            "          -7.8715e-02,  4.5813e-01]],\n",
            "\n",
            "        [[ 5.2556e-02,  4.8972e-02, -4.7245e-02,  ..., -1.5078e-01,\n",
            "          -1.0545e-01, -1.3306e-01],\n",
            "         [ 3.3950e-02,  9.8611e-02, -5.8852e-02,  ..., -1.1781e-01,\n",
            "          -8.7812e-02, -1.0242e-01],\n",
            "         [ 2.0328e-02,  1.2094e-01, -7.0776e-02,  ..., -9.2627e-02,\n",
            "          -7.6525e-02, -8.2378e-02],\n",
            "         ...,\n",
            "         [ 5.6183e-03,  1.3523e-01, -9.6390e-02,  ..., -6.1194e-02,\n",
            "          -6.2959e-02, -5.8787e-02],\n",
            "         [-1.0470e-01,  1.6664e-01, -1.2853e-01,  ...,  4.7071e-02,\n",
            "          -4.0441e-02, -1.0813e-01],\n",
            "         [ 4.9727e-03,  1.2672e-01, -1.1386e-01,  ...,  4.5630e-02,\n",
            "          -8.2141e-02, -7.0971e-02]],\n",
            "\n",
            "        [[ 8.3880e-03,  2.9521e-02, -4.0684e-02,  ..., -3.0563e-02,\n",
            "          -3.0139e-01, -4.7733e-02],\n",
            "         [ 1.8913e-02,  3.6806e-02, -3.4174e-02,  ..., -9.6810e-02,\n",
            "          -2.6085e-01, -4.2913e-02],\n",
            "         [ 2.2195e-02,  5.7417e-02, -3.2949e-02,  ..., -1.1686e-01,\n",
            "          -1.9466e-01, -3.6675e-02],\n",
            "         ...,\n",
            "         [-1.6981e-02,  1.1662e-01,  1.8348e-02,  ..., -8.4306e-02,\n",
            "          -1.4516e-01, -1.3463e-01],\n",
            "         [-1.0297e-01,  1.0545e-01,  7.5943e-02,  ...,  4.3440e-02,\n",
            "          -1.8259e-01, -2.5522e-01],\n",
            "         [ 4.3729e-02,  2.8819e-02,  8.0224e-02,  ..., -5.9842e-02,\n",
            "          -1.4930e-01, -1.2755e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-7.8822e-03,  2.2447e-02, -1.5340e-02,  ..., -1.1322e-02,\n",
            "          -2.7792e-01, -6.3369e-02],\n",
            "         [ 1.4679e-02,  3.6987e-02, -1.8719e-02,  ..., -7.4181e-02,\n",
            "          -2.1928e-01, -5.0442e-02],\n",
            "         [ 2.3843e-02,  5.6965e-02, -2.5203e-02,  ..., -1.0154e-01,\n",
            "          -1.6736e-01, -4.2893e-02],\n",
            "         ...,\n",
            "         [-6.8553e-02,  1.5725e-03, -1.7317e-01,  ..., -1.3313e-02,\n",
            "          -2.0139e-02, -9.1222e-02],\n",
            "         [-1.6928e-01,  1.4829e-02, -3.2181e-01,  ..., -1.1290e-02,\n",
            "           1.4503e-01, -1.6499e-01],\n",
            "         [-8.9641e-02, -2.2764e-02, -2.3573e-01,  ..., -4.8360e-03,\n",
            "           7.7517e-02, -8.6811e-02]],\n",
            "\n",
            "        [[ 5.0620e-02,  1.2312e-01, -6.4549e-02,  ...,  1.2109e-02,\n",
            "          -8.3179e-02, -1.1451e-01],\n",
            "         [ 2.7702e-02,  1.3527e-01, -8.3843e-02,  ..., -2.5448e-02,\n",
            "          -5.7503e-02, -9.6206e-02],\n",
            "         [ 1.6796e-02,  1.3740e-01, -8.9900e-02,  ..., -4.5573e-02,\n",
            "          -5.2884e-02, -7.9187e-02],\n",
            "         ...,\n",
            "         [ 5.6183e-03,  1.3523e-01, -9.6390e-02,  ..., -6.1194e-02,\n",
            "          -6.2959e-02, -5.8787e-02],\n",
            "         [ 5.6183e-03,  1.3523e-01, -9.6390e-02,  ..., -6.1194e-02,\n",
            "          -6.2959e-02, -5.8787e-02],\n",
            "         [ 5.6183e-03,  1.3523e-01, -9.6390e-02,  ..., -6.1194e-02,\n",
            "          -6.2959e-02, -5.8787e-02]],\n",
            "\n",
            "        [[ 1.5408e-02,  6.8961e-02, -1.1636e-02,  ..., -5.9662e-04,\n",
            "          -2.4186e-01, -5.2206e-02],\n",
            "         [ 2.7256e-02,  8.4981e-02, -2.9742e-02,  ..., -5.2669e-02,\n",
            "          -1.7249e-01, -6.1387e-02],\n",
            "         [ 2.5714e-02,  1.0306e-01, -4.1583e-02,  ..., -6.9820e-02,\n",
            "          -1.2076e-01, -5.8438e-02],\n",
            "         ...,\n",
            "         [ 8.3360e-03,  1.4385e-01, -2.0892e-03,  ...,  3.4877e-02,\n",
            "          -1.9133e-01, -1.4420e-01],\n",
            "         [ 7.8980e-03,  1.7876e-01,  3.1423e-02,  ...,  1.2609e-01,\n",
            "          -2.6198e-01, -2.2206e-01],\n",
            "         [-1.0869e-01,  9.9545e-02, -3.4376e-02,  ...,  1.1842e-01,\n",
            "          -2.1852e-01, -1.7359e-01]]], device='cuda:0',\n",
            "       grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 0.0701, -0.3282,  0.1196,  ..., -0.4873, -0.0487,  0.3442],\n",
            "         [ 0.0096, -0.1487,  0.1012,  ..., -0.3801, -0.0390,  0.3076],\n",
            "         [-0.0095, -0.0276,  0.0568,  ..., -0.2579, -0.0480,  0.2115],\n",
            "         ...,\n",
            "         [-0.1791,  0.0862,  0.0420,  ...,  0.0430, -0.1103, -0.1573],\n",
            "         [-0.0414,  0.0381, -0.0203,  ..., -0.0267, -0.1452, -0.1140],\n",
            "         [ 0.0087,  0.0355,  0.0115,  ..., -0.0948, -0.1782, -0.1661]],\n",
            "\n",
            "        [[ 0.0210,  0.1113, -0.1083,  ..., -0.0186, -0.0896, -0.0674],\n",
            "         [ 0.0209,  0.1223, -0.0946,  ..., -0.0473, -0.0835, -0.0630],\n",
            "         [ 0.0157,  0.1287, -0.0907,  ..., -0.0584, -0.0746, -0.0605],\n",
            "         ...,\n",
            "         [ 0.0515,  0.0651,  0.0436,  ..., -0.0241, -0.2696, -0.0331],\n",
            "         [ 0.0806, -0.0270, -0.0046,  ..., -0.1514, -0.1047, -0.0492],\n",
            "         [ 0.0979, -0.0741, -0.0360,  ..., -0.1903, -0.0112,  0.0244]],\n",
            "\n",
            "        [[ 0.0279,  0.1065,  0.0335,  ..., -0.1078, -0.0685, -0.0882],\n",
            "         [ 0.0265,  0.1005, -0.0014,  ..., -0.1101, -0.0606, -0.0637],\n",
            "         [ 0.0218,  0.1065, -0.0312,  ..., -0.0984, -0.0672, -0.0565],\n",
            "         ...,\n",
            "         [ 0.0355,  0.0442,  0.0079,  ..., -0.0051, -0.2770, -0.0521],\n",
            "         [ 0.0570,  0.0435,  0.0053,  ...,  0.0163, -0.2651, -0.0524],\n",
            "         [ 0.0462,  0.0705,  0.0135,  ...,  0.0743, -0.2918, -0.0491]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0136,  0.0581, -0.1233,  ..., -0.0019, -0.0933, -0.0871],\n",
            "         [ 0.0295,  0.1002, -0.1150,  ..., -0.0238, -0.0993, -0.0841],\n",
            "         [ 0.0283,  0.1247, -0.1017,  ..., -0.0408, -0.0905, -0.0769],\n",
            "         ...,\n",
            "         [-0.0098,  0.0223, -0.0285,  ...,  0.0397, -0.4506, -0.1804],\n",
            "         [-0.0154,  0.0110, -0.0108,  ...,  0.0232, -0.3504, -0.1668],\n",
            "         [-0.0171,  0.0271, -0.0425,  ...,  0.0395, -0.3875, -0.1405]],\n",
            "\n",
            "        [[ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         ...,\n",
            "         [-0.0857,  0.1447, -0.0851,  ...,  0.0453, -0.0099, -0.0543],\n",
            "         [ 0.0344,  0.0810, -0.0746,  ...,  0.0064,  0.0507, -0.0545],\n",
            "         [-0.0500,  0.0477, -0.0615,  ...,  0.0135,  0.1171, -0.0730]],\n",
            "\n",
            "        [[-0.0079,  0.0639, -0.0529,  ..., -0.0217, -0.1670, -0.0684],\n",
            "         [ 0.0069,  0.0774, -0.0506,  ..., -0.0796, -0.1424, -0.0526],\n",
            "         [ 0.0152,  0.0863, -0.0526,  ..., -0.0959, -0.1189, -0.0425],\n",
            "         ...,\n",
            "         [ 0.0976, -0.4065,  0.0856,  ..., -0.4781,  0.0258,  0.2730],\n",
            "         [ 0.1779, -0.5799,  0.0860,  ..., -0.4892,  0.0370,  0.2555],\n",
            "         [ 0.0630, -0.6317,  0.1202,  ..., -0.4261,  0.0143,  0.3317]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 6.5564e-03,  5.7947e-02, -3.1012e-02,  ..., -1.0455e-01,\n",
            "          -1.1412e-01, -1.0100e-01],\n",
            "         [ 1.2169e-02,  7.7700e-02, -4.7195e-02,  ..., -9.3226e-02,\n",
            "          -9.1752e-02, -7.0898e-02],\n",
            "         [ 1.5179e-02,  9.4207e-02, -5.7853e-02,  ..., -8.6705e-02,\n",
            "          -8.4166e-02, -5.7437e-02],\n",
            "         ...,\n",
            "         [ 3.2048e-03,  5.3791e-02, -3.0008e-02,  ..., -4.5327e-02,\n",
            "          -1.8256e-01, -4.2285e-02],\n",
            "         [ 3.9779e-02,  7.7206e-02, -3.3507e-02,  ..., -7.3679e-02,\n",
            "          -2.0491e-01, -2.1185e-02],\n",
            "         [ 5.8400e-03,  6.3592e-02, -4.2547e-02,  ..., -1.2228e-01,\n",
            "          -1.1347e-01, -5.2484e-02]],\n",
            "\n",
            "        [[ 4.6270e-02,  1.9258e-02, -3.7513e-02,  ..., -1.5700e-01,\n",
            "          -3.0688e-02,  2.0696e-02],\n",
            "         [ 1.2870e-02,  8.7979e-02, -5.4325e-02,  ..., -1.1971e-01,\n",
            "          -4.4884e-02, -1.1735e-02],\n",
            "         [ 4.3905e-03,  1.1717e-01, -6.9001e-02,  ..., -8.9645e-02,\n",
            "          -5.6790e-02, -3.5544e-02],\n",
            "         ...,\n",
            "         [ 5.6946e-02,  1.3283e-01,  6.7458e-05,  ...,  9.5645e-02,\n",
            "          -2.9348e-01, -1.3359e-01],\n",
            "         [ 3.1834e-02,  8.8872e-02,  2.2273e-02,  ...,  1.1721e-01,\n",
            "          -3.3814e-01, -1.3742e-01],\n",
            "         [-1.3342e-02,  7.4592e-02, -1.4096e-02,  ...,  1.1719e-01,\n",
            "          -3.8443e-01, -1.1725e-01]],\n",
            "\n",
            "        [[ 6.2230e-02,  3.9714e-02, -4.2551e-02,  ..., -4.9733e-02,\n",
            "          -1.7019e-01, -6.5373e-02],\n",
            "         [ 5.0030e-02,  6.4550e-02, -4.5291e-02,  ..., -8.7751e-02,\n",
            "          -1.2954e-01, -6.8038e-02],\n",
            "         [ 3.8970e-02,  8.3116e-02, -5.0998e-02,  ..., -9.4512e-02,\n",
            "          -1.0790e-01, -6.2612e-02],\n",
            "         ...,\n",
            "         [ 1.1197e-01, -3.4645e-02,  2.8581e-02,  ..., -2.9225e-01,\n",
            "           4.9153e-03,  5.9746e-02],\n",
            "         [ 1.0341e-01, -2.0885e-01,  1.6920e-02,  ..., -4.3303e-01,\n",
            "           3.6275e-02,  2.6324e-01],\n",
            "         [-1.1413e-01, -4.3946e-01,  8.5069e-02,  ..., -3.6996e-01,\n",
            "           1.0334e-01,  2.2924e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 8.6062e-03,  2.2342e-02, -2.8594e-02,  ..., -1.7660e-02,\n",
            "          -2.7008e-01, -5.8306e-02],\n",
            "         [ 2.2134e-02,  3.4030e-02, -2.2431e-02,  ..., -7.3366e-02,\n",
            "          -2.5071e-01, -3.9014e-02],\n",
            "         [ 2.4631e-02,  5.4066e-02, -2.4776e-02,  ..., -1.0394e-01,\n",
            "          -1.9170e-01, -3.1471e-02],\n",
            "         ...,\n",
            "         [ 5.6183e-03,  1.3523e-01, -9.6390e-02,  ..., -6.1194e-02,\n",
            "          -6.2959e-02, -5.8787e-02],\n",
            "         [-7.3953e-03,  1.5359e-01, -1.1291e-01,  ..., -1.3122e-02,\n",
            "          -1.0778e-01, -9.5820e-02],\n",
            "         [ 1.8667e-02,  1.2059e-01, -1.2632e-01,  ...,  3.8945e-02,\n",
            "          -1.7984e-01, -1.1935e-01]],\n",
            "\n",
            "        [[ 2.5542e-02,  9.3355e-02, -5.3712e-02,  ...,  6.0124e-03,\n",
            "          -5.8307e-02, -7.2436e-02],\n",
            "         [ 4.0171e-02,  1.1878e-01, -6.5108e-02,  ..., -2.4741e-02,\n",
            "          -8.2942e-02, -7.7401e-02],\n",
            "         [ 3.1879e-02,  1.3483e-01, -7.4519e-02,  ..., -4.4874e-02,\n",
            "          -7.5295e-02, -7.8793e-02],\n",
            "         ...,\n",
            "         [ 5.9183e-02,  1.2203e-01, -2.8695e-01,  ..., -1.1453e-01,\n",
            "          -6.6799e-02, -1.7244e-01],\n",
            "         [ 4.9368e-02,  1.3994e-01, -2.7247e-01,  ..., -1.0066e-01,\n",
            "          -1.1489e-01, -1.5940e-01],\n",
            "         [ 7.8500e-03,  1.2829e-01, -1.7782e-01,  ..., -6.7431e-02,\n",
            "          -1.0583e-01, -1.4864e-01]],\n",
            "\n",
            "        [[-2.1727e-02, -3.9718e-01,  1.1691e-01,  ..., -3.5895e-01,\n",
            "          -3.1855e-02,  3.2341e-01],\n",
            "         [-4.5895e-02, -1.5539e-01,  9.7675e-02,  ..., -2.8236e-01,\n",
            "          -3.5748e-02,  2.5880e-01],\n",
            "         [-3.0032e-02, -1.6741e-02,  5.4285e-02,  ..., -1.8973e-01,\n",
            "          -4.8620e-02,  1.5526e-01],\n",
            "         ...,\n",
            "         [-1.0394e-01,  1.5866e-02, -1.3749e-01,  ...,  6.1914e-02,\n",
            "           4.1059e-02, -8.1733e-02],\n",
            "         [-1.0387e-01, -4.4867e-02, -2.0025e-01,  ...,  2.0893e-02,\n",
            "           1.1686e-01, -8.0412e-02],\n",
            "         [-1.4416e-01, -1.2176e-02, -1.6458e-01,  ...,  2.3088e-02,\n",
            "           4.7522e-02, -7.5642e-02]]], device='cuda:0',\n",
            "       grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 0.0421,  0.0951, -0.0228,  ..., -0.0997, -0.1138, -0.0382],\n",
            "         [ 0.0301,  0.1122, -0.0467,  ..., -0.0888, -0.0772, -0.0490],\n",
            "         [ 0.0161,  0.1266, -0.0659,  ..., -0.0738, -0.0613, -0.0560],\n",
            "         ...,\n",
            "         [ 0.0155,  0.0826, -0.0754,  ...,  0.0671, -0.1420, -0.1601],\n",
            "         [-0.0423,  0.0649, -0.0216,  ...,  0.0713, -0.1633, -0.1486],\n",
            "         [-0.0220,  0.0682, -0.0059,  ...,  0.0785, -0.1723, -0.0635]],\n",
            "\n",
            "        [[ 0.0360,  0.0411, -0.0360,  ..., -0.0360, -0.2733, -0.0651],\n",
            "         [ 0.0302,  0.0587, -0.0387,  ..., -0.0924, -0.1938, -0.0583],\n",
            "         [ 0.0252,  0.0766, -0.0442,  ..., -0.1027, -0.1377, -0.0510],\n",
            "         ...,\n",
            "         [ 0.1247, -0.7780,  0.0175,  ..., -0.4645,  0.0609,  0.2243],\n",
            "         [ 0.0167, -0.8709,  0.0315,  ..., -0.3985, -0.0041,  0.2589],\n",
            "         [ 0.0568, -0.8776,  0.0631,  ..., -0.5170,  0.0577,  0.3038]],\n",
            "\n",
            "        [[-0.0489, -0.3094,  0.0693,  ..., -0.2588, -0.0105,  0.1865],\n",
            "         [-0.0344, -0.1072,  0.0405,  ..., -0.2022, -0.0433,  0.1279],\n",
            "         [-0.0185,  0.0167,  0.0041,  ..., -0.1427, -0.0555,  0.0540],\n",
            "         ...,\n",
            "         [-0.1108,  0.1898, -0.1009,  ...,  0.0196, -0.0567, -0.0944],\n",
            "         [-0.1189,  0.1145, -0.3213,  ...,  0.0248, -0.0156, -0.0955],\n",
            "         [-0.1090,  0.0683, -0.2978,  ...,  0.0112, -0.1109, -0.0877]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0498,  0.1036, -0.1105,  ..., -0.0181, -0.1417, -0.1056],\n",
            "         [ 0.0368,  0.1162, -0.1019,  ..., -0.0479, -0.0945, -0.0977],\n",
            "         [ 0.0239,  0.1247, -0.0934,  ..., -0.0580, -0.0718, -0.0835],\n",
            "         ...,\n",
            "         [-0.0650,  0.0996,  0.0354,  ...,  0.1730, -0.2008, -0.1401],\n",
            "         [ 0.0362,  0.0138,  0.0233,  ..., -0.0905, -0.2534, -0.0407],\n",
            "         [-0.0643,  0.1160,  0.0426,  ..., -0.0318, -0.1572, -0.1468]],\n",
            "\n",
            "        [[ 0.0398,  0.1551, -0.1190,  ..., -0.0601, -0.1053, -0.1081],\n",
            "         [ 0.0293,  0.1582, -0.1079,  ..., -0.0623, -0.0828, -0.0993],\n",
            "         [ 0.0194,  0.1562, -0.0999,  ..., -0.0596, -0.0734, -0.0870],\n",
            "         ...,\n",
            "         [-0.1448,  0.0588,  0.0520,  ...,  0.1694, -0.2213, -0.2306],\n",
            "         [-0.1238,  0.0713, -0.0055,  ...,  0.0927, -0.2753, -0.1757],\n",
            "         [-0.0620,  0.0466, -0.0399,  ...,  0.0131, -0.3691, -0.0642]],\n",
            "\n",
            "        [[ 0.0009,  0.0457, -0.1257,  ...,  0.0239, -0.0607, -0.0770],\n",
            "         [ 0.0290,  0.0929, -0.1111,  ..., -0.0031, -0.0865, -0.0779],\n",
            "         [ 0.0298,  0.1187, -0.1020,  ..., -0.0287, -0.0830, -0.0742],\n",
            "         ...,\n",
            "         [-0.0072,  0.0429, -0.2459,  ..., -0.0634, -0.1564, -0.1014],\n",
            "         [-0.0043,  0.0811, -0.1313,  ..., -0.0307, -0.2801, -0.0956],\n",
            "         [-0.0146,  0.0679, -0.2030,  ..., -0.0323, -0.2327, -0.1159]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 0.0016,  0.0543, -0.0372,  ...,  0.0067, -0.1768, -0.0498],\n",
            "         [ 0.0166,  0.0578, -0.0421,  ..., -0.0559, -0.1662, -0.0362],\n",
            "         [ 0.0219,  0.0732, -0.0455,  ..., -0.0815, -0.1391, -0.0302],\n",
            "         ...,\n",
            "         [-0.0341,  0.1632, -0.0748,  ..., -0.0048, -0.1080, -0.0289],\n",
            "         [-0.0054,  0.1541, -0.1281,  ..., -0.0472, -0.1073, -0.0274],\n",
            "         [-0.0578,  0.1540, -0.1427,  ..., -0.0160, -0.0797, -0.0661]],\n",
            "\n",
            "        [[-0.0179, -0.8277,  0.0728,  ..., -0.2950, -0.0455,  0.3102],\n",
            "         [-0.0150, -0.6368,  0.0740,  ..., -0.2386, -0.0596,  0.2523],\n",
            "         [-0.0079, -0.3880,  0.0461,  ..., -0.1607, -0.0588,  0.1575],\n",
            "         ...,\n",
            "         [-0.1050,  0.1165, -0.1631,  ..., -0.0085,  0.0394, -0.1508],\n",
            "         [ 0.0347,  0.1004, -0.0957,  ..., -0.0153,  0.0118, -0.1553],\n",
            "         [ 0.0550,  0.0298, -0.1061,  ..., -0.0633,  0.0741, -0.1075]],\n",
            "\n",
            "        [[ 0.0067,  0.0961, -0.0971,  ..., -0.0061, -0.1588, -0.0671],\n",
            "         [ 0.0303,  0.1113, -0.0847,  ..., -0.0328, -0.1354, -0.0632],\n",
            "         [ 0.0297,  0.1270, -0.0794,  ..., -0.0474, -0.1032, -0.0581],\n",
            "         ...,\n",
            "         [ 0.0030,  0.0980, -0.0190,  ..., -0.0554, -0.1569, -0.1359],\n",
            "         [ 0.0641,  0.0515, -0.0062,  ..., -0.1517, -0.1979, -0.1962],\n",
            "         [ 0.1032,  0.0957,  0.0077,  ..., -0.1201, -0.2456, -0.1699]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0201,  0.0504,  0.0056,  ..., -0.0934, -0.1479, -0.0657],\n",
            "         [ 0.0250,  0.0600, -0.0187,  ..., -0.1120, -0.1173, -0.0586],\n",
            "         [ 0.0239,  0.0764, -0.0348,  ..., -0.1098, -0.0969, -0.0531],\n",
            "         ...,\n",
            "         [ 0.0195,  0.0705, -0.0122,  ..., -0.0020, -0.1993, -0.0621],\n",
            "         [ 0.0036,  0.0824, -0.0305,  ..., -0.0335, -0.2586, -0.0944],\n",
            "         [ 0.0137,  0.0390, -0.1043,  ..., -0.0236, -0.1970, -0.2123]],\n",
            "\n",
            "        [[-0.0019,  0.0471, -0.0350,  ..., -0.0874, -0.2781, -0.0450],\n",
            "         [ 0.0122,  0.0581, -0.0342,  ..., -0.1166, -0.2153, -0.0431],\n",
            "         [ 0.0191,  0.0716, -0.0370,  ..., -0.1205, -0.1590, -0.0398],\n",
            "         ...,\n",
            "         [-0.0287,  0.0868, -0.0100,  ...,  0.0201, -0.1829, -0.0706],\n",
            "         [-0.0166,  0.1255,  0.0011,  ...,  0.0842, -0.2306, -0.1514],\n",
            "         [-0.0735,  0.0641, -0.0495,  ...,  0.1281, -0.1727, -0.1297]],\n",
            "\n",
            "        [[ 0.0219,  0.0985, -0.1380,  ..., -0.0417, -0.1815, -0.0636],\n",
            "         [ 0.0306,  0.1163, -0.1138,  ..., -0.0614, -0.1358, -0.0551],\n",
            "         [ 0.0262,  0.1282, -0.1003,  ..., -0.0678, -0.0976, -0.0548],\n",
            "         ...,\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [ 0.0015,  0.0853, -0.1227,  ..., -0.0979,  0.0143, -0.0629],\n",
            "         [-0.0222,  0.1056, -0.0760,  ..., -0.1197, -0.0427, -0.1027]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[-3.2472e-03,  1.2778e-01, -9.2579e-02,  ..., -3.0184e-02,\n",
            "          -9.3934e-02, -5.4736e-02],\n",
            "         [ 1.5287e-03,  1.3177e-01, -9.3832e-02,  ..., -4.5186e-02,\n",
            "          -7.3555e-02, -6.4581e-02],\n",
            "         [ 2.9389e-03,  1.3642e-01, -9.3331e-02,  ..., -4.9922e-02,\n",
            "          -6.2141e-02, -6.5111e-02],\n",
            "         ...,\n",
            "         [ 8.5595e-02, -2.1224e-01,  1.3744e-03,  ..., -2.2128e-01,\n",
            "           9.2696e-02,  1.3604e-01],\n",
            "         [ 2.2255e-02, -1.1630e-01,  4.6518e-02,  ..., -2.0090e-01,\n",
            "           1.6829e-02,  1.3814e-01],\n",
            "         [ 6.3048e-02, -7.6793e-02,  5.8710e-03,  ..., -2.1990e-01,\n",
            "           1.1923e-01,  5.2272e-02]],\n",
            "\n",
            "        [[ 2.2073e-02,  1.4484e-01, -6.2823e-02,  ..., -3.2969e-02,\n",
            "          -2.4568e-02, -9.9901e-02],\n",
            "         [ 2.1173e-02,  1.4365e-01, -8.1907e-02,  ..., -4.0326e-02,\n",
            "          -5.5084e-02, -8.5501e-02],\n",
            "         [ 1.8457e-02,  1.4899e-01, -8.8983e-02,  ..., -4.6245e-02,\n",
            "          -6.5650e-02, -7.9001e-02],\n",
            "         ...,\n",
            "         [-1.5481e-01,  1.1768e-01,  2.2027e-02,  ...,  8.1410e-02,\n",
            "          -6.3917e-02, -1.7996e-01],\n",
            "         [-9.1377e-02,  9.8528e-02, -1.3027e-03,  ...,  2.2135e-02,\n",
            "          -6.1245e-02, -1.8135e-01],\n",
            "         [-2.0347e-01,  1.6439e-01,  1.0807e-01,  ...,  7.0077e-02,\n",
            "          -1.9583e-01, -3.0553e-01]],\n",
            "\n",
            "        [[ 4.6861e-02,  1.3168e-01, -3.1331e-02,  ..., -1.3785e-01,\n",
            "          -1.3647e-01, -1.2862e-01],\n",
            "         [ 1.7347e-02,  1.3106e-01, -4.9658e-02,  ..., -1.1468e-01,\n",
            "          -9.7351e-02, -8.9936e-02],\n",
            "         [ 9.1473e-03,  1.2762e-01, -6.4347e-02,  ..., -9.4028e-02,\n",
            "          -8.5316e-02, -6.7820e-02],\n",
            "         ...,\n",
            "         [ 5.6183e-03,  1.3523e-01, -9.6390e-02,  ..., -6.1194e-02,\n",
            "          -6.2959e-02, -5.8787e-02],\n",
            "         [-3.0400e-02,  1.5979e-01, -6.6242e-02,  ...,  5.7603e-02,\n",
            "          -1.3060e-01, -8.5557e-02],\n",
            "         [-7.7298e-03,  9.0795e-02, -8.0877e-02,  ..., -1.8297e-03,\n",
            "          -2.0970e-01, -8.2715e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 2.5846e-02,  6.9568e-02, -7.0259e-02,  ..., -1.7280e-02,\n",
            "          -1.7499e-01, -1.1448e-01],\n",
            "         [ 3.0056e-02,  7.9559e-02, -6.8616e-02,  ..., -5.4091e-02,\n",
            "          -1.4869e-01, -7.9157e-02],\n",
            "         [ 2.7624e-02,  9.6683e-02, -6.7157e-02,  ..., -7.1392e-02,\n",
            "          -1.2138e-01, -6.2274e-02],\n",
            "         ...,\n",
            "         [ 3.6219e-03,  8.9076e-02, -1.2481e-01,  ...,  7.1735e-02,\n",
            "          -1.6104e-01, -8.6810e-02],\n",
            "         [-1.5020e-01,  5.8935e-02, -2.0758e-01,  ...,  5.2312e-02,\n",
            "          -1.5280e-01, -1.6795e-01],\n",
            "         [-1.3229e-02,  4.6167e-02, -2.9990e-01,  ..., -4.2117e-02,\n",
            "          -1.3086e-01, -1.0707e-01]],\n",
            "\n",
            "        [[ 4.0629e-03,  5.5973e-02, -3.4495e-02,  ...,  3.2340e-02,\n",
            "          -2.0343e-01, -4.9816e-02],\n",
            "         [ 2.0309e-02,  6.3169e-02, -3.6706e-02,  ..., -4.4517e-02,\n",
            "          -1.9397e-01, -4.1664e-02],\n",
            "         [ 2.4605e-02,  8.0008e-02, -4.0654e-02,  ..., -7.7149e-02,\n",
            "          -1.5748e-01, -3.8015e-02],\n",
            "         ...,\n",
            "         [ 5.5270e-02,  5.2540e-02, -2.7025e-02,  ..., -1.1650e-01,\n",
            "          -1.5449e-01, -1.4142e-02],\n",
            "         [ 8.0392e-02,  9.9425e-02, -7.1002e-03,  ..., -1.0107e-01,\n",
            "          -1.2390e-01,  5.8031e-03],\n",
            "         [ 3.8889e-02,  9.6382e-02, -2.9056e-03,  ..., -1.0322e-01,\n",
            "          -1.8727e-01, -4.9345e-02]],\n",
            "\n",
            "        [[ 1.2071e-02,  1.4369e-01, -5.0042e-02,  ..., -1.0363e-01,\n",
            "          -9.1696e-02, -5.9951e-02],\n",
            "         [ 4.9152e-03,  1.3983e-01, -7.1597e-02,  ..., -9.6762e-02,\n",
            "          -6.9235e-02, -5.8275e-02],\n",
            "         [ 1.4460e-04,  1.4048e-01, -8.1384e-02,  ..., -8.0103e-02,\n",
            "          -6.0237e-02, -5.8554e-02],\n",
            "         ...,\n",
            "         [ 3.4286e-04, -7.5086e-01, -2.1889e-02,  ..., -3.3084e-01,\n",
            "           9.9162e-02,  2.6739e-01],\n",
            "         [ 2.5336e-02, -7.0192e-01, -1.2907e-02,  ..., -3.3475e-01,\n",
            "           7.7378e-02,  1.6261e-01],\n",
            "         [ 8.7072e-02, -7.0071e-01, -1.7170e-02,  ..., -4.3524e-01,\n",
            "           2.7302e-02,  1.3229e-01]]], device='cuda:0',\n",
            "       grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 2.4306e-02,  8.0520e-02,  1.0062e-02,  ..., -1.0852e-01,\n",
            "          -2.2702e-02,  1.1820e-03],\n",
            "         [ 1.3223e-02,  1.0644e-01, -2.1217e-02,  ..., -9.9515e-02,\n",
            "          -4.6105e-02, -3.2258e-02],\n",
            "         [ 7.0413e-03,  1.2360e-01, -4.7624e-02,  ..., -8.1124e-02,\n",
            "          -5.4953e-02, -5.2841e-02],\n",
            "         ...,\n",
            "         [ 4.6514e-02,  1.1079e-01,  4.3691e-02,  ...,  2.0133e-01,\n",
            "          -3.6416e-01, -7.0351e-02],\n",
            "         [ 5.4956e-02, -1.2468e-02,  2.9925e-02,  ...,  3.4507e-02,\n",
            "          -2.5805e-01, -5.7319e-02],\n",
            "         [ 7.9262e-02, -4.4203e-02,  1.8881e-02,  ..., -1.4835e-01,\n",
            "          -2.4091e-01, -1.8060e-02]],\n",
            "\n",
            "        [[-4.9777e-02,  6.5606e-02,  1.2399e-02,  ...,  5.4012e-02,\n",
            "          -2.2962e-01, -7.6752e-02],\n",
            "         [-2.0737e-03,  4.8348e-02, -1.5445e-02,  ..., -4.2639e-02,\n",
            "          -2.2399e-01, -4.6681e-02],\n",
            "         [ 1.3763e-02,  5.6366e-02, -2.3618e-02,  ..., -8.7024e-02,\n",
            "          -1.8338e-01, -3.4913e-02],\n",
            "         ...,\n",
            "         [-9.6459e-03, -8.2579e-02, -1.5914e-01,  ..., -1.1187e-01,\n",
            "          -4.1377e-02, -1.1439e-01],\n",
            "         [ 5.9088e-02, -2.7694e-01, -1.0266e-01,  ..., -3.0353e-01,\n",
            "           2.1156e-02,  1.7665e-02],\n",
            "         [ 9.2824e-02, -4.2630e-01, -3.7101e-02,  ..., -2.3505e-01,\n",
            "           7.9637e-02,  4.4869e-02]],\n",
            "\n",
            "        [[ 4.7044e-02,  9.2631e-02, -5.4880e-02,  ..., -6.5923e-02,\n",
            "          -1.7850e-01, -4.7345e-02],\n",
            "         [ 3.9895e-02,  1.0483e-01, -6.5868e-02,  ..., -8.6322e-02,\n",
            "          -1.2752e-01, -4.8711e-02],\n",
            "         [ 2.4936e-02,  1.1720e-01, -7.4179e-02,  ..., -8.1021e-02,\n",
            "          -9.1742e-02, -4.9682e-02],\n",
            "         ...,\n",
            "         [ 5.6183e-03,  1.3523e-01, -9.6390e-02,  ..., -6.1194e-02,\n",
            "          -6.2959e-02, -5.8787e-02],\n",
            "         [ 5.6183e-03,  1.3523e-01, -9.6390e-02,  ..., -6.1194e-02,\n",
            "          -6.2959e-02, -5.8787e-02],\n",
            "         [-1.0669e-01,  1.8185e-01, -2.0904e-01,  ..., -4.3047e-02,\n",
            "           7.2372e-02, -1.3632e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.6832e-02,  1.4336e-01, -1.2082e-01,  ..., -3.7268e-02,\n",
            "          -1.4017e-01, -7.8253e-02],\n",
            "         [ 1.9818e-02,  1.4759e-01, -1.0409e-01,  ..., -5.4239e-02,\n",
            "          -1.1229e-01, -6.8416e-02],\n",
            "         [ 1.7671e-02,  1.4826e-01, -9.5151e-02,  ..., -5.9004e-02,\n",
            "          -9.0810e-02, -6.1550e-02],\n",
            "         ...,\n",
            "         [-7.6008e-02, -5.5414e-03, -6.7683e-02,  ..., -4.7083e-02,\n",
            "           6.4382e-02, -1.1105e-01],\n",
            "         [-1.6254e-02,  1.7384e-02, -5.5668e-02,  ..., -1.7563e-02,\n",
            "           3.5756e-02, -5.4069e-02],\n",
            "         [-1.0672e-02,  5.0024e-02, -5.4338e-02,  ...,  1.5312e-03,\n",
            "          -4.6528e-02, -8.8508e-02]],\n",
            "\n",
            "        [[ 4.9758e-02,  9.2865e-02, -2.4357e-02,  ..., -1.1700e-01,\n",
            "          -1.4190e-01, -5.0436e-02],\n",
            "         [ 3.4864e-02,  1.0438e-01, -4.5760e-02,  ..., -1.0553e-01,\n",
            "          -9.7577e-02, -5.6988e-02],\n",
            "         [ 2.2446e-02,  1.1379e-01, -6.0944e-02,  ..., -8.9067e-02,\n",
            "          -7.9832e-02, -5.6993e-02],\n",
            "         ...,\n",
            "         [ 5.6183e-03,  1.3523e-01, -9.6390e-02,  ..., -6.1194e-02,\n",
            "          -6.2959e-02, -5.8787e-02],\n",
            "         [ 5.6183e-03,  1.3523e-01, -9.6390e-02,  ..., -6.1194e-02,\n",
            "          -6.2959e-02, -5.8787e-02],\n",
            "         [-6.7229e-02,  1.1773e-01, -1.9224e-01,  ..., -4.0941e-02,\n",
            "          -3.9179e-03, -1.5845e-01]],\n",
            "\n",
            "        [[ 2.2526e-02, -5.3272e-01, -1.7555e-02,  ..., -2.2162e-01,\n",
            "          -6.7596e-02,  1.0697e-01],\n",
            "         [ 1.0773e-02, -2.2408e-01, -2.9130e-02,  ..., -1.6146e-01,\n",
            "          -7.1654e-02,  2.6729e-02],\n",
            "         [ 1.0133e-02, -2.4233e-02, -4.3360e-02,  ..., -1.0506e-01,\n",
            "          -7.3537e-02, -2.8129e-02],\n",
            "         ...,\n",
            "         [-6.5235e-02,  7.7057e-02, -9.0202e-02,  ..., -1.2843e-01,\n",
            "          -3.8567e-02, -8.7846e-02],\n",
            "         [ 6.1564e-02,  1.8774e-01,  1.2609e-02,  ..., -2.0455e-01,\n",
            "          -1.8111e-01, -5.3028e-04],\n",
            "         [ 7.0698e-02,  1.3866e-01,  2.6361e-02,  ..., -1.0717e-01,\n",
            "          -2.2236e-01, -5.0368e-02]]], device='cuda:0',\n",
            "       grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 0.0554, -0.0153, -0.0013,  ..., -0.1633, -0.1389, -0.0215],\n",
            "         [ 0.0406,  0.0335, -0.0163,  ..., -0.1487, -0.0970, -0.0287],\n",
            "         [ 0.0305,  0.0666, -0.0333,  ..., -0.1232, -0.0841, -0.0352],\n",
            "         ...,\n",
            "         [-0.0713,  0.1292, -0.1074,  ..., -0.0149,  0.0060, -0.1001],\n",
            "         [ 0.0320,  0.1664, -0.0500,  ..., -0.0473, -0.0999, -0.0308],\n",
            "         [ 0.0959,  0.0801, -0.0654,  ..., -0.0391, -0.2066, -0.0429]],\n",
            "\n",
            "        [[ 0.0126, -0.2178, -0.0274,  ..., -0.1473, -0.0361, -0.0126],\n",
            "         [ 0.0007, -0.0314, -0.0443,  ..., -0.1140, -0.0600, -0.0461],\n",
            "         [ 0.0018,  0.0638, -0.0602,  ..., -0.0823, -0.0684, -0.0645],\n",
            "         ...,\n",
            "         [ 0.0729,  0.1788, -0.1988,  ..., -0.1705,  0.0714, -0.0551],\n",
            "         [ 0.0585,  0.1180, -0.2390,  ..., -0.1375,  0.0625, -0.0279],\n",
            "         [ 0.0351,  0.1141, -0.2060,  ..., -0.1411, -0.0542, -0.0507]],\n",
            "\n",
            "        [[ 0.0134,  0.1798, -0.0848,  ..., -0.0213, -0.0756, -0.0673],\n",
            "         [ 0.0218,  0.1525, -0.0997,  ..., -0.0482, -0.0756, -0.0667],\n",
            "         [ 0.0135,  0.1486, -0.0997,  ..., -0.0551, -0.0648, -0.0657],\n",
            "         ...,\n",
            "         [-0.1033,  0.0265, -0.0571,  ..., -0.1175,  0.0094, -0.1893],\n",
            "         [-0.0666, -0.0211, -0.1739,  ..., -0.1702,  0.1756, -0.1440],\n",
            "         [-0.0045, -0.2930, -0.1210,  ..., -0.1274,  0.1519, -0.0925]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0190,  0.0945, -0.0466,  ..., -0.0240, -0.0935, -0.0611],\n",
            "         [ 0.0220,  0.1140, -0.0608,  ..., -0.0477, -0.0880, -0.0567],\n",
            "         [ 0.0190,  0.1247, -0.0711,  ..., -0.0587, -0.0779, -0.0555],\n",
            "         ...,\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588]],\n",
            "\n",
            "        [[-0.0029,  0.1301, -0.0987,  ..., -0.0394, -0.0758, -0.0862],\n",
            "         [ 0.0037,  0.1265, -0.0979,  ..., -0.0601, -0.0746, -0.0737],\n",
            "         [ 0.0044,  0.1286, -0.0941,  ..., -0.0650, -0.0699, -0.0650],\n",
            "         ...,\n",
            "         [-0.0290,  0.0988, -0.1411,  ...,  0.0077,  0.0263, -0.0796],\n",
            "         [ 0.0405,  0.1411, -0.1120,  ..., -0.0460, -0.0287, -0.0234],\n",
            "         [ 0.0022,  0.1328, -0.0689,  ..., -0.0475,  0.0028, -0.1229]],\n",
            "\n",
            "        [[ 0.0624,  0.0610, -0.0091,  ..., -0.1107, -0.1447, -0.0650],\n",
            "         [ 0.0366,  0.0754, -0.0336,  ..., -0.1016, -0.0963, -0.0744],\n",
            "         [ 0.0255,  0.0907, -0.0477,  ..., -0.0914, -0.0861, -0.0657],\n",
            "         ...,\n",
            "         [-0.0447,  0.0526, -0.0870,  ...,  0.0227, -0.1229, -0.1011],\n",
            "         [-0.0925,  0.0376, -0.2026,  ...,  0.0291,  0.0018, -0.1008],\n",
            "         [-0.1518,  0.0650, -0.1415,  ...,  0.0909, -0.0542, -0.0951]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 0.0546,  0.0917, -0.0808,  ..., -0.0861, -0.1061, -0.0699],\n",
            "         [ 0.0287,  0.1136, -0.0770,  ..., -0.0804, -0.0872, -0.0696],\n",
            "         [ 0.0210,  0.1206, -0.0776,  ..., -0.0763, -0.0841, -0.0627],\n",
            "         ...,\n",
            "         [-0.0405,  0.1623, -0.0612,  ...,  0.0328, -0.0211, -0.1749],\n",
            "         [ 0.0959,  0.0998, -0.0978,  ..., -0.0470, -0.0075, -0.1536],\n",
            "         [ 0.0797,  0.1904, -0.1918,  ...,  0.0161,  0.0677, -0.1763]],\n",
            "\n",
            "        [[ 0.0333,  0.1553, -0.1539,  ..., -0.0959, -0.0911, -0.0653],\n",
            "         [ 0.0235,  0.1665, -0.1230,  ..., -0.0753, -0.0829, -0.0749],\n",
            "         [ 0.0168,  0.1606, -0.1077,  ..., -0.0629, -0.0770, -0.0778],\n",
            "         ...,\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [-0.0637,  0.0975, -0.1339,  ..., -0.0214, -0.0327, -0.0878]],\n",
            "\n",
            "        [[ 0.0063, -0.1604, -0.0516,  ..., -0.0621, -0.0732, -0.0616],\n",
            "         [ 0.0177, -0.0192, -0.0629,  ..., -0.0737, -0.0883, -0.0682],\n",
            "         [ 0.0190,  0.0709, -0.0696,  ..., -0.0677, -0.0829, -0.0757],\n",
            "         ...,\n",
            "         [ 0.0563,  0.1645, -0.1311,  ...,  0.0173, -0.0772, -0.1899],\n",
            "         [ 0.0054,  0.1976, -0.0885,  ...,  0.0815, -0.0978, -0.2703],\n",
            "         [ 0.0099,  0.1676, -0.0560,  ...,  0.1156, -0.1899, -0.1565]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         ...,\n",
            "         [-0.1281,  0.1805, -0.2537,  ...,  0.0010,  0.0844, -0.1384],\n",
            "         [-0.2230,  0.2090, -0.3193,  ...,  0.0091,  0.0782, -0.1019],\n",
            "         [-0.1084,  0.2138, -0.3619,  ...,  0.0049,  0.0737, -0.0818]],\n",
            "\n",
            "        [[ 0.0271,  0.1569, -0.0734,  ..., -0.0497, -0.0460, -0.0803],\n",
            "         [ 0.0258,  0.1549, -0.0852,  ..., -0.0565, -0.0577, -0.0721],\n",
            "         [ 0.0175,  0.1499, -0.0932,  ..., -0.0582, -0.0596, -0.0683],\n",
            "         ...,\n",
            "         [-0.0203,  0.0072, -0.1873,  ..., -0.0932,  0.0983,  0.0011],\n",
            "         [ 0.0070, -0.1200, -0.2246,  ..., -0.1289,  0.1529, -0.0643],\n",
            "         [ 0.0222, -0.2096, -0.1055,  ..., -0.0739,  0.1309, -0.0968]],\n",
            "\n",
            "        [[-0.0313,  0.0864, -0.1114,  ...,  0.0410, -0.1190, -0.0652],\n",
            "         [ 0.0036,  0.1046, -0.0985,  ..., -0.0162, -0.1132, -0.0625],\n",
            "         [ 0.0119,  0.1183, -0.0918,  ..., -0.0461, -0.0932, -0.0613],\n",
            "         ...,\n",
            "         [ 0.0554, -0.3689, -0.1012,  ..., -0.2603,  0.0237, -0.1221],\n",
            "         [ 0.0455, -0.4145, -0.0889,  ..., -0.3090,  0.0043, -0.0328],\n",
            "         [ 0.0458, -0.6085, -0.0477,  ..., -0.3470,  0.0650,  0.0572]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 0.0769,  0.1694, -0.0868,  ...,  0.0016, -0.0820, -0.0924],\n",
            "         [ 0.0566,  0.1442, -0.1013,  ..., -0.0392, -0.0793, -0.0759],\n",
            "         [ 0.0327,  0.1412, -0.1000,  ..., -0.0532, -0.0714, -0.0667],\n",
            "         ...,\n",
            "         [-0.1412,  0.0313,  0.0108,  ...,  0.0719, -0.4107, -0.0809],\n",
            "         [-0.1227,  0.0358,  0.0167,  ...,  0.0722, -0.4112, -0.0800],\n",
            "         [-0.1028,  0.0247,  0.0169,  ...,  0.0872, -0.4073, -0.0970]],\n",
            "\n",
            "        [[ 0.0166,  0.0919, -0.0899,  ..., -0.0373, -0.0859, -0.0683],\n",
            "         [ 0.0217,  0.1115, -0.0860,  ..., -0.0602, -0.0808, -0.0694],\n",
            "         [ 0.0157,  0.1234, -0.0864,  ..., -0.0653, -0.0711, -0.0660],\n",
            "         ...,\n",
            "         [ 0.0158,  0.0575, -0.1252,  ..., -0.1014,  0.0508,  0.0077],\n",
            "         [ 0.0049,  0.1250, -0.0476,  ..., -0.0509,  0.0225, -0.0450],\n",
            "         [ 0.0393,  0.1599, -0.0306,  ..., -0.0446, -0.0012, -0.0301]],\n",
            "\n",
            "        [[ 0.0475,  0.0763, -0.0743,  ..., -0.0091, -0.1756, -0.0655],\n",
            "         [ 0.0422,  0.0900, -0.0614,  ..., -0.0656, -0.1464, -0.0566],\n",
            "         [ 0.0350,  0.1009, -0.0600,  ..., -0.0814, -0.1166, -0.0483],\n",
            "         ...,\n",
            "         [ 0.0103,  0.1079, -0.1276,  ..., -0.1291, -0.0012, -0.2229],\n",
            "         [-0.0566,  0.1559, -0.0974,  ..., -0.0755, -0.0039, -0.0864],\n",
            "         [-0.0055,  0.0643, -0.1142,  ..., -0.1373,  0.0244, -0.0260]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0019,  0.3094, -0.1577,  ...,  0.0141, -0.1276, -0.0882],\n",
            "         [ 0.0368,  0.2614, -0.1226,  ...,  0.0065, -0.1332, -0.0879],\n",
            "         [ 0.0461,  0.2157, -0.0986,  ..., -0.0122, -0.1215, -0.0795],\n",
            "         ...,\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [-0.0168,  0.1182, -0.1349,  ..., -0.0387, -0.1155, -0.1170]],\n",
            "\n",
            "        [[ 0.0287, -0.1000, -0.0827,  ..., -0.0593, -0.0290, -0.0767],\n",
            "         [ 0.0228,  0.0349, -0.0760,  ..., -0.0575, -0.0805, -0.0766],\n",
            "         [ 0.0181,  0.1018, -0.0762,  ..., -0.0568, -0.0824, -0.0789],\n",
            "         ...,\n",
            "         [-0.0011,  0.1904, -0.3285,  ..., -0.1312,  0.0743, -0.1571],\n",
            "         [ 0.0185,  0.3147, -0.2660,  ..., -0.0891, -0.0746, -0.0966],\n",
            "         [ 0.0226,  0.3333, -0.1406,  ..., -0.0823, -0.1040, -0.0801]],\n",
            "\n",
            "        [[ 0.0044, -0.5328, -0.0122,  ..., -0.2401, -0.0731,  0.0906],\n",
            "         [-0.0042, -0.3050, -0.0055,  ..., -0.1778, -0.0772,  0.0538],\n",
            "         [-0.0048, -0.0938, -0.0177,  ..., -0.1176, -0.0726, -0.0024],\n",
            "         ...,\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [-0.1031,  0.1506, -0.1233,  ..., -0.0164, -0.0170, -0.1228]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[-0.0328,  0.0118, -0.0012,  ...,  0.0372, -0.3135, -0.0550],\n",
            "         [-0.0052,  0.0007, -0.0093,  ..., -0.0466, -0.3372, -0.0476],\n",
            "         [ 0.0088,  0.0043, -0.0134,  ..., -0.0965, -0.3247, -0.0364],\n",
            "         ...,\n",
            "         [-0.0191,  0.1903, -0.1526,  ...,  0.0489, -0.0585, -0.0825],\n",
            "         [ 0.0177,  0.1884, -0.1755,  ...,  0.0518, -0.0413, -0.1221],\n",
            "         [-0.0343,  0.2134, -0.0771,  ...,  0.0611, -0.0587, -0.1057]],\n",
            "\n",
            "        [[ 0.0355,  0.1561, -0.0486,  ..., -0.0628, -0.0383, -0.0391],\n",
            "         [ 0.0169,  0.1471, -0.0702,  ..., -0.0654, -0.0475, -0.0548],\n",
            "         [ 0.0108,  0.1428, -0.0808,  ..., -0.0632, -0.0569, -0.0604],\n",
            "         ...,\n",
            "         [-0.1370,  0.0083,  0.0127,  ...,  0.0400, -0.1753, -0.2237],\n",
            "         [ 0.0074, -0.0110,  0.0132,  ..., -0.0088, -0.1862, -0.1187],\n",
            "         [ 0.0047,  0.0406,  0.0169,  ...,  0.0021, -0.1859, -0.1343]],\n",
            "\n",
            "        [[ 0.0135,  0.1224, -0.0744,  ..., -0.1098, -0.0457, -0.0259],\n",
            "         [ 0.0115,  0.1308, -0.0806,  ..., -0.0920, -0.0562, -0.0402],\n",
            "         [ 0.0083,  0.1369, -0.0869,  ..., -0.0742, -0.0602, -0.0521],\n",
            "         ...,\n",
            "         [ 0.0363,  0.0253,  0.0119,  ..., -0.0394, -0.2919, -0.2734],\n",
            "         [-0.0141,  0.0083, -0.0347,  ..., -0.1051, -0.1189, -0.2887],\n",
            "         [-0.0629,  0.0836, -0.0300,  ..., -0.0620, -0.0400, -0.2939]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0098,  0.1051, -0.0995,  ..., -0.0600, -0.1046, -0.0711],\n",
            "         [ 0.0086,  0.1191, -0.0920,  ..., -0.0677, -0.0826, -0.0672],\n",
            "         [ 0.0080,  0.1261, -0.0893,  ..., -0.0669, -0.0715, -0.0626],\n",
            "         ...,\n",
            "         [ 0.0380,  0.1109, -0.1698,  ..., -0.0981,  0.0101, -0.0920],\n",
            "         [ 0.0102,  0.0666, -0.2176,  ..., -0.1122,  0.0544, -0.1275],\n",
            "         [ 0.0163,  0.0648, -0.2189,  ..., -0.0833,  0.0747, -0.1317]],\n",
            "\n",
            "        [[ 0.0342,  0.2377, -0.1201,  ..., -0.0497, -0.0905, -0.0586],\n",
            "         [ 0.0254,  0.2046, -0.0986,  ..., -0.0420, -0.0774, -0.0710],\n",
            "         [ 0.0187,  0.1778, -0.0919,  ..., -0.0415, -0.0723, -0.0776],\n",
            "         ...,\n",
            "         [-0.0365, -0.0935, -0.1294,  ..., -0.0680,  0.0245, -0.0747],\n",
            "         [-0.0546, -0.0072, -0.1136,  ..., -0.0428, -0.0213, -0.1149],\n",
            "         [-0.0034,  0.0965, -0.1242,  ..., -0.0024, -0.1350, -0.1196]],\n",
            "\n",
            "        [[ 0.0208,  0.1030, -0.0857,  ..., -0.0408, -0.0893, -0.0619],\n",
            "         [ 0.0214,  0.1176, -0.0894,  ..., -0.0624, -0.0801, -0.0675],\n",
            "         [ 0.0142,  0.1273, -0.0900,  ..., -0.0639, -0.0693, -0.0641],\n",
            "         ...,\n",
            "         [-0.1702,  0.1765,  0.0074,  ...,  0.0077,  0.0560, -0.1710],\n",
            "         [-0.0761,  0.1109,  0.0372,  ...,  0.0075, -0.0209, -0.0738],\n",
            "         [-0.0025,  0.0684,  0.0013,  ...,  0.0071, -0.0385, -0.0785]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 3.1327e-02,  1.5951e-01, -6.8825e-02,  ...,  1.1188e-02,\n",
            "          -1.0583e-01, -7.5215e-02],\n",
            "         [ 3.5857e-02,  1.5442e-01, -7.7122e-02,  ..., -2.8337e-02,\n",
            "          -9.5779e-02, -7.3075e-02],\n",
            "         [ 2.7465e-02,  1.4911e-01, -8.2916e-02,  ..., -4.7219e-02,\n",
            "          -8.0506e-02, -6.8811e-02],\n",
            "         ...,\n",
            "         [ 2.5769e-02,  1.6888e-01, -3.1157e-02,  ..., -2.7747e-03,\n",
            "          -1.8781e-01, -1.2591e-01],\n",
            "         [ 4.4680e-02,  1.7211e-01, -2.1694e-02,  ..., -2.2983e-02,\n",
            "          -1.7052e-01, -8.1948e-02],\n",
            "         [ 6.5522e-02,  1.4857e-01, -6.2215e-02,  ..., -1.1697e-01,\n",
            "          -2.0539e-01, -1.4868e-02]],\n",
            "\n",
            "        [[ 3.4970e-02,  5.9113e-02, -1.3375e-02,  ..., -6.7995e-02,\n",
            "          -1.4576e-01, -8.0165e-02],\n",
            "         [ 3.7592e-02,  8.2082e-02, -3.1222e-02,  ..., -9.3299e-02,\n",
            "          -1.1629e-01, -6.2436e-02],\n",
            "         [ 3.2035e-02,  9.5351e-02, -4.7204e-02,  ..., -9.7011e-02,\n",
            "          -9.8493e-02, -5.4612e-02],\n",
            "         ...,\n",
            "         [ 2.2357e-02,  1.1570e-01, -6.8337e-02,  ..., -3.9013e-02,\n",
            "          -7.2053e-02, -1.5298e-01],\n",
            "         [-4.8229e-04,  1.2072e-01, -1.1895e-01,  ..., -6.7113e-02,\n",
            "          -7.6470e-02, -1.2570e-01],\n",
            "         [-4.8605e-02,  1.2929e-01, -7.1368e-02,  ..., -3.4541e-02,\n",
            "           8.7382e-03, -1.4813e-01]],\n",
            "\n",
            "        [[ 3.6287e-02,  9.7132e-02, -3.1056e-02,  ..., -7.1500e-02,\n",
            "          -1.2054e-01, -1.1693e-01],\n",
            "         [ 4.0839e-02,  1.1346e-01, -5.3663e-02,  ..., -9.1027e-02,\n",
            "          -1.0262e-01, -8.7671e-02],\n",
            "         [ 2.7875e-02,  1.2530e-01, -6.8909e-02,  ..., -8.7138e-02,\n",
            "          -8.2377e-02, -7.3215e-02],\n",
            "         ...,\n",
            "         [-1.6918e-01,  1.3275e-01, -1.8925e-02,  ...,  5.9161e-02,\n",
            "          -1.1240e-01, -1.1526e-01],\n",
            "         [-6.7573e-02,  6.1151e-02, -2.7275e-02,  ...,  6.8415e-02,\n",
            "          -3.2317e-01, -5.6134e-02],\n",
            "         [-5.5225e-02,  5.1154e-02, -8.7131e-02,  ...,  2.7209e-02,\n",
            "          -3.0349e-01, -5.5905e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.2374e-02,  1.6375e-01, -1.3629e-01,  ..., -3.2968e-02,\n",
            "          -8.5973e-02, -1.2089e-01],\n",
            "         [ 2.1240e-02,  1.6097e-01, -1.2659e-01,  ..., -3.8975e-02,\n",
            "          -9.1743e-02, -1.0451e-01],\n",
            "         [ 2.0400e-02,  1.6062e-01, -1.1070e-01,  ..., -4.4200e-02,\n",
            "          -8.6223e-02, -9.2610e-02],\n",
            "         ...,\n",
            "         [-3.0275e-02,  1.4037e-01, -1.2795e-01,  ..., -8.0568e-02,\n",
            "          -2.7881e-01, -8.7333e-02],\n",
            "         [ 1.7789e-03,  8.9090e-02, -6.7485e-02,  ..., -5.3312e-02,\n",
            "          -2.6850e-01, -1.2820e-01],\n",
            "         [-1.9854e-02,  9.2639e-02, -6.1175e-02,  ..., -8.3409e-02,\n",
            "          -3.7022e-01, -1.3948e-01]],\n",
            "\n",
            "        [[ 2.3564e-02,  1.3006e-01, -1.0103e-01,  ..., -2.9867e-02,\n",
            "          -1.1759e-01, -8.9221e-02],\n",
            "         [ 2.1459e-02,  1.4420e-01, -9.5987e-02,  ..., -4.8794e-02,\n",
            "          -9.4364e-02, -7.9685e-02],\n",
            "         [ 1.8277e-02,  1.4369e-01, -9.3468e-02,  ..., -5.7832e-02,\n",
            "          -8.0721e-02, -7.1182e-02],\n",
            "         ...,\n",
            "         [-1.6231e-02, -1.0578e-01, -3.0472e-02,  ..., -6.5934e-02,\n",
            "           1.9742e-02, -3.8301e-02],\n",
            "         [-6.4512e-02, -1.7097e-01,  1.5598e-02,  ...,  7.5106e-04,\n",
            "           4.4287e-02, -1.1668e-01],\n",
            "         [-1.5195e-01, -1.6181e-01, -4.4625e-02,  ..., -8.9245e-03,\n",
            "           1.0415e-01, -6.5057e-02]],\n",
            "\n",
            "        [[ 2.8622e-02,  8.1884e-02, -2.6026e-02,  ..., -3.2702e-02,\n",
            "          -7.2223e-02, -7.0426e-02],\n",
            "         [ 3.3341e-02,  9.8268e-02, -4.5152e-02,  ..., -5.9920e-02,\n",
            "          -7.6206e-02, -6.7430e-02],\n",
            "         [ 2.7918e-02,  1.1157e-01, -5.9719e-02,  ..., -6.9771e-02,\n",
            "          -7.5296e-02, -6.2726e-02],\n",
            "         ...,\n",
            "         [-4.8897e-02, -6.1313e-01, -9.4134e-02,  ..., -1.6155e-01,\n",
            "           8.3657e-02, -1.7237e-02],\n",
            "         [-1.8074e-02, -5.6017e-01, -1.0931e-01,  ..., -9.7323e-02,\n",
            "           7.2329e-02, -2.4469e-02],\n",
            "         [ 4.7572e-03, -4.6377e-01, -9.4720e-02,  ..., -9.5470e-02,\n",
            "           3.8879e-02, -8.4273e-02]]], device='cuda:0',\n",
            "       grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 0.0830,  0.1617, -0.0552,  ..., -0.1375, -0.1204, -0.0198],\n",
            "         [ 0.0462,  0.1573, -0.0694,  ..., -0.1163, -0.0717, -0.0397],\n",
            "         [ 0.0219,  0.1503, -0.0780,  ..., -0.0883, -0.0624, -0.0493],\n",
            "         ...,\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [-0.1117,  0.1855,  0.0081,  ...,  0.0171, -0.1134, -0.1532]],\n",
            "\n",
            "        [[ 0.0204,  0.1348, -0.0796,  ..., -0.0360, -0.0598, -0.0909],\n",
            "         [ 0.0303,  0.1381, -0.0867,  ..., -0.0507, -0.0724, -0.0822],\n",
            "         [ 0.0260,  0.1393, -0.0904,  ..., -0.0580, -0.0713, -0.0761],\n",
            "         ...,\n",
            "         [-0.0027, -0.1331,  0.0004,  ..., -0.2394,  0.0433,  0.0805],\n",
            "         [ 0.0244, -0.2034, -0.0023,  ..., -0.1973,  0.0363,  0.0390],\n",
            "         [ 0.0562, -0.1601,  0.0087,  ..., -0.1645,  0.0210, -0.0141]],\n",
            "\n",
            "        [[ 0.0061,  0.0598, -0.0515,  ..., -0.0395, -0.2087, -0.0384],\n",
            "         [ 0.0225,  0.0712, -0.0489,  ..., -0.0807, -0.1752, -0.0370],\n",
            "         [ 0.0243,  0.0861, -0.0517,  ..., -0.0932, -0.1345, -0.0354],\n",
            "         ...,\n",
            "         [-0.0625,  0.0653, -0.2107,  ...,  0.0181, -0.0285, -0.0944],\n",
            "         [ 0.0047,  0.1077, -0.1013,  ...,  0.0418, -0.1499, -0.0806],\n",
            "         [ 0.0443,  0.0687, -0.0612,  ...,  0.0381, -0.1989, -0.0957]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0087,  0.0965, -0.0969,  ..., -0.1096, -0.2225, -0.1229],\n",
            "         [ 0.0021,  0.1038, -0.0875,  ..., -0.1015, -0.1576, -0.0970],\n",
            "         [ 0.0100,  0.1081, -0.0794,  ..., -0.0952, -0.1210, -0.0752],\n",
            "         ...,\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [-0.0429,  0.0404, -0.1204,  ..., -0.1297,  0.0473, -0.0863]],\n",
            "\n",
            "        [[-0.0437, -0.0743, -0.0469,  ..., -0.0223, -0.0233, -0.0746],\n",
            "         [-0.0089,  0.0171, -0.0652,  ..., -0.0435, -0.0656, -0.0726],\n",
            "         [ 0.0049,  0.0736, -0.0757,  ..., -0.0560, -0.0733, -0.0660],\n",
            "         ...,\n",
            "         [ 0.0211,  0.0706, -0.0767,  ...,  0.0410, -0.3346, -0.0897],\n",
            "         [ 0.0049,  0.1001, -0.0482,  ...,  0.0492, -0.3710, -0.0705],\n",
            "         [ 0.0095,  0.0607, -0.0586,  ...,  0.0820, -0.3307, -0.0750]],\n",
            "\n",
            "        [[ 0.0163, -0.2645, -0.0780,  ..., -0.0427, -0.0755, -0.0727],\n",
            "         [ 0.0256, -0.0558, -0.0803,  ..., -0.0457, -0.0883, -0.0807],\n",
            "         [ 0.0251,  0.0593, -0.0813,  ..., -0.0471, -0.0872, -0.0833],\n",
            "         ...,\n",
            "         [ 0.0870,  0.1344,  0.0369,  ..., -0.0634, -0.2585, -0.0010],\n",
            "         [ 0.0846,  0.0847,  0.0035,  ..., -0.1530, -0.1561, -0.0137],\n",
            "         [ 0.0414,  0.0324, -0.0175,  ..., -0.2272, -0.0430, -0.0208]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[-1.7131e-02,  1.0316e-01, -4.1848e-02,  ..., -3.8751e-02,\n",
            "          -1.2222e-01, -8.5053e-02],\n",
            "         [ 6.5311e-03,  1.0040e-01, -6.0393e-02,  ..., -7.2883e-02,\n",
            "          -1.0436e-01, -7.0292e-02],\n",
            "         [ 1.1179e-02,  1.0814e-01, -6.7429e-02,  ..., -8.0477e-02,\n",
            "          -8.7550e-02, -5.9749e-02],\n",
            "         ...,\n",
            "         [ 1.3013e-02,  8.5579e-02, -2.6543e-01,  ..., -1.1034e-01,\n",
            "           9.1035e-02, -1.2908e-01],\n",
            "         [ 2.2453e-02,  7.4855e-02, -1.9514e-01,  ..., -9.2025e-02,\n",
            "          -1.0743e-01, -1.0201e-01],\n",
            "         [-3.2596e-02,  1.2108e-01, -2.9823e-01,  ..., -9.1246e-02,\n",
            "           1.7723e-04, -1.6983e-01]],\n",
            "\n",
            "        [[ 3.3385e-02,  7.7558e-03, -1.3288e-02,  ..., -1.2099e-01,\n",
            "          -5.9545e-02, -3.6507e-02],\n",
            "         [ 1.9914e-02,  8.8988e-02, -4.0299e-02,  ..., -9.7112e-02,\n",
            "          -7.2867e-02, -5.5398e-02],\n",
            "         [ 1.4600e-02,  1.1915e-01, -6.2245e-02,  ..., -7.8869e-02,\n",
            "          -7.3863e-02, -6.4599e-02],\n",
            "         ...,\n",
            "         [ 2.9945e-02,  2.0285e-01,  6.3501e-03,  ...,  8.3555e-03,\n",
            "          -1.2632e-01, -1.1450e-01],\n",
            "         [-7.8558e-03,  1.3310e-01, -9.0629e-02,  ...,  7.3300e-02,\n",
            "          -9.3474e-02, -1.5766e-01],\n",
            "         [-2.4492e-01,  1.9567e-01,  5.6366e-02,  ...,  1.3320e-01,\n",
            "          -1.6994e-01, -3.1580e-01]],\n",
            "\n",
            "        [[ 4.4456e-02,  8.6963e-02, -8.9524e-02,  ..., -2.4945e-02,\n",
            "          -1.4494e-01, -7.9094e-02],\n",
            "         [ 3.3610e-02,  1.1878e-01, -8.7301e-02,  ..., -4.7700e-02,\n",
            "          -1.0801e-01, -7.6120e-02],\n",
            "         [ 2.4982e-02,  1.3046e-01, -8.9312e-02,  ..., -5.7093e-02,\n",
            "          -8.6848e-02, -7.1141e-02],\n",
            "         ...,\n",
            "         [-8.2469e-02, -4.4394e-03, -1.4680e-01,  ...,  2.1838e-02,\n",
            "          -1.0066e-01, -3.3070e-02],\n",
            "         [-5.5355e-02,  1.5531e-02, -1.0646e-01,  ...,  1.8288e-02,\n",
            "          -4.7106e-02, -1.6807e-02],\n",
            "         [-1.7984e-01,  3.2626e-02, -9.4979e-02,  ...,  4.2349e-02,\n",
            "          -2.6589e-02, -9.1282e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 2.8049e-03,  1.0829e-01, -6.8881e-02,  ..., -1.0463e-01,\n",
            "          -4.4348e-02, -4.7740e-02],\n",
            "         [-4.8191e-03,  1.2129e-01, -8.2157e-02,  ..., -9.7884e-02,\n",
            "          -4.6447e-02, -4.8605e-02],\n",
            "         [-8.2357e-03,  1.3175e-01, -8.7865e-02,  ..., -7.9464e-02,\n",
            "          -4.9090e-02, -5.3444e-02],\n",
            "         ...,\n",
            "         [-2.2306e-02,  1.2353e-02, -2.1903e-01,  ..., -1.9050e-01,\n",
            "          -1.0823e-01, -6.2806e-02],\n",
            "         [-3.1431e-04,  3.3878e-02, -1.3735e-01,  ..., -1.4153e-01,\n",
            "          -1.1809e-01,  2.7896e-02],\n",
            "         [ 4.2444e-02,  2.5331e-02, -1.9534e-01,  ..., -1.7763e-01,\n",
            "          -7.9366e-02,  2.9662e-02]],\n",
            "\n",
            "        [[ 2.8063e-02,  4.7224e-02, -6.1533e-02,  ..., -2.5737e-02,\n",
            "          -2.2562e-01, -7.4923e-02],\n",
            "         [ 3.1457e-02,  5.9728e-02, -5.2466e-02,  ..., -7.8011e-02,\n",
            "          -1.7360e-01, -6.5722e-02],\n",
            "         [ 2.9439e-02,  7.7732e-02, -5.1877e-02,  ..., -9.3638e-02,\n",
            "          -1.3360e-01, -5.7111e-02],\n",
            "         ...,\n",
            "         [ 3.4773e-02,  7.9667e-02, -3.9481e-02,  ..., -1.2594e-01,\n",
            "          -6.6254e-02,  2.7175e-02],\n",
            "         [-6.2398e-03, -1.4787e-02, -5.8983e-02,  ..., -1.5660e-01,\n",
            "           5.6995e-03,  5.3577e-02],\n",
            "         [-8.8191e-02, -1.0920e-01, -9.5162e-02,  ..., -1.6077e-01,\n",
            "           5.3236e-02,  4.7890e-02]],\n",
            "\n",
            "        [[ 2.2210e-02,  8.8951e-02, -1.3398e-02,  ..., -1.5679e-01,\n",
            "          -4.9494e-02, -2.1958e-02],\n",
            "         [ 1.2423e-02,  1.0200e-01, -3.5841e-02,  ..., -1.2148e-01,\n",
            "          -5.3860e-02, -3.3315e-02],\n",
            "         [ 7.8633e-03,  1.1303e-01, -5.5906e-02,  ..., -9.2630e-02,\n",
            "          -6.1224e-02, -4.3048e-02],\n",
            "         ...,\n",
            "         [ 1.1346e-02,  1.1427e-01, -9.5383e-02,  ..., -5.1398e-02,\n",
            "          -3.2522e-02, -1.5287e-02],\n",
            "         [-2.5807e-03,  9.3768e-02, -1.6258e-01,  ..., -5.3925e-02,\n",
            "           2.4637e-02, -6.1687e-02],\n",
            "         [-5.7342e-02,  1.3946e-01, -5.1501e-02,  ...,  4.4151e-02,\n",
            "          -5.7644e-02, -4.5197e-02]]], device='cuda:0',\n",
            "       grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 0.0301,  0.1972, -0.1540,  ..., -0.0470, -0.1211, -0.1017],\n",
            "         [ 0.0331,  0.1879, -0.1356,  ..., -0.0530, -0.1042, -0.0934],\n",
            "         [ 0.0265,  0.1744, -0.1151,  ..., -0.0524, -0.0888, -0.0855],\n",
            "         ...,\n",
            "         [-0.0509, -0.0640, -0.1503,  ..., -0.0481,  0.1148, -0.0446],\n",
            "         [-0.0338, -0.0482, -0.1017,  ..., -0.0700,  0.0926, -0.0413],\n",
            "         [-0.0085, -0.0220, -0.0838,  ..., -0.0540,  0.0694,  0.0010]],\n",
            "\n",
            "        [[ 0.0013,  0.0378, -0.0003,  ...,  0.0188, -0.2310, -0.0625],\n",
            "         [ 0.0189,  0.0315, -0.0252,  ..., -0.0842, -0.2322, -0.0637],\n",
            "         [ 0.0232,  0.0527, -0.0311,  ..., -0.1076, -0.1759, -0.0523],\n",
            "         ...,\n",
            "         [-0.0475,  0.0969, -0.1235,  ..., -0.0042, -0.0467, -0.0730],\n",
            "         [ 0.0036,  0.0667, -0.2447,  ..., -0.0343, -0.1311, -0.0537],\n",
            "         [ 0.0016,  0.0674, -0.1572,  ...,  0.0044, -0.1329, -0.0702]],\n",
            "\n",
            "        [[-0.0105,  0.0533, -0.0599,  ...,  0.0147, -0.1001, -0.0494],\n",
            "         [ 0.0210,  0.0791, -0.0651,  ..., -0.0308, -0.1115, -0.0487],\n",
            "         [ 0.0238,  0.1015, -0.0690,  ..., -0.0540, -0.0968, -0.0486],\n",
            "         ...,\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [-0.0358,  0.1083, -0.0647,  ...,  0.0130, -0.0680, -0.0989]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0294,  0.1149, -0.1267,  ..., -0.1302, -0.0867, -0.0135],\n",
            "         [ 0.0173,  0.1300, -0.1166,  ..., -0.0972, -0.0801, -0.0369],\n",
            "         [ 0.0136,  0.1336, -0.1078,  ..., -0.0769, -0.0777, -0.0472],\n",
            "         ...,\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [-0.0388,  0.1316, -0.1118,  ..., -0.1039, -0.0410, -0.0975],\n",
            "         [ 0.0445,  0.0788, -0.0615,  ..., -0.2052, -0.0839, -0.0100]],\n",
            "\n",
            "        [[-0.0489, -0.0137, -0.0996,  ..., -0.1220, -0.0276,  0.0089],\n",
            "         [-0.0281,  0.0657, -0.0982,  ..., -0.0910, -0.0547, -0.0241],\n",
            "         [-0.0133,  0.1045, -0.0973,  ..., -0.0727, -0.0632, -0.0445],\n",
            "         ...,\n",
            "         [-0.0851,  0.0403, -0.0385,  ..., -0.0353, -0.0109, -0.0910],\n",
            "         [-0.0864, -0.0500, -0.0285,  ..., -0.1079,  0.0356, -0.0737],\n",
            "         [-0.1193,  0.0419,  0.0247,  ..., -0.0374, -0.0468, -0.0057]],\n",
            "\n",
            "        [[ 0.0084,  0.1068, -0.0908,  ..., -0.0083, -0.0596, -0.0511],\n",
            "         [ 0.0161,  0.1247, -0.0904,  ..., -0.0389, -0.0609, -0.0583],\n",
            "         [ 0.0141,  0.1311, -0.0921,  ..., -0.0519, -0.0618, -0.0595],\n",
            "         ...,\n",
            "         [-0.0694,  0.0500, -0.1819,  ..., -0.0534,  0.0133, -0.0566],\n",
            "         [-0.1389,  0.0761, -0.1253,  ..., -0.0229,  0.0299, -0.0880],\n",
            "         [-0.0913,  0.1349, -0.1199,  ..., -0.0114, -0.0585, -0.1097]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 0.0176,  0.0645, -0.0722,  ..., -0.0587, -0.0488, -0.0313],\n",
            "         [ 0.0140,  0.1131, -0.0832,  ..., -0.0624, -0.0628, -0.0587],\n",
            "         [ 0.0120,  0.1347, -0.0885,  ..., -0.0590, -0.0676, -0.0692],\n",
            "         ...,\n",
            "         [ 0.1065,  0.0057, -0.0746,  ..., -0.1734,  0.0015,  0.0372],\n",
            "         [ 0.2395, -0.2179,  0.0087,  ..., -0.3459,  0.0305,  0.3306],\n",
            "         [ 0.2721, -0.4986,  0.0619,  ..., -0.4886,  0.0518,  0.5480]],\n",
            "\n",
            "        [[ 0.0114,  0.0863, -0.1290,  ..., -0.0334, -0.1111, -0.0654],\n",
            "         [ 0.0180,  0.1083, -0.1074,  ..., -0.0525, -0.0965, -0.0591],\n",
            "         [ 0.0170,  0.1214, -0.0993,  ..., -0.0603, -0.0825, -0.0565],\n",
            "         ...,\n",
            "         [-0.0196,  0.1793, -0.3453,  ..., -0.0464,  0.0343, -0.1094],\n",
            "         [-0.0662,  0.2327, -0.3749,  ..., -0.0298,  0.0629, -0.1090],\n",
            "         [-0.0124,  0.3051, -0.2436,  ..., -0.0117, -0.0749, -0.0738]],\n",
            "\n",
            "        [[ 0.0214,  0.1057, -0.0632,  ..., -0.0339, -0.1038, -0.0503],\n",
            "         [ 0.0157,  0.1144, -0.0771,  ..., -0.0640, -0.0828, -0.0546],\n",
            "         [ 0.0093,  0.1221, -0.0823,  ..., -0.0689, -0.0711, -0.0556],\n",
            "         ...,\n",
            "         [ 0.0377,  0.0770, -0.0850,  ...,  0.0114, -0.1215, -0.0919],\n",
            "         [ 0.0905, -0.0323, -0.1649,  ..., -0.1069, -0.0472, -0.0797],\n",
            "         [ 0.0780, -0.0269, -0.1055,  ..., -0.2067, -0.1199, -0.1143]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0375,  0.1163, -0.0314,  ..., -0.1534, -0.0635, -0.0131],\n",
            "         [ 0.0184,  0.1243, -0.0445,  ..., -0.1200, -0.0434, -0.0326],\n",
            "         [ 0.0081,  0.1316, -0.0597,  ..., -0.0880, -0.0449, -0.0461],\n",
            "         ...,\n",
            "         [-0.0808,  0.0597, -0.1536,  ...,  0.0759, -0.0782, -0.0703],\n",
            "         [ 0.0048,  0.0604, -0.0571,  ...,  0.1144, -0.1207, -0.0664],\n",
            "         [-0.1059,  0.1043,  0.0144,  ...,  0.1129, -0.1397, -0.1014]],\n",
            "\n",
            "        [[-0.0829,  0.0811, -0.0335,  ..., -0.0466, -0.0656, -0.0425],\n",
            "         [-0.0373,  0.0934, -0.0678,  ..., -0.0603, -0.0789, -0.0538],\n",
            "         [-0.0113,  0.1051, -0.0800,  ..., -0.0683, -0.0833, -0.0550],\n",
            "         ...,\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [-0.0846,  0.1530, -0.2067,  ..., -0.0225, -0.0256, -0.1487],\n",
            "         [ 0.0119,  0.1494, -0.1773,  ..., -0.0550, -0.0499, -0.1263]],\n",
            "\n",
            "        [[-0.0063,  0.1414, -0.1126,  ..., -0.0395, -0.0768, -0.0836],\n",
            "         [ 0.0037,  0.1524, -0.1069,  ..., -0.0544, -0.0652, -0.0801],\n",
            "         [ 0.0044,  0.1497, -0.1027,  ..., -0.0576, -0.0599, -0.0733],\n",
            "         ...,\n",
            "         [-0.0416,  0.0452,  0.0061,  ...,  0.0549, -0.3285, -0.0691],\n",
            "         [-0.0738,  0.0385,  0.0022,  ...,  0.0783, -0.3052, -0.0776],\n",
            "         [-0.0472,  0.0439,  0.0251,  ...,  0.0943, -0.3914, -0.1097]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 0.0962, -0.4500,  0.0961,  ..., -0.4131,  0.0079,  0.4860],\n",
            "         [ 0.0055, -0.2673,  0.1237,  ..., -0.3663, -0.0104,  0.3940],\n",
            "         [-0.0308, -0.1006,  0.1014,  ..., -0.2738, -0.0257,  0.3333],\n",
            "         ...,\n",
            "         [-0.0387,  0.0646, -0.1159,  ...,  0.0350, -0.1516, -0.0632],\n",
            "         [ 0.0464,  0.0805, -0.0267,  ...,  0.1041, -0.1805, -0.0546],\n",
            "         [ 0.1170,  0.0350, -0.0243,  ...,  0.0247, -0.1412, -0.0160]],\n",
            "\n",
            "        [[ 0.0323,  0.2879, -0.1735,  ..., -0.0182, -0.0890, -0.0724],\n",
            "         [ 0.0386,  0.2440, -0.1295,  ..., -0.0228, -0.0883, -0.0771],\n",
            "         [ 0.0344,  0.2037, -0.1092,  ..., -0.0313, -0.0823, -0.0772],\n",
            "         ...,\n",
            "         [-0.0223,  0.1519, -0.1045,  ..., -0.0366, -0.1189, -0.0760],\n",
            "         [-0.0173,  0.0676, -0.2146,  ..., -0.1098, -0.1551, -0.1478],\n",
            "         [-0.1137,  0.1066, -0.2982,  ..., -0.0509, -0.0847, -0.2274]],\n",
            "\n",
            "        [[ 0.0493,  0.0649, -0.1081,  ..., -0.1537, -0.0697, -0.0787],\n",
            "         [ 0.0244,  0.1242, -0.0922,  ..., -0.1104, -0.0663, -0.0812],\n",
            "         [ 0.0154,  0.1415, -0.0882,  ..., -0.0841, -0.0689, -0.0785],\n",
            "         ...,\n",
            "         [-0.0680,  0.1631, -0.1675,  ..., -0.0519, -0.1397, -0.2292],\n",
            "         [-0.0421,  0.0801, -0.2208,  ..., -0.0052, -0.1513, -0.1186],\n",
            "         [-0.0731,  0.0940, -0.0634,  ...,  0.0350, -0.1580, -0.1614]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0084,  0.0511, -0.0145,  ...,  0.0318, -0.1776, -0.0408],\n",
            "         [ 0.0251,  0.0597, -0.0275,  ..., -0.0552, -0.1612, -0.0404],\n",
            "         [ 0.0262,  0.0787, -0.0375,  ..., -0.0840, -0.1270, -0.0387],\n",
            "         ...,\n",
            "         [-0.1699,  0.1388, -0.1513,  ...,  0.0715, -0.0899, -0.1620],\n",
            "         [-0.0559,  0.0525, -0.1804,  ...,  0.0592, -0.2153, -0.0940],\n",
            "         [-0.1252,  0.0910, -0.0379,  ...,  0.0679, -0.1927, -0.1021]],\n",
            "\n",
            "        [[ 0.0502,  0.1645, -0.1062,  ..., -0.0450, -0.0946, -0.0737],\n",
            "         [ 0.0377,  0.1499, -0.1097,  ..., -0.0556, -0.0774, -0.0680],\n",
            "         [ 0.0228,  0.1473, -0.1055,  ..., -0.0553, -0.0663, -0.0641],\n",
            "         ...,\n",
            "         [-0.1912,  0.0289, -0.1477,  ...,  0.0571, -0.0674, -0.0904],\n",
            "         [-0.0600,  0.0424, -0.1211,  ...,  0.0471, -0.1239, -0.0547],\n",
            "         [-0.0938,  0.0491, -0.1939,  ...,  0.0265, -0.1315, -0.0797]],\n",
            "\n",
            "        [[ 0.0053,  0.0170, -0.0027,  ..., -0.0316, -0.3343, -0.0535],\n",
            "         [ 0.0153,  0.0147, -0.0155,  ..., -0.0976, -0.3143, -0.0481],\n",
            "         [ 0.0211,  0.0337, -0.0213,  ..., -0.1222, -0.2500, -0.0405],\n",
            "         ...,\n",
            "         [ 0.0200, -0.8892,  0.0513,  ..., -0.6008,  0.1463,  0.2587],\n",
            "         [ 0.0950, -0.9190,  0.0606,  ..., -0.5664,  0.1281,  0.2936],\n",
            "         [ 0.0717, -0.9420,  0.0597,  ..., -0.5039,  0.0671,  0.3124]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 8.4272e-02,  6.4707e-02, -3.6624e-02,  ..., -4.7868e-02,\n",
            "          -8.9738e-02, -5.5542e-02],\n",
            "         [ 5.3519e-02,  8.7258e-02, -4.9754e-02,  ..., -6.7091e-02,\n",
            "          -7.9232e-02, -5.9765e-02],\n",
            "         [ 3.4646e-02,  1.0374e-01, -6.1857e-02,  ..., -7.2398e-02,\n",
            "          -7.7279e-02, -5.7900e-02],\n",
            "         ...,\n",
            "         [-1.0229e-01,  1.3540e-01, -4.2156e-02,  ...,  1.0883e-01,\n",
            "          -1.1583e-01, -6.7900e-02],\n",
            "         [-6.4807e-02,  1.1010e-01, -5.3730e-02,  ...,  4.4191e-02,\n",
            "          -1.9357e-01, -3.5420e-02],\n",
            "         [-7.0537e-02,  1.0096e-01,  1.7365e-02,  ...,  5.3485e-02,\n",
            "          -1.6757e-01, -7.0812e-02]],\n",
            "\n",
            "        [[ 7.2847e-03,  1.3378e-01, -1.2107e-01,  ..., -5.0302e-02,\n",
            "          -1.4311e-01, -1.0720e-01],\n",
            "         [ 1.0100e-02,  1.3542e-01, -1.0817e-01,  ..., -7.0382e-02,\n",
            "          -9.9507e-02, -9.6092e-02],\n",
            "         [ 6.5643e-03,  1.4121e-01, -9.6615e-02,  ..., -6.8298e-02,\n",
            "          -7.4055e-02, -8.1995e-02],\n",
            "         ...,\n",
            "         [-4.2896e-02,  8.0249e-02, -7.5970e-02,  ..., -3.4373e-02,\n",
            "          -4.1124e-02, -1.1962e-01],\n",
            "         [ 4.5585e-03,  3.1133e-02, -1.3039e-01,  ..., -8.2514e-02,\n",
            "          -1.3154e-02, -1.4583e-01],\n",
            "         [-4.1447e-02,  6.3554e-03, -6.3515e-02,  ..., -5.3652e-02,\n",
            "          -2.6896e-02, -1.1668e-01]],\n",
            "\n",
            "        [[ 2.5407e-02,  8.6285e-02, -2.0749e-02,  ..., -2.2307e-03,\n",
            "          -1.7124e-01, -5.9142e-02],\n",
            "         [ 3.5265e-02,  1.0013e-01, -3.4038e-02,  ..., -4.7251e-02,\n",
            "          -1.5133e-01, -5.1336e-02],\n",
            "         [ 3.2710e-02,  1.1212e-01, -4.5168e-02,  ..., -6.6138e-02,\n",
            "          -1.1752e-01, -4.6910e-02],\n",
            "         ...,\n",
            "         [-3.3335e-02,  8.4085e-02, -1.4275e-01,  ..., -7.9475e-02,\n",
            "          -1.1836e-02, -3.9217e-03],\n",
            "         [ 1.6057e-02,  1.0543e-01, -1.1729e-01,  ..., -1.4584e-01,\n",
            "           5.5420e-02, -5.8034e-03],\n",
            "         [ 1.3276e-01, -8.9352e-02, -8.5989e-02,  ..., -3.7655e-01,\n",
            "           4.4641e-03,  1.5922e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-1.9274e-02,  7.4044e-02, -4.9391e-02,  ..., -2.1465e-02,\n",
            "          -1.6474e-01, -5.8049e-02],\n",
            "         [ 7.9666e-04,  9.0328e-02, -5.3620e-02,  ..., -6.8679e-02,\n",
            "          -1.3279e-01, -5.5583e-02],\n",
            "         [ 1.0016e-02,  1.0211e-01, -5.7958e-02,  ..., -8.1983e-02,\n",
            "          -1.0778e-01, -5.0263e-02],\n",
            "         ...,\n",
            "         [ 5.6183e-03,  1.3523e-01, -9.6390e-02,  ..., -6.1194e-02,\n",
            "          -6.2959e-02, -5.8787e-02],\n",
            "         [-9.3405e-02,  1.4727e-01, -1.4577e-01,  ..., -7.6752e-02,\n",
            "          -9.7135e-03, -7.1897e-02],\n",
            "         [-1.3483e-01,  2.2331e-01, -2.8501e-02,  ...,  5.9739e-03,\n",
            "          -1.7476e-02, -1.0167e-01]],\n",
            "\n",
            "        [[ 1.6150e-02,  1.0278e-01, -8.5758e-02,  ...,  4.3532e-04,\n",
            "          -1.4251e-01, -5.4996e-02],\n",
            "         [ 3.1542e-02,  1.2061e-01, -8.1684e-02,  ..., -3.5468e-02,\n",
            "          -1.2334e-01, -5.5546e-02],\n",
            "         [ 2.6759e-02,  1.3141e-01, -8.0922e-02,  ..., -5.1855e-02,\n",
            "          -9.8450e-02, -5.5783e-02],\n",
            "         ...,\n",
            "         [ 1.6532e-02,  5.0381e-02, -2.4252e-01,  ..., -1.8707e-01,\n",
            "           1.0891e-01, -1.3828e-02],\n",
            "         [ 3.3093e-03,  5.8975e-02, -1.1635e-01,  ..., -1.5268e-01,\n",
            "           1.1320e-02, -2.4731e-02],\n",
            "         [ 1.2153e-01, -1.0131e-01, -1.2935e-01,  ..., -2.8625e-01,\n",
            "           2.1606e-02,  4.3194e-02]],\n",
            "\n",
            "        [[ 1.5895e-02, -8.7137e-01,  9.7958e-02,  ..., -4.0412e-01,\n",
            "          -1.9142e-02,  3.5993e-01],\n",
            "         [-9.9538e-03, -7.6061e-01,  1.0723e-01,  ..., -3.6319e-01,\n",
            "          -5.1316e-02,  3.6454e-01],\n",
            "         [-1.6195e-02, -5.5571e-01,  9.1738e-02,  ..., -2.9130e-01,\n",
            "          -5.6477e-02,  3.4744e-01],\n",
            "         ...,\n",
            "         [ 8.0153e-02,  2.4144e-02, -3.3123e-02,  ..., -2.2824e-01,\n",
            "           1.2308e-02,  1.4700e-02],\n",
            "         [ 7.7452e-02,  3.3462e-02, -1.8913e-03,  ..., -2.4912e-01,\n",
            "          -1.3918e-03,  1.3613e-01],\n",
            "         [ 1.4271e-01, -9.8704e-02,  8.3019e-02,  ..., -3.2879e-01,\n",
            "          -5.9222e-02,  2.4135e-01]]], device='cuda:0',\n",
            "       grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[-1.3765e-02,  5.5890e-02, -4.3355e-02,  ..., -2.6433e-02,\n",
            "          -1.5426e-01, -4.4971e-02],\n",
            "         [ 1.1668e-02,  7.0687e-02, -4.0765e-02,  ..., -5.9629e-02,\n",
            "          -1.4546e-01, -3.7523e-02],\n",
            "         [ 2.1028e-02,  8.8113e-02, -4.6732e-02,  ..., -7.6192e-02,\n",
            "          -1.2362e-01, -3.5705e-02],\n",
            "         ...,\n",
            "         [-1.1137e-01,  9.5392e-02, -1.7358e-01,  ...,  1.4965e-02,\n",
            "          -4.5584e-02, -1.2651e-01],\n",
            "         [-1.9593e-01,  1.7997e-01, -1.1061e-01,  ...,  3.7426e-02,\n",
            "          -3.7199e-02, -1.5888e-01],\n",
            "         [-9.1523e-02,  1.1605e-01, -1.9527e-01,  ...,  2.2906e-02,\n",
            "           3.0017e-02, -8.0623e-02]],\n",
            "\n",
            "        [[-8.7036e-03,  4.2413e-02, -8.4577e-02,  ..., -6.3592e-02,\n",
            "          -5.1155e-02, -7.5203e-02],\n",
            "         [-2.4685e-03,  8.3622e-02, -8.7345e-02,  ..., -6.3350e-02,\n",
            "          -6.1769e-02, -6.7247e-02],\n",
            "         [ 4.3329e-03,  1.0589e-01, -8.7734e-02,  ..., -6.4171e-02,\n",
            "          -6.8846e-02, -6.0573e-02],\n",
            "         ...,\n",
            "         [-7.2961e-02,  1.0398e-01, -2.5497e-01,  ...,  6.5162e-03,\n",
            "          -1.3415e-01, -1.5821e-01],\n",
            "         [ 4.0002e-03,  1.0268e-01, -2.2659e-01,  ..., -4.0092e-02,\n",
            "          -1.8789e-01, -1.4708e-01],\n",
            "         [-3.0340e-02,  1.1239e-01, -1.9246e-01,  ..., -9.9738e-03,\n",
            "          -2.2099e-01, -1.3001e-01]],\n",
            "\n",
            "        [[-8.9643e-03,  9.4890e-02,  1.4857e-03,  ..., -2.4450e-01,\n",
            "          -1.1856e-02,  1.9618e-01],\n",
            "         [-3.3705e-02,  9.9328e-02,  6.6709e-03,  ..., -2.1043e-01,\n",
            "          -2.4009e-02,  1.1912e-01],\n",
            "         [-2.9664e-02,  1.1410e-01, -9.9832e-03,  ..., -1.4378e-01,\n",
            "          -4.0579e-02,  4.9847e-02],\n",
            "         ...,\n",
            "         [ 1.0158e-01,  1.1129e-01, -6.0633e-02,  ..., -4.1263e-04,\n",
            "          -1.9382e-01, -4.9967e-02],\n",
            "         [ 1.1059e-01,  8.8973e-02, -8.9153e-02,  ..., -1.0931e-01,\n",
            "          -1.1572e-01, -4.7945e-02],\n",
            "         [ 3.4571e-02,  1.0368e-01, -9.9522e-02,  ...,  7.2860e-03,\n",
            "          -1.0351e-01, -8.5215e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-1.9177e-02,  1.1745e-01, -4.4430e-02,  ..., -3.3465e-02,\n",
            "          -6.3240e-02, -6.5579e-02],\n",
            "         [ 5.1244e-03,  1.1940e-01, -6.2447e-02,  ..., -5.4640e-02,\n",
            "          -6.3975e-02, -6.4441e-02],\n",
            "         [ 1.0391e-02,  1.2400e-01, -7.3269e-02,  ..., -6.0785e-02,\n",
            "          -6.1948e-02, -6.0249e-02],\n",
            "         ...,\n",
            "         [ 5.5475e-02,  4.8852e-03, -1.8558e-01,  ..., -1.4827e-01,\n",
            "           1.5822e-01, -2.2155e-03],\n",
            "         [ 1.0555e-01,  1.5216e-01, -4.4265e-02,  ..., -1.5597e-01,\n",
            "          -2.5378e-02,  2.9180e-02],\n",
            "         [ 2.0935e-01, -1.0699e-01, -3.0365e-02,  ..., -3.3151e-01,\n",
            "           2.9603e-02,  1.1960e-01]],\n",
            "\n",
            "        [[ 1.8352e-02,  2.2825e-02, -5.9221e-02,  ..., -1.5413e-01,\n",
            "          -2.9680e-02,  3.3269e-02],\n",
            "         [-2.2496e-03,  8.1348e-02, -5.7606e-02,  ..., -1.2533e-01,\n",
            "          -5.1771e-02, -1.0403e-02],\n",
            "         [-2.9132e-03,  1.1770e-01, -6.4054e-02,  ..., -8.8149e-02,\n",
            "          -6.1342e-02, -4.2739e-02],\n",
            "         ...,\n",
            "         [ 8.9102e-02,  3.3974e-03, -8.2811e-02,  ..., -1.7879e-01,\n",
            "           5.1330e-02, -2.7501e-02],\n",
            "         [ 6.7202e-02, -4.3026e-02, -1.3830e-01,  ..., -2.0345e-01,\n",
            "           8.3198e-02,  2.1654e-02],\n",
            "         [ 5.2654e-02, -5.0504e-02, -7.9641e-02,  ..., -2.1862e-01,\n",
            "           1.8804e-04,  4.6141e-02]],\n",
            "\n",
            "        [[ 9.2699e-02, -3.6965e-02,  9.8824e-02,  ..., -3.2012e-01,\n",
            "          -1.7513e-02,  2.4051e-01],\n",
            "         [ 3.4164e-02,  3.2711e-02,  7.3773e-02,  ..., -2.5370e-01,\n",
            "          -1.9527e-02,  1.8295e-01],\n",
            "         [ 7.9697e-03,  7.0470e-02,  3.0765e-02,  ..., -1.7485e-01,\n",
            "          -3.8934e-02,  1.0079e-01],\n",
            "         ...,\n",
            "         [-1.3326e-01,  1.4755e-01,  5.6389e-03,  ...,  8.4872e-02,\n",
            "          -1.3763e-01, -5.9582e-02],\n",
            "         [ 1.9327e-02,  6.2004e-02, -5.1671e-02,  ..., -4.3273e-02,\n",
            "          -2.0937e-02, -8.8360e-02],\n",
            "         [ 1.8759e-02,  1.7890e-01, -2.2126e-02,  ..., -9.6755e-02,\n",
            "          -1.1037e-02, -4.7006e-02]]], device='cuda:0',\n",
            "       grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 9.6091e-03,  1.9575e-01, -8.8436e-02,  ...,  1.6575e-02,\n",
            "          -9.3455e-02, -7.3856e-02],\n",
            "         [ 2.8165e-02,  1.7369e-01, -8.9975e-02,  ..., -9.3033e-03,\n",
            "          -9.5992e-02, -7.4552e-02],\n",
            "         [ 2.6672e-02,  1.6118e-01, -8.9316e-02,  ..., -3.2239e-02,\n",
            "          -8.3649e-02, -7.1948e-02],\n",
            "         ...,\n",
            "         [ 1.3705e-01, -7.1291e-01, -3.3067e-02,  ..., -3.1357e-01,\n",
            "           1.1261e-01,  5.0980e-02],\n",
            "         [ 6.1208e-02, -7.6213e-01, -6.6322e-02,  ..., -2.6735e-01,\n",
            "           9.1520e-02,  3.0500e-02],\n",
            "         [ 1.0718e-01, -7.6445e-01, -7.4397e-02,  ..., -3.1726e-01,\n",
            "           9.1209e-02,  2.7213e-02]],\n",
            "\n",
            "        [[ 1.9420e-02,  1.3497e-01, -1.4332e-01,  ..., -3.1192e-02,\n",
            "          -1.5514e-01, -8.2705e-02],\n",
            "         [ 2.9101e-02,  1.4595e-01, -1.1542e-01,  ..., -4.5745e-02,\n",
            "          -1.1301e-01, -7.0511e-02],\n",
            "         [ 2.3934e-02,  1.4716e-01, -1.0493e-01,  ..., -5.3450e-02,\n",
            "          -8.4416e-02, -6.5727e-02],\n",
            "         ...,\n",
            "         [ 1.1380e-02,  1.8697e-01, -4.7350e-02,  ..., -6.3863e-02,\n",
            "          -5.1030e-02, -5.2021e-02],\n",
            "         [ 4.8505e-02,  1.6992e-01, -5.0469e-02,  ..., -1.3857e-01,\n",
            "          -2.7202e-02, -3.4640e-02],\n",
            "         [ 3.0302e-02,  2.1805e-01, -1.1834e-02,  ..., -9.0904e-02,\n",
            "          -5.1306e-02, -4.3660e-02]],\n",
            "\n",
            "        [[ 5.5476e-02,  1.0206e-01, -7.7384e-02,  ..., -1.2761e-02,\n",
            "          -1.1612e-01, -6.4889e-02],\n",
            "         [ 4.0296e-02,  1.1895e-01, -7.9179e-02,  ..., -3.9636e-02,\n",
            "          -8.9006e-02, -6.9477e-02],\n",
            "         [ 2.3997e-02,  1.3054e-01, -8.3383e-02,  ..., -4.9941e-02,\n",
            "          -7.1041e-02, -6.9653e-02],\n",
            "         ...,\n",
            "         [ 3.2470e-02,  2.6640e-03, -9.3830e-02,  ..., -1.5897e-03,\n",
            "          -5.4954e-02, -8.1404e-02],\n",
            "         [ 3.3343e-02, -9.6782e-02, -1.4539e-01,  ..., -1.3870e-02,\n",
            "           1.3131e-01, -7.9078e-02],\n",
            "         [ 7.0498e-04, -9.2197e-02, -1.9432e-01,  ..., -1.5699e-03,\n",
            "           1.0145e-01, -6.9068e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 4.9517e-02, -9.3936e-03, -1.1346e-03,  ..., -2.0989e-01,\n",
            "          -3.0584e-02,  1.0172e-01],\n",
            "         [ 1.8195e-03,  5.8161e-02, -1.2233e-02,  ..., -1.6087e-01,\n",
            "          -5.1520e-02,  3.8012e-02],\n",
            "         [-4.5901e-03,  9.9469e-02, -3.5300e-02,  ..., -1.0845e-01,\n",
            "          -5.9517e-02, -1.3025e-02],\n",
            "         ...,\n",
            "         [ 1.4132e-02,  4.0943e-02,  1.4411e-02,  ...,  1.3031e-01,\n",
            "          -3.6419e-01, -1.1433e-01],\n",
            "         [ 8.7876e-03,  1.6836e-02,  1.9168e-02,  ...,  5.2262e-02,\n",
            "          -3.2878e-01, -7.1013e-02],\n",
            "         [ 8.6985e-03,  1.6811e-02,  7.4876e-03,  ..., -8.0137e-03,\n",
            "          -3.3318e-01, -8.4517e-02]],\n",
            "\n",
            "        [[ 2.0726e-02,  5.6338e-02, -5.3957e-02,  ..., -1.6853e-01,\n",
            "          -4.2613e-02,  1.5935e-02],\n",
            "         [-7.4308e-03,  1.0146e-01, -6.6193e-02,  ..., -1.2561e-01,\n",
            "          -4.7375e-02, -2.3977e-02],\n",
            "         [-7.6109e-03,  1.1974e-01, -7.5682e-02,  ..., -9.0791e-02,\n",
            "          -5.7173e-02, -4.5574e-02],\n",
            "         ...,\n",
            "         [ 5.9499e-02,  1.4207e-01, -4.3223e-02,  ..., -1.6550e-01,\n",
            "           4.1708e-03,  3.2641e-02],\n",
            "         [-2.2447e-03,  1.3039e-01, -3.2130e-02,  ..., -1.1095e-01,\n",
            "          -1.8512e-02,  2.7480e-02],\n",
            "         [ 4.5697e-03,  4.7471e-02, -8.4441e-02,  ..., -1.3156e-01,\n",
            "           5.9447e-02,  8.8849e-03]],\n",
            "\n",
            "        [[ 1.3301e-02,  1.7275e-01, -4.0543e-02,  ..., -1.0641e-01,\n",
            "          -1.9697e-02, -4.5845e-02],\n",
            "         [-4.4200e-03,  1.6060e-01, -6.5496e-02,  ..., -8.9079e-02,\n",
            "          -2.8640e-02, -5.0420e-02],\n",
            "         [-8.5252e-03,  1.4952e-01, -8.1663e-02,  ..., -7.3790e-02,\n",
            "          -4.1983e-02, -5.3929e-02],\n",
            "         ...,\n",
            "         [-1.1088e-01,  9.7386e-02, -3.1141e-01,  ...,  2.0674e-02,\n",
            "          -7.1717e-03, -1.0906e-01],\n",
            "         [-5.1635e-02,  1.0365e-01, -2.1783e-01,  ...,  2.4137e-02,\n",
            "          -7.7317e-02, -8.3530e-02],\n",
            "         [-1.9976e-01,  1.4289e-01, -2.9273e-01,  ...,  3.2775e-02,\n",
            "           6.2136e-03, -8.3871e-02]]], device='cuda:0',\n",
            "       grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 0.0205, -0.6607, -0.0610,  ..., -0.1783, -0.0137,  0.0146],\n",
            "         [ 0.0062, -0.4098, -0.0500,  ..., -0.1171, -0.0749, -0.0206],\n",
            "         [ 0.0061, -0.1389, -0.0490,  ..., -0.0849, -0.0815, -0.0524],\n",
            "         ...,\n",
            "         [ 0.2574, -0.5070,  0.1057,  ..., -0.5849,  0.0584,  0.3989],\n",
            "         [ 0.2107, -0.6967,  0.1121,  ..., -0.6482, -0.0071,  0.4586],\n",
            "         [ 0.1811, -0.5504,  0.1273,  ..., -0.6100, -0.0854,  0.4224]],\n",
            "\n",
            "        [[ 0.0030,  0.1791, -0.0480,  ..., -0.0943, -0.0401, -0.0421],\n",
            "         [-0.0085,  0.1591, -0.0726,  ..., -0.0789, -0.0440, -0.0502],\n",
            "         [-0.0065,  0.1464, -0.0862,  ..., -0.0685, -0.0530, -0.0543],\n",
            "         ...,\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [-0.0976,  0.1334, -0.1882,  ..., -0.0480,  0.0244, -0.0335],\n",
            "         [-0.0247,  0.0629, -0.0986,  ..., -0.0695,  0.0260,  0.0154]],\n",
            "\n",
            "        [[ 0.0395,  0.0241, -0.1138,  ..., -0.0033, -0.0994, -0.0707],\n",
            "         [ 0.0448,  0.0992, -0.1065,  ..., -0.0202, -0.1074, -0.0802],\n",
            "         [ 0.0392,  0.1341, -0.0970,  ..., -0.0333, -0.0971, -0.0819],\n",
            "         ...,\n",
            "         [-0.1038,  0.0440,  0.0166,  ...,  0.1081, -0.3715, -0.1662],\n",
            "         [-0.0473,  0.0129, -0.0197,  ...,  0.0643, -0.3937, -0.0910],\n",
            "         [-0.0451,  0.0320,  0.0031,  ...,  0.0936, -0.3179, -0.0948]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0169,  0.0180, -0.0127,  ..., -0.0799, -0.2735, -0.0563],\n",
            "         [ 0.0246,  0.0301, -0.0212,  ..., -0.1156, -0.2188, -0.0462],\n",
            "         [ 0.0265,  0.0525, -0.0286,  ..., -0.1246, -0.1635, -0.0407],\n",
            "         ...,\n",
            "         [-0.0631,  0.0418,  0.0064,  ...,  0.0700, -0.4155, -0.1002],\n",
            "         [-0.1014,  0.0179, -0.0271,  ..., -0.0261, -0.3976, -0.0889],\n",
            "         [-0.0700,  0.0547,  0.0202,  ..., -0.0013, -0.3168, -0.1104]],\n",
            "\n",
            "        [[ 0.0100,  0.0945, -0.0781,  ..., -0.1049,  0.0025, -0.0221],\n",
            "         [-0.0021,  0.1251, -0.0843,  ..., -0.0814, -0.0277, -0.0442],\n",
            "         [-0.0040,  0.1340, -0.0904,  ..., -0.0668, -0.0454, -0.0547],\n",
            "         ...,\n",
            "         [-0.0516,  0.1333, -0.1610,  ..., -0.0755, -0.0074, -0.0234],\n",
            "         [-0.0067, -0.1377, -0.2929,  ..., -0.1392,  0.1392,  0.0463],\n",
            "         [ 0.0387, -0.4131, -0.2016,  ..., -0.1969,  0.1210,  0.1458]],\n",
            "\n",
            "        [[ 0.0179,  0.1621, -0.1173,  ...,  0.0176, -0.1019, -0.0548],\n",
            "         [ 0.0440,  0.1653, -0.0943,  ..., -0.0104, -0.1094, -0.0638],\n",
            "         [ 0.0403,  0.1548, -0.0871,  ..., -0.0332, -0.0942, -0.0656],\n",
            "         ...,\n",
            "         [ 0.0425, -0.1415, -0.1983,  ..., -0.2873,  0.1960, -0.0318],\n",
            "         [ 0.0444, -0.1788, -0.1101,  ..., -0.1787,  0.1194, -0.0733],\n",
            "         [ 0.0368, -0.2486, -0.1136,  ..., -0.1961,  0.0715, -0.0543]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 1.0411e-01, -3.6380e-01,  1.7445e-01,  ..., -5.1545e-01,\n",
            "          -3.5181e-02,  3.6222e-01],\n",
            "         [ 3.0859e-02, -1.5310e-01,  1.5907e-01,  ..., -4.2108e-01,\n",
            "          -2.9680e-02,  3.8440e-01],\n",
            "         [ 2.0548e-03, -2.5846e-02,  1.1365e-01,  ..., -3.1282e-01,\n",
            "          -4.1488e-02,  3.3524e-01],\n",
            "         ...,\n",
            "         [-1.6492e-01,  1.3250e-01, -2.7985e-01,  ..., -3.9768e-02,\n",
            "           5.9670e-02, -1.0180e-01],\n",
            "         [-1.1415e-01,  1.8625e-01, -2.0331e-01,  ...,  9.3788e-03,\n",
            "           3.2429e-02, -1.0955e-01],\n",
            "         [-6.0438e-02,  2.3743e-01, -1.2881e-01,  ...,  1.7661e-02,\n",
            "          -9.4163e-03, -1.3637e-01]],\n",
            "\n",
            "        [[ 7.6627e-03,  1.0658e-01, -6.2342e-02,  ..., -4.3355e-02,\n",
            "          -3.0083e-02, -2.1290e-02],\n",
            "         [ 2.2280e-02,  1.0885e-01, -7.1164e-02,  ..., -5.2790e-02,\n",
            "          -4.0916e-02, -4.4878e-02],\n",
            "         [ 1.8113e-02,  1.2606e-01, -7.6999e-02,  ..., -5.4792e-02,\n",
            "          -4.5475e-02, -5.8334e-02],\n",
            "         ...,\n",
            "         [ 8.6734e-02, -7.6807e-01,  6.6488e-02,  ..., -5.1374e-01,\n",
            "          -1.7195e-02,  4.2915e-01],\n",
            "         [ 2.3014e-01, -9.0500e-01,  8.6069e-02,  ..., -6.8623e-01,\n",
            "           6.5414e-02,  4.4140e-01],\n",
            "         [ 1.0978e-01, -9.4783e-01,  6.2807e-02,  ..., -6.2936e-01,\n",
            "           3.7947e-02,  4.1725e-01]],\n",
            "\n",
            "        [[ 6.1516e-03,  1.6435e-02, -3.4448e-04,  ...,  2.6767e-02,\n",
            "          -2.9046e-01, -3.5247e-02],\n",
            "         [ 1.4814e-02,  1.2058e-02, -9.9064e-03,  ..., -5.5673e-02,\n",
            "          -3.2036e-01, -4.0619e-02],\n",
            "         [ 2.0461e-02,  2.3388e-02, -1.4323e-02,  ..., -9.9606e-02,\n",
            "          -2.8941e-01, -3.3699e-02],\n",
            "         ...,\n",
            "         [ 9.0212e-02,  7.8902e-02, -7.5704e-02,  ..., -2.9910e-01,\n",
            "           2.3067e-02,  1.0498e-02],\n",
            "         [ 7.0463e-04,  8.2560e-02,  3.3846e-03,  ..., -2.9593e-01,\n",
            "          -4.7596e-02,  1.1428e-01],\n",
            "         [ 7.1458e-02, -1.9346e-01,  4.3002e-02,  ..., -5.1372e-01,\n",
            "          -5.6001e-03,  2.2544e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-2.2859e-02,  1.9139e-02, -1.0809e-02,  ..., -6.3048e-02,\n",
            "          -3.1009e-01, -6.5761e-02],\n",
            "         [ 2.5574e-03,  1.2143e-02, -1.6373e-02,  ..., -1.1059e-01,\n",
            "          -2.7818e-01, -4.3210e-02],\n",
            "         [ 1.3154e-02,  2.7324e-02, -1.8903e-02,  ..., -1.3044e-01,\n",
            "          -2.2348e-01, -3.3763e-02],\n",
            "         ...,\n",
            "         [ 4.3977e-02,  2.6721e-02, -2.1545e-02,  ..., -5.6727e-02,\n",
            "          -1.1974e-01, -1.1483e-01],\n",
            "         [-3.0219e-02,  7.2121e-02, -2.5965e-02,  ...,  2.4400e-02,\n",
            "          -8.9685e-02, -1.1994e-01],\n",
            "         [-7.2837e-02,  3.8376e-02, -1.0331e-01,  ...,  5.3489e-02,\n",
            "          -6.8320e-03, -1.3618e-01]],\n",
            "\n",
            "        [[-2.5907e-02, -2.3934e-01, -5.8612e-02,  ..., -9.3346e-02,\n",
            "          -3.9300e-02,  9.8882e-02],\n",
            "         [-9.1300e-03, -6.4188e-02, -6.1424e-02,  ..., -1.0001e-01,\n",
            "          -5.1162e-02,  2.2659e-02],\n",
            "         [-2.0626e-04,  4.2952e-02, -6.3368e-02,  ..., -7.8344e-02,\n",
            "          -5.9751e-02, -2.7726e-02],\n",
            "         ...,\n",
            "         [ 3.1748e-02,  9.5523e-02,  3.5065e-02,  ..., -1.0903e-01,\n",
            "          -2.5858e-01,  2.5961e-02],\n",
            "         [ 8.0061e-02,  9.7595e-02,  5.3990e-02,  ..., -1.0505e-01,\n",
            "          -1.5826e-01, -8.6826e-05],\n",
            "         [ 1.2197e-01, -6.2063e-02, -6.4107e-03,  ..., -7.9350e-02,\n",
            "          -4.6009e-03, -6.0595e-03]],\n",
            "\n",
            "        [[ 1.7883e-02, -6.1672e-02, -7.5849e-02,  ..., -1.2255e-01,\n",
            "          -6.0779e-02, -5.9460e-02],\n",
            "         [ 1.1530e-02,  5.8194e-02, -7.3067e-02,  ..., -1.0006e-01,\n",
            "          -7.5872e-02, -6.6541e-02],\n",
            "         [ 9.1897e-03,  1.1142e-01, -7.4757e-02,  ..., -7.9451e-02,\n",
            "          -7.5555e-02, -7.2353e-02],\n",
            "         ...,\n",
            "         [-3.9368e-02, -9.7776e-02, -2.2064e-01,  ..., -9.3501e-02,\n",
            "           1.1157e-01, -4.2039e-02],\n",
            "         [-3.9846e-02, -7.5534e-02, -1.3771e-01,  ..., -1.8876e-02,\n",
            "           1.9293e-01, -3.4780e-02],\n",
            "         [-4.5851e-02, -6.6739e-02, -1.0311e-01,  ..., -3.7179e-02,\n",
            "           1.3610e-01,  5.9682e-04]]], device='cuda:0',\n",
            "       grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 2.0481e-02,  2.1636e-01, -9.1633e-02,  ..., -1.2128e-02,\n",
            "          -6.6325e-02, -8.0662e-02],\n",
            "         [ 2.5794e-02,  1.9483e-01, -8.9706e-02,  ..., -3.8821e-02,\n",
            "          -6.7524e-02, -7.0288e-02],\n",
            "         [ 1.9345e-02,  1.6864e-01, -9.1452e-02,  ..., -5.2760e-02,\n",
            "          -6.3415e-02, -6.4950e-02],\n",
            "         ...,\n",
            "         [ 5.0609e-03,  1.5832e-01, -8.2780e-02,  ..., -5.1185e-02,\n",
            "          -7.3332e-02, -8.1564e-02],\n",
            "         [ 2.6560e-02,  4.1705e-02, -1.4519e-01,  ..., -3.1292e-02,\n",
            "          -1.0869e-01, -1.2772e-02],\n",
            "         [-1.6732e-02,  8.2384e-02, -1.9459e-01,  ..., -4.6593e-02,\n",
            "          -5.1093e-02, -8.0849e-02]],\n",
            "\n",
            "        [[-1.0974e-02, -9.0783e-01,  9.7343e-02,  ..., -4.0337e-01,\n",
            "          -5.8004e-02,  3.2384e-01],\n",
            "         [-3.7404e-02, -7.7175e-01,  1.1208e-01,  ..., -3.5217e-01,\n",
            "          -6.2409e-02,  3.4309e-01],\n",
            "         [-3.6471e-02, -5.8661e-01,  9.1662e-02,  ..., -2.6466e-01,\n",
            "          -6.1978e-02,  3.3924e-01],\n",
            "         ...,\n",
            "         [ 5.6183e-03,  1.3523e-01, -9.6390e-02,  ..., -6.1194e-02,\n",
            "          -6.2959e-02, -5.8787e-02],\n",
            "         [ 5.6183e-03,  1.3523e-01, -9.6390e-02,  ..., -6.1194e-02,\n",
            "          -6.2959e-02, -5.8787e-02],\n",
            "         [-1.4135e-01,  1.8103e-01, -1.6203e-01,  ..., -2.5994e-02,\n",
            "           3.4453e-02, -1.7109e-01]],\n",
            "\n",
            "        [[-3.5699e-02, -8.9859e-02,  7.0147e-02,  ..., -3.6001e-01,\n",
            "          -1.5727e-02,  2.6644e-01],\n",
            "         [-4.2578e-02, -1.2471e-02,  7.2625e-02,  ..., -3.1344e-01,\n",
            "          -3.0544e-02,  2.0542e-01],\n",
            "         [-3.4989e-02,  5.2163e-02,  3.9441e-02,  ..., -2.1340e-01,\n",
            "          -4.2993e-02,  1.2381e-01],\n",
            "         ...,\n",
            "         [ 2.1741e-02,  1.0659e-01,  1.2820e-01,  ..., -4.3943e-01,\n",
            "          -1.9104e-01,  2.0245e-03],\n",
            "         [ 9.1017e-02, -3.1027e-02,  1.1786e-01,  ..., -4.9928e-01,\n",
            "          -8.4985e-02,  1.1607e-01],\n",
            "         [ 1.3019e-01, -6.4317e-02,  9.9862e-02,  ..., -5.4638e-01,\n",
            "          -8.9797e-02,  2.3077e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-3.9653e-02,  9.6569e-02, -9.4743e-02,  ...,  5.2387e-02,\n",
            "          -5.6453e-02, -1.1398e-01],\n",
            "         [ 7.1780e-03,  1.0445e-01, -9.0366e-02,  ...,  2.1003e-04,\n",
            "          -8.5259e-02, -8.3578e-02],\n",
            "         [ 1.8211e-02,  1.1822e-01, -8.9122e-02,  ..., -3.7654e-02,\n",
            "          -8.7144e-02, -7.2165e-02],\n",
            "         ...,\n",
            "         [-1.5959e-02,  4.1173e-02, -1.0649e-01,  ..., -9.4586e-02,\n",
            "           3.7867e-02, -1.4620e-01],\n",
            "         [ 1.5107e-02, -7.7869e-02, -1.0807e-01,  ..., -8.6725e-02,\n",
            "           4.0122e-02, -1.5025e-01],\n",
            "         [-1.5931e-02, -2.6366e-01, -9.8072e-02,  ..., -1.0239e-01,\n",
            "           8.5086e-02, -1.5027e-01]],\n",
            "\n",
            "        [[ 4.5105e-02, -5.1070e-03, -5.1970e-02,  ..., -8.2390e-02,\n",
            "          -1.8045e-02, -3.9040e-02],\n",
            "         [ 2.9292e-02,  4.0674e-02, -6.4412e-02,  ..., -7.8374e-02,\n",
            "          -5.8253e-02, -4.7691e-02],\n",
            "         [ 2.3853e-02,  7.4076e-02, -7.2096e-02,  ..., -7.7090e-02,\n",
            "          -7.6757e-02, -4.8778e-02],\n",
            "         ...,\n",
            "         [ 2.1614e-02,  8.3135e-02,  4.9523e-02,  ..., -1.4907e-01,\n",
            "          -1.7149e-01, -1.9374e-02],\n",
            "         [-9.0064e-03,  6.4622e-02,  7.9999e-02,  ..., -6.9427e-02,\n",
            "          -1.4905e-01, -9.1097e-02],\n",
            "         [ 5.6694e-02, -5.7622e-03,  8.3719e-02,  ..., -6.1736e-02,\n",
            "          -1.5151e-01, -8.6657e-02]],\n",
            "\n",
            "        [[-1.6542e-02,  3.1160e-02, -7.6744e-02,  ..., -4.0051e-02,\n",
            "           5.7839e-03, -3.7900e-02],\n",
            "         [ 2.2725e-03,  7.9863e-02, -8.4098e-02,  ..., -4.9852e-02,\n",
            "          -4.2579e-02, -5.4867e-02],\n",
            "         [ 7.5289e-03,  1.0965e-01, -8.7471e-02,  ..., -5.5613e-02,\n",
            "          -5.7791e-02, -6.0884e-02],\n",
            "         ...,\n",
            "         [-4.8107e-02,  1.1705e-01, -3.1460e-02,  ...,  4.6410e-02,\n",
            "          -9.3876e-02, -1.0913e-01],\n",
            "         [ 1.1951e-02,  7.6494e-02, -1.8116e-02,  ...,  7.7256e-02,\n",
            "          -9.8792e-02, -1.4208e-01],\n",
            "         [-1.0834e-02,  7.1396e-02, -4.4032e-02,  ...,  1.0440e-01,\n",
            "          -8.0668e-02, -1.4025e-01]]], device='cuda:0',\n",
            "       grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[-0.0307,  0.1625, -0.1260,  ..., -0.0262, -0.0769, -0.0591],\n",
            "         [-0.0131,  0.1426, -0.1150,  ..., -0.0365, -0.0683, -0.0561],\n",
            "         [-0.0048,  0.1425, -0.1050,  ..., -0.0444, -0.0611, -0.0600],\n",
            "         ...,\n",
            "         [-0.0904,  0.1410,  0.0138,  ...,  0.0710, -0.1403, -0.1407],\n",
            "         [-0.0198,  0.0585,  0.0254,  ...,  0.1060, -0.2244, -0.1579],\n",
            "         [-0.0076,  0.0471,  0.0342,  ...,  0.1077, -0.3475, -0.1212]],\n",
            "\n",
            "        [[ 0.0069,  0.1563, -0.0831,  ..., -0.0311, -0.0831, -0.0706],\n",
            "         [ 0.0207,  0.1491, -0.0918,  ..., -0.0577, -0.0788, -0.0663],\n",
            "         [ 0.0136,  0.1471, -0.0936,  ..., -0.0619, -0.0658, -0.0648],\n",
            "         ...,\n",
            "         [-0.0641,  0.0552, -0.0604,  ..., -0.0770,  0.1149,  0.0043],\n",
            "         [-0.0266,  0.1314, -0.0306,  ..., -0.0314, -0.0015,  0.0221],\n",
            "         [ 0.0061,  0.1388, -0.0360,  ..., -0.0085, -0.0818, -0.0062]],\n",
            "\n",
            "        [[ 0.0524, -0.0049,  0.1225,  ..., -0.4364, -0.0506,  0.2308],\n",
            "         [ 0.0029,  0.0216,  0.1106,  ..., -0.3589, -0.0360,  0.2223],\n",
            "         [-0.0125,  0.0500,  0.0710,  ..., -0.2491, -0.0446,  0.1520],\n",
            "         ...,\n",
            "         [-0.0724,  0.1888,  0.0050,  ...,  0.0156, -0.1284, -0.1402],\n",
            "         [-0.1410,  0.1413, -0.0269,  ...,  0.0907, -0.0518, -0.2071],\n",
            "         [-0.1228,  0.0823, -0.0567,  ...,  0.0452, -0.0995, -0.1888]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0228, -0.0909, -0.0641,  ..., -0.0550, -0.0680, -0.1065],\n",
            "         [ 0.0362,  0.0237, -0.0684,  ..., -0.0684, -0.0877, -0.0950],\n",
            "         [ 0.0288,  0.0887, -0.0743,  ..., -0.0697, -0.0797, -0.0907],\n",
            "         ...,\n",
            "         [ 0.0162, -0.0227, -0.0846,  ..., -0.1293,  0.0311,  0.0401],\n",
            "         [ 0.0701, -0.2323, -0.0188,  ..., -0.2031,  0.0528,  0.1404],\n",
            "         [-0.0715, -0.2283, -0.0169,  ..., -0.2087,  0.0984,  0.0731]],\n",
            "\n",
            "        [[ 0.0664,  0.0170,  0.0327,  ..., -0.1408, -0.1022, -0.0579],\n",
            "         [ 0.0494,  0.0499, -0.0031,  ..., -0.1439, -0.0758, -0.0562],\n",
            "         [ 0.0351,  0.0732, -0.0270,  ..., -0.1249, -0.0725, -0.0527],\n",
            "         ...,\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [-0.0519,  0.1388, -0.1757,  ..., -0.0093, -0.0710, -0.1198]],\n",
            "\n",
            "        [[ 0.0232,  0.0734, -0.0303,  ...,  0.0193, -0.1184, -0.0690],\n",
            "         [ 0.0327,  0.0888, -0.0342,  ..., -0.0406, -0.1183, -0.0516],\n",
            "         [ 0.0311,  0.1031, -0.0448,  ..., -0.0651, -0.1027, -0.0463],\n",
            "         ...,\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [-0.0551,  0.1524, -0.1325,  ..., -0.1067, -0.0073, -0.1674],\n",
            "         [ 0.0117,  0.1018, -0.1963,  ..., -0.0922, -0.0689, -0.0842]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 1.4903e-02,  3.1195e-02, -9.3443e-03,  ...,  2.8392e-03,\n",
            "          -2.7192e-01, -7.5335e-02],\n",
            "         [ 2.1181e-02,  3.9921e-02, -1.9681e-02,  ..., -7.2315e-02,\n",
            "          -2.1760e-01, -6.0738e-02],\n",
            "         [ 2.3721e-02,  5.8084e-02, -2.6713e-02,  ..., -1.0056e-01,\n",
            "          -1.6801e-01, -4.8395e-02],\n",
            "         ...,\n",
            "         [ 5.3749e-02,  4.0812e-02, -4.8232e-02,  ..., -1.5328e-01,\n",
            "           1.1913e-02,  1.7279e-04],\n",
            "         [ 8.3561e-02,  2.1089e-03, -9.0720e-03,  ..., -1.5359e-01,\n",
            "           8.5362e-03,  3.4105e-02],\n",
            "         [-5.2602e-03,  5.5513e-02,  4.6885e-02,  ..., -4.8332e-02,\n",
            "           3.7685e-03, -1.1972e-02]],\n",
            "\n",
            "        [[ 3.1617e-02,  1.3299e-01, -4.8874e-02,  ..., -3.0248e-02,\n",
            "          -8.7397e-02, -2.5709e-02],\n",
            "         [ 2.6495e-02,  1.3882e-01, -6.6444e-02,  ..., -4.6296e-02,\n",
            "          -7.2948e-02, -4.5212e-02],\n",
            "         [ 1.9652e-02,  1.4059e-01, -7.8515e-02,  ..., -5.3138e-02,\n",
            "          -6.5927e-02, -5.5486e-02],\n",
            "         ...,\n",
            "         [ 7.1827e-02,  3.6815e-02,  5.0091e-02,  ...,  3.5725e-02,\n",
            "          -5.0331e-01, -2.7610e-02],\n",
            "         [ 9.6504e-02,  2.5960e-03,  3.0887e-02,  ..., -3.1186e-02,\n",
            "          -5.1722e-01, -6.5642e-02],\n",
            "         [ 4.6064e-02,  3.2155e-02,  2.6251e-02,  ..., -2.2204e-04,\n",
            "          -4.5126e-01, -1.5719e-01]],\n",
            "\n",
            "        [[-2.6470e-03,  8.1583e-02, -5.3292e-02,  ..., -4.2451e-02,\n",
            "          -1.7310e-01, -1.0626e-01],\n",
            "         [ 7.5727e-03,  9.6368e-02, -6.0083e-02,  ..., -8.0640e-02,\n",
            "          -1.4368e-01, -8.7673e-02],\n",
            "         [ 1.2369e-02,  1.0448e-01, -6.0782e-02,  ..., -8.8920e-02,\n",
            "          -1.1613e-01, -6.5152e-02],\n",
            "         ...,\n",
            "         [ 7.9966e-02,  1.0963e-01, -3.6916e-02,  ..., -1.1696e-01,\n",
            "          -3.9775e-02,  3.0367e-02],\n",
            "         [ 1.5219e-01,  1.0244e-01, -4.1371e-02,  ..., -3.0767e-01,\n",
            "           1.1538e-02,  6.9718e-02],\n",
            "         [ 1.5961e-01, -2.0337e-01, -3.2305e-02,  ..., -4.6281e-01,\n",
            "           3.1778e-02,  1.5974e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-1.0331e-02, -7.2837e-02, -9.7511e-03,  ..., -1.5291e-01,\n",
            "          -2.8523e-02,  2.7598e-02],\n",
            "         [-3.1734e-03,  3.6363e-02, -3.0246e-02,  ..., -1.2564e-01,\n",
            "          -4.2966e-02, -1.9490e-02],\n",
            "         [-5.1983e-04,  9.6285e-02, -5.2298e-02,  ..., -9.2485e-02,\n",
            "          -5.1081e-02, -4.8011e-02],\n",
            "         ...,\n",
            "         [ 2.7874e-02,  7.2725e-02, -1.0776e-01,  ...,  5.5466e-02,\n",
            "          -2.8339e-01, -5.1464e-02],\n",
            "         [ 4.9263e-02,  4.8789e-02, -5.3599e-02,  ...,  4.1904e-02,\n",
            "          -3.0267e-01, -4.2462e-02],\n",
            "         [ 3.9706e-02,  3.8009e-02, -1.6922e-02,  ...,  7.6250e-02,\n",
            "          -3.2583e-01, -4.4975e-02]],\n",
            "\n",
            "        [[ 2.6405e-02,  1.2778e-01, -1.0958e-01,  ..., -3.9214e-02,\n",
            "          -1.0381e-01, -6.2679e-02],\n",
            "         [ 2.0838e-02,  1.3707e-01, -1.0060e-01,  ..., -6.0809e-02,\n",
            "          -8.1035e-02, -6.4887e-02],\n",
            "         [ 1.3603e-02,  1.3798e-01, -9.4733e-02,  ..., -6.4070e-02,\n",
            "          -6.9840e-02, -6.1819e-02],\n",
            "         ...,\n",
            "         [ 1.9785e-01, -6.6542e-01,  7.2473e-02,  ..., -5.4413e-01,\n",
            "           3.3106e-02,  4.6312e-01],\n",
            "         [ 2.3152e-01, -8.2547e-01,  1.1541e-01,  ..., -6.2179e-01,\n",
            "           1.0418e-01,  4.7138e-01],\n",
            "         [ 1.6570e-01, -8.8954e-01,  8.1036e-02,  ..., -5.6692e-01,\n",
            "           4.6328e-02,  5.4200e-01]],\n",
            "\n",
            "        [[ 3.2766e-02,  1.5875e-01, -1.0085e-01,  ..., -8.8349e-02,\n",
            "          -7.5414e-02, -6.3345e-02],\n",
            "         [ 1.5519e-02,  1.5542e-01, -9.7732e-02,  ..., -8.4049e-02,\n",
            "          -6.1349e-02, -6.3389e-02],\n",
            "         [ 5.9134e-03,  1.5022e-01, -9.6824e-02,  ..., -7.2762e-02,\n",
            "          -5.8222e-02, -6.2241e-02],\n",
            "         ...,\n",
            "         [-1.5455e-01,  1.2146e-01, -3.5067e-01,  ...,  2.8184e-02,\n",
            "          -2.2783e-02, -1.0652e-01],\n",
            "         [-3.2824e-02,  1.0830e-01, -3.7398e-01,  ..., -2.7110e-02,\n",
            "          -1.6230e-02, -3.2835e-02],\n",
            "         [ 7.3026e-03,  1.6117e-01, -3.7299e-01,  ..., -3.8767e-02,\n",
            "           8.1581e-02,  1.2882e-03]]], device='cuda:0',\n",
            "       grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[-0.0062,  0.0604, -0.0044,  ..., -0.0524, -0.0211, -0.0405],\n",
            "         [ 0.0050,  0.0807, -0.0379,  ..., -0.0591, -0.0540, -0.0509],\n",
            "         [ 0.0112,  0.0966, -0.0580,  ..., -0.0675, -0.0697, -0.0531],\n",
            "         ...,\n",
            "         [ 0.0606, -0.2562,  0.0343,  ..., -0.3587,  0.0565,  0.0211],\n",
            "         [ 0.0221, -0.2574,  0.0256,  ..., -0.3287,  0.0176,  0.0163],\n",
            "         [ 0.0978, -0.4093,  0.0278,  ..., -0.4808, -0.0084,  0.1336]],\n",
            "\n",
            "        [[ 0.0620,  0.0163, -0.0128,  ..., -0.0875, -0.2787, -0.1037],\n",
            "         [ 0.0519,  0.0330, -0.0226,  ..., -0.1079, -0.1981, -0.0773],\n",
            "         [ 0.0435,  0.0533, -0.0297,  ..., -0.1128, -0.1438, -0.0628],\n",
            "         ...,\n",
            "         [-0.0279,  0.0662, -0.0978,  ...,  0.0113, -0.1159, -0.0698],\n",
            "         [-0.0202,  0.0133, -0.1021,  ..., -0.0276, -0.0686, -0.1433],\n",
            "         [-0.0309, -0.0068, -0.0518,  ..., -0.0027, -0.0476, -0.1586]],\n",
            "\n",
            "        [[ 0.0204, -0.1227,  0.0093,  ..., -0.2626, -0.0320,  0.1704],\n",
            "         [-0.0086, -0.0350,  0.0101,  ..., -0.2075, -0.0445,  0.1153],\n",
            "         [-0.0129,  0.0487, -0.0092,  ..., -0.1317, -0.0508,  0.0455],\n",
            "         ...,\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [-0.0885,  0.1524, -0.0781,  ..., -0.0274, -0.0886, -0.0300],\n",
            "         [ 0.0495,  0.0673, -0.0947,  ..., -0.0419, -0.0741, -0.0062]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0372,  0.0386, -0.0584,  ..., -0.0560, -0.2483, -0.0380],\n",
            "         [ 0.0268,  0.0570, -0.0586,  ..., -0.0968, -0.1770, -0.0415],\n",
            "         [ 0.0230,  0.0738, -0.0591,  ..., -0.1028, -0.1334, -0.0407],\n",
            "         ...,\n",
            "         [-0.0011,  0.1789, -0.1306,  ...,  0.0779, -0.1179, -0.0806],\n",
            "         [ 0.0450,  0.1831, -0.1047,  ...,  0.0668, -0.0721, -0.0739],\n",
            "         [ 0.0432,  0.1930, -0.0477,  ..., -0.1044, -0.1414,  0.0010]],\n",
            "\n",
            "        [[-0.0101, -0.8241,  0.1453,  ..., -0.4366,  0.0411,  0.4029],\n",
            "         [-0.0609, -0.6473,  0.1352,  ..., -0.3439, -0.0277,  0.3912],\n",
            "         [-0.0530, -0.3760,  0.0992,  ..., -0.2626, -0.0496,  0.3677],\n",
            "         ...,\n",
            "         [-0.1793,  0.0913, -0.2322,  ...,  0.0331,  0.0279, -0.0956],\n",
            "         [-0.1826,  0.1093, -0.1643,  ...,  0.0507, -0.0198, -0.0750],\n",
            "         [-0.1241,  0.0902, -0.1425,  ...,  0.0610, -0.0670, -0.0861]],\n",
            "\n",
            "        [[ 0.0304,  0.2436, -0.1394,  ..., -0.0178, -0.1053, -0.0131],\n",
            "         [ 0.0371,  0.1957, -0.1169,  ..., -0.0345, -0.0953, -0.0370],\n",
            "         [ 0.0287,  0.1758, -0.1026,  ..., -0.0428, -0.0821, -0.0558],\n",
            "         ...,\n",
            "         [-0.0315,  0.0367,  0.0061,  ...,  0.0754, -0.4091, -0.0531],\n",
            "         [-0.0640,  0.0452,  0.0178,  ...,  0.1315, -0.4050, -0.0932],\n",
            "         [-0.0774,  0.0367,  0.0308,  ...,  0.1506, -0.3756, -0.1384]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 6.2023e-03, -2.3866e-01,  3.7768e-02,  ..., -2.6195e-01,\n",
            "          -3.2278e-02,  1.1000e-01],\n",
            "         [-1.2958e-02, -5.3518e-02,  1.3357e-02,  ..., -1.8612e-01,\n",
            "          -4.8474e-02,  3.8638e-02],\n",
            "         [-7.3463e-03,  4.5376e-02, -1.9486e-02,  ..., -1.2187e-01,\n",
            "          -6.0341e-02, -1.1965e-02],\n",
            "         ...,\n",
            "         [-5.7757e-02,  1.2200e-01, -2.1976e-01,  ..., -6.6241e-03,\n",
            "          -1.0830e-01, -1.8219e-01],\n",
            "         [-1.7869e-02,  1.4699e-01, -1.0524e-01,  ..., -1.5524e-01,\n",
            "          -2.0395e-01, -3.1450e-02],\n",
            "         [ 8.7849e-02,  1.1440e-01, -4.8547e-02,  ..., -3.1176e-01,\n",
            "          -1.3503e-01, -7.7485e-03]],\n",
            "\n",
            "        [[ 1.2635e-02,  4.4235e-02, -5.0520e-02,  ..., -1.1184e-02,\n",
            "          -9.7675e-02, -8.7397e-02],\n",
            "         [ 2.8737e-02,  8.0108e-02, -6.0888e-02,  ..., -3.6295e-02,\n",
            "          -1.0184e-01, -6.4679e-02],\n",
            "         [ 2.8525e-02,  1.0656e-01, -6.7498e-02,  ..., -5.2818e-02,\n",
            "          -9.0487e-02, -5.5289e-02],\n",
            "         ...,\n",
            "         [-1.1710e-01,  4.2826e-02, -1.6197e-04,  ...,  9.5518e-02,\n",
            "          -3.2320e-01, -9.3918e-02],\n",
            "         [-1.2051e-01,  4.1617e-02,  1.5142e-03,  ...,  1.0261e-01,\n",
            "          -3.0367e-01, -7.3912e-02],\n",
            "         [-1.0344e-01,  4.5813e-02,  2.6589e-02,  ...,  8.7892e-02,\n",
            "          -3.1172e-01, -4.2188e-02]],\n",
            "\n",
            "        [[ 3.9352e-02,  1.1041e-01, -7.3455e-02,  ..., -6.2311e-02,\n",
            "          -6.7941e-02, -2.4782e-02],\n",
            "         [ 2.8639e-02,  1.1791e-01, -7.8897e-02,  ..., -6.6632e-02,\n",
            "          -6.0185e-02, -4.0133e-02],\n",
            "         [ 1.8490e-02,  1.2578e-01, -8.4337e-02,  ..., -6.3873e-02,\n",
            "          -5.9695e-02, -4.9051e-02],\n",
            "         ...,\n",
            "         [-2.9497e-02,  1.7095e-01,  9.2872e-02,  ..., -1.3544e-01,\n",
            "          -5.0294e-02,  8.8216e-02],\n",
            "         [-8.3526e-02,  1.1141e-01,  1.1516e-01,  ..., -1.1042e-01,\n",
            "          -3.4136e-02,  9.6405e-02],\n",
            "         [-2.3286e-01,  8.9046e-02,  1.1400e-01,  ...,  3.1038e-02,\n",
            "          -1.7952e-01, -5.9239e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 3.7177e-02,  1.6893e-01, -4.7420e-02,  ..., -1.3246e-01,\n",
            "          -1.0688e-01, -2.9863e-04],\n",
            "         [ 1.1810e-02,  1.5499e-01, -5.5249e-02,  ..., -1.1187e-01,\n",
            "          -5.6652e-02, -3.3445e-02],\n",
            "         [ 2.1620e-03,  1.4874e-01, -6.4210e-02,  ..., -8.4479e-02,\n",
            "          -5.0903e-02, -4.9611e-02],\n",
            "         ...,\n",
            "         [ 5.6183e-03,  1.3523e-01, -9.6390e-02,  ..., -6.1194e-02,\n",
            "          -6.2959e-02, -5.8787e-02],\n",
            "         [-6.2814e-02,  1.7672e-01, -9.2873e-02,  ..., -4.4735e-02,\n",
            "          -6.5002e-02, -3.3402e-02],\n",
            "         [-3.7121e-02,  1.4855e-01, -1.1475e-01,  ..., -5.4333e-02,\n",
            "          -8.6250e-02, -6.2090e-02]],\n",
            "\n",
            "        [[-2.2098e-02,  1.1475e-01, -9.9667e-02,  ...,  2.4382e-02,\n",
            "          -8.3327e-02, -7.5241e-02],\n",
            "         [ 5.5858e-03,  1.2766e-01, -8.8063e-02,  ..., -1.6563e-02,\n",
            "          -7.8792e-02, -6.9089e-02],\n",
            "         [ 1.2318e-02,  1.3377e-01, -8.5504e-02,  ..., -4.0217e-02,\n",
            "          -7.1316e-02, -6.4093e-02],\n",
            "         ...,\n",
            "         [-2.1342e-01,  1.0505e-01, -1.1150e-01,  ..., -3.9673e-02,\n",
            "           1.5576e-02, -9.9532e-02],\n",
            "         [-2.5357e-01,  1.2534e-01, -4.9172e-02,  ...,  3.4508e-02,\n",
            "           3.6766e-02, -1.1091e-01],\n",
            "         [-1.1788e-01,  1.0944e-01, -1.2075e-02,  ..., -4.3337e-02,\n",
            "           1.3562e-02, -1.6516e-01]],\n",
            "\n",
            "        [[-3.0575e-02,  1.2474e-02,  3.0883e-03,  ...,  4.8308e-02,\n",
            "          -3.2772e-01, -6.5918e-02],\n",
            "         [-6.7415e-03,  7.8838e-04, -7.2390e-03,  ..., -5.7860e-02,\n",
            "          -3.3862e-01, -5.2868e-02],\n",
            "         [ 5.7674e-03,  6.3637e-03, -1.2387e-02,  ..., -1.0522e-01,\n",
            "          -3.2349e-01, -4.1305e-02],\n",
            "         ...,\n",
            "         [-1.3365e-01,  1.0567e-01, -5.6117e-02,  ...,  6.2321e-02,\n",
            "          -8.7601e-02, -6.4128e-02],\n",
            "         [-1.6370e-01,  4.2625e-02, -2.1758e-01,  ...,  1.8479e-02,\n",
            "          -4.9859e-03, -2.6048e-02],\n",
            "         [-1.0578e-01,  7.0967e-02, -2.1185e-01,  ...,  3.0460e-03,\n",
            "          -2.9666e-03, -8.1806e-02]]], device='cuda:0',\n",
            "       grad_fn=<CudnnRnnBackward0>)\n",
            "batch size: 100\n",
            "embed shape: torch.Size([100, 50, 64])\n",
            "\n",
            "lstm out shape: torch.Size([100, 50, 300])\n",
            "lstm out: tensor([[[ 0.0782,  0.1459, -0.0318,  ..., -0.2435, -0.0810,  0.0049],\n",
            "         [ 0.0307,  0.1419, -0.0381,  ..., -0.1737, -0.0553, -0.0154],\n",
            "         [ 0.0115,  0.1342, -0.0507,  ..., -0.1192, -0.0598, -0.0316],\n",
            "         ...,\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [-0.0781,  0.0915, -0.1195,  ..., -0.0186, -0.0854, -0.0838],\n",
            "         [-0.0890,  0.0839, -0.1204,  ..., -0.0250, -0.1131, -0.1130]],\n",
            "\n",
            "        [[ 0.0083,  0.0165,  0.0016,  ..., -0.0095, -0.2904, -0.0174],\n",
            "         [ 0.0158,  0.0199, -0.0115,  ..., -0.0839, -0.3075, -0.0335],\n",
            "         [ 0.0207,  0.0366, -0.0169,  ..., -0.1150, -0.2466, -0.0304],\n",
            "         ...,\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588]],\n",
            "\n",
            "        [[-0.0677,  0.0452, -0.0066,  ..., -0.0445, -0.1557, -0.0555],\n",
            "         [-0.0160,  0.0554, -0.0391,  ..., -0.0859, -0.1355, -0.0569],\n",
            "         [ 0.0037,  0.0751, -0.0490,  ..., -0.0978, -0.1119, -0.0503],\n",
            "         ...,\n",
            "         [-0.1469,  0.0156, -0.1339,  ..., -0.0016, -0.0993, -0.0713],\n",
            "         [-0.0994,  0.0054, -0.3511,  ..., -0.0491,  0.0291, -0.0650],\n",
            "         [-0.1091,  0.0658, -0.2543,  ..., -0.0083,  0.0726, -0.1060]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0138,  0.1509, -0.1037,  ..., -0.0565, -0.0860, -0.0633],\n",
            "         [ 0.0129,  0.1543, -0.1079,  ..., -0.0544, -0.0597, -0.0710],\n",
            "         [ 0.0086,  0.1542, -0.1057,  ..., -0.0502, -0.0505, -0.0708],\n",
            "         ...,\n",
            "         [ 0.0056,  0.1352, -0.0964,  ..., -0.0612, -0.0630, -0.0588],\n",
            "         [-0.1862,  0.2133, -0.0782,  ...,  0.0239,  0.0219, -0.0783],\n",
            "         [-0.0156,  0.1450, -0.0634,  ...,  0.0091, -0.0929, -0.0465]],\n",
            "\n",
            "        [[-0.0413,  0.1613, -0.0297,  ..., -0.0777, -0.0346, -0.1151],\n",
            "         [-0.0257,  0.1613, -0.0605,  ..., -0.0824, -0.0252, -0.0958],\n",
            "         [-0.0187,  0.1574, -0.0755,  ..., -0.0693, -0.0342, -0.0807],\n",
            "         ...,\n",
            "         [-0.0799,  0.0843, -0.0590,  ...,  0.0886, -0.2126, -0.0829],\n",
            "         [-0.0840,  0.0484, -0.0425,  ...,  0.1576, -0.3146, -0.0657],\n",
            "         [ 0.0163,  0.0202, -0.0524,  ...,  0.0760, -0.3554, -0.0374]],\n",
            "\n",
            "        [[ 0.0099,  0.1149, -0.0960,  ..., -0.0101, -0.0987, -0.0712],\n",
            "         [ 0.0272,  0.1258, -0.0938,  ..., -0.0350, -0.0975, -0.0707],\n",
            "         [ 0.0234,  0.1362, -0.0914,  ..., -0.0472, -0.0839, -0.0689],\n",
            "         ...,\n",
            "         [-0.0367,  0.0491,  0.0349,  ...,  0.0658, -0.3462, -0.1213],\n",
            "         [-0.0879,  0.1099,  0.0255,  ...,  0.1054, -0.4371, -0.1190],\n",
            "         [-0.1012,  0.0265,  0.0028,  ...,  0.0959, -0.3713, -0.0940]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n",
            "Epoch 5\n",
            "train_loss : 0.717659265200297 val_loss : 0.9123389273881912\n",
            "train_accuracy : 68.37000274658203 val_accuracy : 58.45000076293945\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_tr_acc=[t.cpu().numpy() for t in epoch_tr_acc]\n",
        "epoch_vl_acc= [t.cpu().numpy() for t in epoch_vl_acc]\n",
        "\n",
        "fig = plt.figure(figsize = (20, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epoch_tr_acc, label='Train Acc')\n",
        "plt.plot(epoch_vl_acc, label='Validation Acc')\n",
        "plt.title(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epoch_tr_loss, label='Train loss')\n",
        "plt.plot(epoch_vl_loss, label='Validation loss')\n",
        "plt.title(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RjiW3nQvNI41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "e7a7a46b-2bf5-4784-e262-1fcf44318cb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_text(text):\n",
        "        word_seq = np.array([vocab[preprocess_string(word)] for word in text.split()\n",
        "                         if preprocess_string(word) in vocab.keys()])\n",
        "        word_seq = np.expand_dims(word_seq,axis=0)\n",
        "        pad =  torch.from_numpy(padding_(word_seq,500))\n",
        "        inputs = pad.to(device)\n",
        "        batch_size = 1\n",
        "        h = model.init_hidden(batch_size)\n",
        "        h = tuple([each.data for each in h])\n",
        "        output, h = model(inputs, h)\n",
        "        return(output)"
      ],
      "metadata": {
        "id": "QrJ7xT2Jgl6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = 7\n",
        "print(df['text'][index])\n",
        "print('='*70)\n",
        "print(f'Actual sentiment is  : {df[\"label\"][index]}')\n",
        "print('='*70)\n",
        "pro = predict_text(df['text'][index])\n",
        "status = torch.argmax(pro, dim = 1)\n",
        "print(f'Predicted sentiment is {status.cpu().item()} with a probability of {F.softmax(pro, dim = 1)[0].detach().cpu().numpy()}')"
      ],
      "metadata": {
        "id": "t62DMmfUgqAc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0b838af-2153-447f-920e-49af5fc1fe9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For months  many market watchers have described Intel as complacent as we've seen the surge in new products from companies such as ARM  NVIDIA and Xilinx This week  Intel is hitting back\n",
            "======================================================================\n",
            "Actual sentiment is  : 2\n",
            "======================================================================\n",
            "batch size: 1\n",
            "embed shape: torch.Size([1, 500, 64])\n",
            "\n",
            "lstm out shape: torch.Size([1, 500, 300])\n",
            "lstm out: tensor([[[ 0.0192,  0.0488, -0.0304,  ..., -0.0459, -0.0460, -0.0082],\n",
            "         [ 0.0104,  0.0831, -0.0568,  ..., -0.0641, -0.0515, -0.0272],\n",
            "         [ 0.0053,  0.1046, -0.0719,  ..., -0.0663, -0.0575, -0.0398],\n",
            "         ...,\n",
            "         [-0.0211,  0.0785, -0.0898,  ...,  0.0789, -0.1941, -0.1018],\n",
            "         [ 0.0161,  0.0909, -0.0163,  ...,  0.1266, -0.2162, -0.1267],\n",
            "         [ 0.0137,  0.0908, -0.0182,  ...,  0.0535, -0.2585, -0.0619]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n",
            "Predicted sentiment is 1 with a probability of [0.1346409  0.7090708  0.15628833]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "0 = Positive, 1 = Negative, 2 = Neutral (annotated by GPT 3.5)\n",
        "\n",
        "Very low number of negative sentiment tweets. We would likely need to use the balanced dataset for our categorization."
      ],
      "metadata": {
        "id": "tVC4cDMsfMB7"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}